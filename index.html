<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/time-machine.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Future&#39;s blog">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Future&#39;s blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Future">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Future's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Future's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/21/%E6%9C%AC%E7%AB%99%E9%A3%9F%E7%94%A8%E6%8C%87%E5%8D%97-How-to-use-this-blog/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Future">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Future's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/03/21/%E6%9C%AC%E7%AB%99%E9%A3%9F%E7%94%A8%E6%8C%87%E5%8D%97-How-to-use-this-blog/" class="post-title-link" itemprop="url">How to use this blog 本站食用指南</a>
        </h2>

        <div class="post-meta">
          
              <i class="fa fa-thumbtack"></i>
              <font color=7D26CD>Pinned</font>
              <span class="post-meta-divider">|</span>
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-03-21 16:14:47" itemprop="dateCreated datePublished" datetime="2023-03-21T16:14:47+08:00">2023-03-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-05-29 11:23:24" itemprop="dateModified" datetime="2023-05-29T11:23:24+08:00">2023-05-29</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Hello! 你好！</p>
<p>This blog is established on 2023&#x2F;3&#x2F;21. The aim to build such a blog website which is used to share my programming &#x2F; other knowledges.</p>
<p>本站建立于2023.3.21，建立之初是为了拥有一个自己的知识内容分享平台。</p>
<p>This is the home page, and this article is pinned. If you want to search for some past articles, use “Categories”, “Archives” or “Search” on the top of the page.</p>
<p>这是主页。这篇文章置顶于此。如果你想找指定文章内容，可以在页面上方“Categories”或者“Archives”寻找。</p>
<p><strong>Future and Plans 计划与憧憬</strong></p>
<p>On the first day of our blog, let’s plan it: 建博客第一天，就先来立一些这个博客的计划：</p>
<ul>
<li>Language 语言：We will mainly use English in this blog. 网站主要使用英文。（单词也比较简单，对于中文用户其实阅读难度不大啦）</li>
<li>Contents 内容：The content should mainly be my original content, or a comprehensive content that combines other existing materials with my annotations or supplements. 内容主要应该是我的原创内容，或者是结合了其他现有资料，配合我的注释或是增补的综合内容。</li>
<li>Categories 类别：<ul>
<li>Deep Learning: Things related to PyTorch, deep learning。和torch，深度学习相关的会在这。</li>
<li>QRH: Quick Reference Handbook of tools&#x2F;programming languages&#x2F;packages, 一些工具、软件、包、语言的快速参考手册。</li>
<li>QuickIntro: Quick Introduction to a tools&#x2F;programming languages&#x2F;packages, 快速入门某一工具、软件、包、语言。</li>
<li>Techniques: Techniques and tricks in something. 技艺，一些环境配置、瞎折腾的技术、搭建过程会写在这。</li>
<li>Problem Solving: Record the problems I met and how did I solve it out. 记录我遇到的问题以及我的解决方案。</li>
<li>Others: 其它。</li>
</ul>
</li>
</ul>
<p><strong>TODO list：</strong></p>
<p>在下面列一些近期可能想做的topic（立flag中）</p>
<ul>
<li>QRH</li>
</ul>
<ul>
<li><input checked disabled type="checkbox"> Matplotlib common functions</li>
<li><input disabled type="checkbox"> conda common commands</li>
<li><input disabled type="checkbox"> Linux shell common commands</li>
<li><input disabled type="checkbox"> vim common keys</li>
<li><input checked disabled type="checkbox"> latex common code blocks</li>
<li><input disabled type="checkbox"> </li>
</ul>
<ul>
<li>QuickIntro</li>
</ul>
<ul>
<li><input checked disabled type="checkbox"> NumPy quick intro</li>
<li><input disabled type="checkbox"> 15 min regex</li>
<li><input disabled type="checkbox"> 15 min  Makefile</li>
<li><input disabled type="checkbox"> 15 min  MySQL</li>
<li><input disabled type="checkbox"> </li>
</ul>
<ul>
<li>Techniques</li>
</ul>
<ul>
<li><input checked disabled type="checkbox"> C++ complex type declaration rules (In Chinese, 2&#x2F;2)</li>
<li><input disabled type="checkbox"> </li>
</ul>
<ul>
<li>Others</li>
</ul>
<ul>
<li><input disabled type="checkbox"> Emm…</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/20/Pandas-MySQL-Leetcode-Practice-Notes-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Future">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Future's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/20/Pandas-MySQL-Leetcode-Practice-Notes-1/" class="post-title-link" itemprop="url">Pandas & MySQL Leetcode Practice Notes 1-Condition Filter & Misc</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-08-20 20:05:37" itemprop="dateCreated datePublished" datetime="2023-08-20T20:05:37+08:00">2023-08-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-08-23 18:32:43" itemprop="dateModified" datetime="2023-08-23T18:32:43+08:00">2023-08-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Pandas-SQL/" itemprop="url" rel="index"><span itemprop="name">Pandas_SQL</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>There are 30 questions to practice Python Pandas and MySQL on <a target="_blank" rel="noopener" href="https://leetcode.cn/studyplan/30-days-of-pandas/">https://leetcode.cn/studyplan/30-days-of-pandas/</a> . These questions basically practice the fundamental skills. Pandas and MySQL are similar in some aspects, and one can write the same functional code in two languages. I’ll practice my skill with these questions, and write notes about the equivalent operations in two languages. </p>
<p>Accordingly, there are six sections in this section (so will use 6 articles in total):</p>
<ol>
<li>Condition Filter &amp; Misc</li>
<li>String Manipulation</li>
<li>Data Manipulation</li>
<li>Data Statistics</li>
<li>Grouping</li>
<li>Merging</li>
</ol>
<h1 id="1-Condition-Filter-Join-Misc"><a href="#1-Condition-Filter-Join-Misc" class="headerlink" title="1. Condition Filter &amp; Join &amp; Misc"></a>1. Condition Filter &amp; Join &amp; Misc</h1><p>The four related question in this area are:</p>
<h2 id="1-1-Condition"><a href="#1-1-Condition" class="headerlink" title="1.1 Condition"></a>1.1 Condition</h2><p>Typical condition filter work is asking us to do like this:</p>
<blockquote>
<p>Filter the <strong>big</strong> country in World scheme. A country is <strong>big</strong> if:</p>
<ul>
<li>it has an area of at least three million (i.e., <code>3000000 km2</code>), or</li>
<li>it has a population of at least twenty-five million (i.e., <code>25000000</code>).</li>
</ul>
</blockquote>
<p>We’ll use this example to illustrate the point. </p>
<h3 id="1-1-a-MySQL"><a href="#1-1-a-MySQL" class="headerlink" title="1.1.a MySQL"></a>1.1.a MySQL</h3><p>A typical condition filter in SQL looks like this:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">&lt;</span>col_names<span class="operator">&gt;</span></span><br><span class="line"><span class="keyword">from</span> <span class="operator">&lt;</span>table_name<span class="operator">&gt;</span></span><br><span class="line"><span class="keyword">where</span> (</span><br><span class="line">	<span class="operator">&lt;</span><span class="keyword">condition</span> predicate<span class="operator">&gt;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>SQL allows the use of the logical connectives <strong>and, or, and not</strong>. </p>
<p>The operands of the logical connectives can be expressions involving the comparison operators <strong>&lt;, &lt;&#x3D;, &gt;, &gt;&#x3D;, &#x3D;, and &lt;&gt;</strong> (Note that, equal symbol is one <code>=</code>, but not <code>==</code>!)</p>
<p>For example, in the codes above, the answer is</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> name, population, area</span><br><span class="line"><span class="keyword">FROM</span> World</span><br><span class="line"><span class="keyword">WHERE</span> area <span class="operator">&gt;=</span> <span class="number">3000000</span> <span class="keyword">OR</span> population <span class="operator">&gt;=</span> <span class="number">25000000</span></span><br></pre></td></tr></table></figure>



<h3 id="1-1-b-Pandas"><a href="#1-1-b-Pandas" class="headerlink" title="1.1.b Pandas"></a>1.1.b Pandas</h3><p>Filter operations in pandas is a little bit difference. We should know two basics things first:</p>
<ul>
<li>Condition result in Index Series</li>
</ul>
<p>If we want to get all data that are greater than 5 in <code>df</code>‘s <code>col1</code> column, the code and result is as below:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(df)</span><br><span class="line"><span class="comment"># OUTPUT:</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  name  col1  col2</span></span><br><span class="line"><span class="string">0    a     1    10</span></span><br><span class="line"><span class="string">1    b     2     9</span></span><br><span class="line"><span class="string">2    c     3     8</span></span><br><span class="line"><span class="string">3    d     4     7</span></span><br><span class="line"><span class="string">4    e     5     6</span></span><br><span class="line"><span class="string">5    f     6     5</span></span><br><span class="line"><span class="string">6    g     7     4</span></span><br><span class="line"><span class="string">7    h     8     3</span></span><br><span class="line"><span class="string">8    i     9     2</span></span><br><span class="line"><span class="string">9    j    10     1</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(df[<span class="string">&#x27;col1&#x27;</span>] &gt; <span class="number">5</span>)</span><br><span class="line"><span class="comment"># OUTPUT: </span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">0    False</span></span><br><span class="line"><span class="string">1    False</span></span><br><span class="line"><span class="string">2    False</span></span><br><span class="line"><span class="string">3    False</span></span><br><span class="line"><span class="string">4    False</span></span><br><span class="line"><span class="string">5     True</span></span><br><span class="line"><span class="string">6     True</span></span><br><span class="line"><span class="string">7     True</span></span><br><span class="line"><span class="string">8     True</span></span><br><span class="line"><span class="string">9     True</span></span><br><span class="line"><span class="string">Name: col1, dtype: bool</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<p>As you can see, the result of <code>df[&#39;col1&#39;] &gt; 5</code> is a bool Series with <code>len(df)</code> size. The entry is True only when the corresponding entry at <code>col1</code> satisfies the condition. </p>
<ul>
<li>Indexing by the Index Series</li>
</ul>
<p>We can pass this bool index series in to the <code>df.loc[]</code>. Then, only the rows with the condition satisfied are displayed:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">a = df.loc[df[<span class="string">&#x27;col1&#x27;</span>] &gt; <span class="number">5</span>]</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  name  col1  col2</span></span><br><span class="line"><span class="string">5    f     6     5</span></span><br><span class="line"><span class="string">6    g     7     4</span></span><br><span class="line"><span class="string">7    h     8     3</span></span><br><span class="line"><span class="string">8    i     9     2</span></span><br><span class="line"><span class="string">9    j    10     1</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<p>By such method, we can do the filtering in the pandas. Note that:</p>
<p>For the <code>and</code>, <code>or</code> and <code>not</code> logic operators, the corresponding operator in pandas is <code>&amp;</code>, <code>|</code> and <code>~</code>. For example:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cond1 = df[<span class="string">&#x27;col1&#x27;</span>] &gt; <span class="number">5</span></span><br><span class="line">cond2 = df[<span class="string">&#x27;col2&#x27;</span>] &gt; <span class="number">2</span></span><br><span class="line"></span><br><span class="line">b = df.loc[cond1 &amp; cond2]</span><br><span class="line"></span><br><span class="line">c = df.loc[ (df[<span class="string">&#x27;col1&#x27;</span>] &gt; <span class="number">5</span>) &amp; (df[<span class="string">&#x27;col2&#x27;</span>] &gt; <span class="number">2</span>) ]</span><br></pre></td></tr></table></figure>

<p><strong>Note</strong> that the <code>()</code> symbols are necessary because of the operator precedence. </p>
<h2 id="1-2-Misc"><a href="#1-2-Misc" class="headerlink" title="1.2 Misc"></a>1.2 Misc</h2><h3 id="1-2-1-Rename-output-columns"><a href="#1-2-1-Rename-output-columns" class="headerlink" title="1.2.1 Rename output columns"></a>1.2.1 Rename output columns</h3><h4 id="1-2-1-a-MySQL"><a href="#1-2-1-a-MySQL" class="headerlink" title="1.2.1.a MySQL"></a>1.2.1.a MySQL</h4><p>In MySQL, rename a column is very easy. By default, if the command is</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> name, population, area</span><br><span class="line"><span class="keyword">FROM</span> World</span><br><span class="line"><span class="keyword">WHERE</span> area <span class="operator">&gt;=</span> <span class="number">3000000</span> <span class="keyword">OR</span> population <span class="operator">&gt;=</span> <span class="number">25000000</span></span><br></pre></td></tr></table></figure>

<p>Then the output columns are named <code>name</code>, <code>population</code>, and <code>area</code>. To rename, just use the <code>as</code> symbol to give a new name as output:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- change name =&gt; Name, population =&gt; Population, area =&gt; Area</span></span><br><span class="line"><span class="keyword">SELECT</span> name <span class="keyword">as</span> Name, population <span class="keyword">as</span> Population, area <span class="keyword">as</span> Area</span><br><span class="line"><span class="keyword">FROM</span> World</span><br><span class="line"><span class="keyword">WHERE</span> area <span class="operator">&gt;=</span> <span class="number">3000000</span> <span class="keyword">OR</span> population <span class="operator">&gt;=</span> <span class="number">25000000</span></span><br></pre></td></tr></table></figure>

<h4 id="1-2-1-b-Pandas"><a href="#1-2-1-b-Pandas" class="headerlink" title="1.2.1.b Pandas"></a>1.2.1.b Pandas</h4><p>In pandas, we can use the <code>pd.DataFrame.rename</code> function to rename the columns. The input parameter <code>columns</code> is a <code>dict[str, str]</code>, where key is the old name, and the value is the new name. For example in the example above:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = df.rename(columns=&#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Name&#x27;</span>, <span class="string">&#x27;population&#x27;</span>: <span class="string">&#x27;Population&#x27;</span>, <span class="string">&#x27;area&#x27;</span>: <span class="string">&#x27;Area&#x27;</span>&#125;)</span><br></pre></td></tr></table></figure>

<h3 id="1-2-2-Swap-output-columns-order"><a href="#1-2-2-Swap-output-columns-order" class="headerlink" title="1.2.2 Swap output columns order"></a>1.2.2 Swap output columns order</h3>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/15/Pytorch-Practical-Basics-10/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Future">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Future's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/15/Pytorch-Practical-Basics-10/" class="post-title-link" itemprop="url">PyTorch Practical Hand-Written Modules Basics 10--Summary and Conclusion</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-08-15 19:01:03" itemprop="dateCreated datePublished" datetime="2023-08-15T19:01:03+08:00">2023-08-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-08-21 18:21:20" itemprop="dateModified" datetime="2023-08-21T18:21:20+08:00">2023-08-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Finally, we have finished all contents we want to talk about. In this section, we’ll do a quick summary about what we have talked about and plan for the future of this series.</p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>In our ten sections of tutorial, we are learning from low-level (tensors) to high-level (modules). In detail, the structure looks like this:</p>
<ul>
<li>Tensor operations (Sec <a target="_blank" rel="noopener" href="https://i-am-future.github.io/2023/05/27/Pytorch-Practical-Basics-1/">1</a>, <a target="_blank" rel="noopener" href="https://i-am-future.github.io/2023/05/27/Pytorch-Practical-Basics-2/">2</a>)</li>
<li>Tensor-wise operations (Sec <a target="_blank" rel="noopener" href="https://i-am-future.github.io/2023/06/17/Pytorch-Practical-Basics-3/">3</a>)</li>
<li>Module basics (Sec <a target="_blank" rel="noopener" href="https://i-am-future.github.io/2023/06/18/Pytorch-Practical-Basics-4/">4</a>)</li>
<li>Implement by pure-python (Sec <a target="_blank" rel="noopener" href="https://i-am-future.github.io/2023/07/09/Pytorch-Practical-Basics-5/">5</a> ResNet)</li>
<li>Implement by CUDA (Sec <a target="_blank" rel="noopener" href="https://i-am-future.github.io/2023/07/30/Pytorch-Practical-Basics-6/">6</a>, <a target="_blank" rel="noopener" href="https://i-am-future.github.io/2023/08/01/Pytorch-Practical-Basics-7/">7</a>, <a target="_blank" rel="noopener" href="https://i-am-future.github.io/2023/08/02/Pytorch-Practical-Basics-8/">8</a>, <a target="_blank" rel="noopener" href="https://i-am-future.github.io/2023/08/07/Pytorch-Practical-Basics-9/">9</a>)</li>
</ul>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>From our tutorial, we know that the model consists of <code>nn.Module</code>s. We implement the <code>forward()</code> function with many tensor-wise operations to do the forward pass. </p>
<p>The PyTorch is highly optimized. The Python side <strong>is enough for most cases</strong>. So, it is unnecessary to implement the algorithm in C++/CUDA. (Ref to sec 9. Our CUDA matrix multiplication operation is slower than the PyTorch’s). In addition, when we are writing in native Python, we <strong>don’t need to worry about the correctness of the gradient calculation</strong>. </p>
<p>But just in some rare cases, the <code>forward()</code> implementation is complicated, and they may contain <code>for</code> loop. The performance is low. Under such circumstances, you may consider to write the operator by yourself. But keep in mind that:</p>
<ul>
<li>You need to check if the forward &amp; backward propagations are correct;</li>
<li>You need to do benchmarks - does my operator really get faster?</li>
</ul>
<p>Therefore, manually write a optimized CUDA operator is time consuming and complicated. In addition, one should be equipped with proficient CUDA knowledge. But once you write the good CUDA operators, your program will boost for many times. They are all about trade-off.</p>
<h1 id="Announce-in-Advance"><a href="#Announce-in-Advance" class="headerlink" title="Announce in Advance"></a>Announce in Advance</h1><p>Finally, let’s talk about some things I will do in the future: </p>
<ul>
<li>This series will not end. For this series article 11 and later: we’ll talk about some famous model implementations.</li>
<li>As I said above, writing CUDA operator needs proficient CUDA knowledge. So I’ll setup a new series to tell you how to write good CUDA programs: <em>CUDA Medium Tutorials</em></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/07/Pytorch-Practical-Basics-9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Future">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Future's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/07/Pytorch-Practical-Basics-9/" class="post-title-link" itemprop="url">PyTorch Practical Hand-Written Modules Basics 9--Compiling and Testing the Module</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-08-07 14:07:03" itemprop="dateCreated datePublished" datetime="2023-08-07T14:07:03+08:00">2023-08-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-08-15 19:02:40" itemprop="dateModified" datetime="2023-08-15T19:02:40+08:00">2023-08-15</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>In the section 6 to 9, we’ll investigate how to use <code>torch.autograd.Function</code> to implement the hand-written operators. The tentative outline is:</p>
<ul>
<li><input checked disabled type="checkbox"> In the section 6, we talk about the basics of <code>torch.autograd.Function</code>. <strong>The operators defined by <code>torch.autograd.Function</code> can be automatically back-propagated.</strong></li>
<li><input checked disabled type="checkbox"> In the last section (7), we’ll talk about <strong>mathematic derivation</strong> for the “linear layer” operator. </li>
<li><input checked disabled type="checkbox"> In the section (8), we talk about <strong>writing C++ CUDA extension</strong> for the “linear layer” operator. </li>
<li><input checked disabled type="checkbox"> In this section (9), we talk details about <strong>building the extension</strong> to a python module, as well as <strong>testing</strong> the module. Then we’ll <strong>conclude</strong> the things we’ve done so far.</li>
</ul>
<p><strong>Note:</strong></p>
<ul>
<li>This blog is written with following reference: <ul>
<li>PyTorch official tutorial about CUDA extension: <a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/advanced/cpp_extension.html">website</a>.</li>
<li>YouTube video about writing CUDA extension: <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=l_Rpk6CRJYI">video</a>, <a target="_blank" rel="noopener" href="https://github.com/kwea123/pytorch-cppcuda-tutorial">code</a>.</li>
</ul>
</li>
<li>For how to write CUDA code, you can follow <a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">official documentation</a>, <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/34587739">blogs</a> (In Chinese). You can search by yourself for English tutorials and video tutorials.</li>
<li>This blog only talk some important points in the matrix multiplication example. Code are picked by pieces for illustration. Whole code is at: <a target="_blank" rel="noopener" href="https://github.com/I-am-Future/PyTorch-Linear-Operator-CUDA">code</a>.</li>
</ul>
<h1 id="Python-side-Wrapper"><a href="#Python-side-Wrapper" class="headerlink" title="Python-side Wrapper"></a>Python-side Wrapper</h1><p>Purely using C++ extension functions is not enough in our case. As mentioned in the Section 6, we need to build our operators with <code>torch.autograd.Function</code>. It is not convenient to let the user define the operator wrapper every time, so it’s better if we can write the wrapper in a python module. Then, users can easily import our python module, and using the wrapper class and functions in it.</p>
<img src="/2023/08/07/Pytorch-Practical-Basics-9/cudaops-struct-improved.drawio.png" alt="cudaops-struct-improved.drawio" style="zoom:50%;">

<p>The python module is at <code>mylinearops/</code>. Follow the section 6, we define some <code>autograd.Function</code> operators and <code>nn.Module</code> modules in the <code>mylinearops/mylinearops.py</code>. Then, we export the operators and modules by the code in the <code>mylinearops/__init__.py</code>:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> .mylinearops <span class="keyword">import</span> matmul</span><br><span class="line"><span class="keyword">from</span> .mylinearops <span class="keyword">import</span> linearop</span><br><span class="line"><span class="keyword">from</span> .mylinearops <span class="keyword">import</span> LinearLayer</span><br></pre></td></tr></table></figure>

<p>As a result, when user imports the <code>mylinearops</code>, only the <code>matmul</code> (Y &#x3D; XW) function, <code>linearop</code> (Y &#x3D; XW+b) function and <code>LinearLayer</code> module are public to the users. </p>
<h1 id="Writing-setup-py-and-Building"><a href="#Writing-setup-py-and-Building" class="headerlink" title="Writing setup.py and Building"></a>Writing setup.py and Building</h1><h2 id="setup-py-script"><a href="#setup-py-script" class="headerlink" title="setup.py script"></a>setup.py script</h2><p>The <code>setup.py</code> script is general same for all packages. Next time, you can just copy-paste the code above and modify some key components. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> os.path <span class="keyword">as</span> osp</span><br><span class="line"><span class="keyword">from</span> setuptools <span class="keyword">import</span> setup</span><br><span class="line"><span class="keyword">from</span> torch.utils.cpp_extension <span class="keyword">import</span> CUDAExtension, BuildExtension</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ROOT_DIR = osp.dirname(osp.abspath(__file__))</span><br><span class="line">include_dirs = [osp.join(ROOT_DIR, <span class="string">&quot;include&quot;</span>)]</span><br><span class="line"></span><br><span class="line">SRC_DIR = osp.join(ROOT_DIR, <span class="string">&quot;src&quot;</span>)</span><br><span class="line">sources = glob.glob(osp.join(SRC_DIR, <span class="string">&#x27;*.cpp&#x27;</span>))+glob.glob(osp.join(SRC_DIR, <span class="string">&#x27;*.cu&#x27;</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">setup(</span><br><span class="line">    name=<span class="string">&#x27;mylinearops&#x27;</span>,</span><br><span class="line">    version=<span class="string">&#x27;1.0&#x27;</span>,</span><br><span class="line">    author=...,</span><br><span class="line">    author_email=...,</span><br><span class="line">    description=<span class="string">&#x27;Hand-written Linear ops for PyTorch&#x27;</span>,</span><br><span class="line">    long_description=<span class="string">&#x27;Simple demo for writing Linear ops in CUDA extensions with PyTorch&#x27;</span>,</span><br><span class="line">    ext_modules=[</span><br><span class="line">        CUDAExtension(</span><br><span class="line">            name=<span class="string">&#x27;mylinearops_cuda&#x27;</span>,</span><br><span class="line">            sources=sources,</span><br><span class="line">            include_dirs=include_dirs,</span><br><span class="line">            extra_compile_args=&#123;<span class="string">&#x27;cxx&#x27;</span>: [<span class="string">&#x27;-O2&#x27;</span>],</span><br><span class="line">                                <span class="string">&#x27;nvcc&#x27;</span>: [<span class="string">&#x27;-O2&#x27;</span>]&#125;</span><br><span class="line">        )</span><br><span class="line">    ],</span><br><span class="line">    py_modules=[<span class="string">&#x27;mylinearops.mylinearops&#x27;</span>],</span><br><span class="line">    cmdclass=&#123;</span><br><span class="line">        <span class="string">&#x27;build_ext&#x27;</span>: BuildExtension</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>At the beginning, we first get the path information. We get the <code>include_dirs</code> (Where we store our <code>.h</code> headers), <code>sources</code> (Where we store our C++&#x2F;CUDA source code) directory. </p>
<p>Then, we call the <code>setup</code> function. The parameter explanation are as following:</p>
<ul>
<li><code>name</code>: The package name, how do users call this program</li>
<li><code>version</code>: The version number, decided by the creator</li>
<li><code>author</code>: The creator’s name</li>
<li><code>author_email</code>: The creator’s email</li>
<li><code>description</code>: The package’s description, short version</li>
<li><code>long_description</code>: The package’s description, long version</li>
<li><code>ext_modules</code>: <strong>Key</strong> in our building process. When we are building the PyTorch CUDA extension, we should use <code>CUDAExtension</code>, so that the build helper can know how to compile correctly<ul>
<li><code>name</code>: the CUDA extension name. We import this name in our wrapper to access the cuda functions</li>
<li><code>sources</code>: the source files</li>
<li><code>include_dirs</code>: the header files</li>
<li><code>extra_compile_args</code>: The extra compiling flags. <code>&#123;&#39;cxx&#39;: [&#39;-O2&#39;], nvcc&#39;: [&#39;-O2&#39;]&#125;</code> is commonly used, which means using <code>-O2</code> optimization level when compiling</li>
</ul>
</li>
<li><code>py_modules</code>: The Python modules needed for the package, which is our wrapper, <code>mylinearops</code>. In most cases, the wrapper module has the same name as the overall package name. (<code>&#39;mylinearops.mylinearops&#39;</code> stands for <code>&#39;mylinearops/mylinearops.py&#39;</code>)</li>
<li><code>cmdclass</code>: When building the PyTorch CUDA extension, we always pass in this: <code>&#123;&#39;build_ext&#39;: BuildExtension&#125;</code></li>
</ul>
<h2 id="Building"><a href="#Building" class="headerlink" title="Building"></a>Building</h2><p>Then, we can build the package. We first activate the conda environment where we want to install in:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate &lt;target_env&gt;</span><br></pre></td></tr></table></figure>

<p>Then run:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> &lt;proj_root&gt;</span><br><span class="line">python setup.py install</span><br></pre></td></tr></table></figure>

<p><strong>Note:</strong> Don’t run <code>pip install .</code>, otherwise your python module will not be successfully installed, at least in my case.</p>
<p>It may take some time to compile it. If the building process ends up with some error message, go and fix them. If it finally displays something as “successfully installed mylinearops”, then you are ready to go.</p>
<p>To check if the installation is successful, we can try to import it:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ python</span><br><span class="line">Python 3.9.15 (main, Nov 24 2022, 14:31:59) </span><br><span class="line">[GCC 11.2.0] :: Anaconda, Inc. on linux</span><br><span class="line">Type <span class="string">&quot;help&quot;</span>, <span class="string">&quot;copyright&quot;</span>, <span class="string">&quot;credits&quot;</span> or <span class="string">&quot;license&quot;</span> <span class="keyword">for</span> more information.</span><br><span class="line">&gt;&gt;&gt; import mylinearops</span><br><span class="line">&gt;&gt;&gt; <span class="built_in">dir</span>(mylinearops)</span><br><span class="line">[<span class="string">&#x27;LinearLayer&#x27;</span>, <span class="string">&#x27;__builtins__&#x27;</span>, <span class="string">&#x27;__cached__&#x27;</span>, <span class="string">&#x27;__doc__&#x27;</span>, <span class="string">&#x27;__file__&#x27;</span>, <span class="string">&#x27;__loader__&#x27;</span>, <span class="string">&#x27;__name__&#x27;</span>, <span class="string">&#x27;__package__&#x27;</span>, <span class="string">&#x27;__path__&#x27;</span>, <span class="string">&#x27;__spec__&#x27;</span>, <span class="string">&#x27;linearop&#x27;</span>, <span class="string">&#x27;matmul&#x27;</span>, <span class="string">&#x27;mylinearops&#x27;</span>]</span><br><span class="line">&gt;&gt;&gt; </span><br></pre></td></tr></table></figure>

<p>Further testing will be mentioned in the next subsection.</p>
<h1 id="Module-Testing"><a href="#Module-Testing" class="headerlink" title="Module Testing"></a>Module Testing</h1><p>We will test the forward and backward of <code>matmul</code> and <code>LinearLayer</code> calculations respectively. To verify the answer, we’ll compare our answer with the PyTorch’s implementation or with <code>torch.autograd.gradcheck</code>. To increase the accuracy, we recommend to use <code>double</code> (<code>torch.float64</code>) type instead of <code>float</code> (<code>torch.float32</code>).</p>
<p>For tensors: create with argument <code>dtype=torch.float64</code>.</p>
<p>For modules: a good way is to use <code>model.double()</code> to convert all the parameters and buffers to <code>double</code>. </p>
<h2 id="forward"><a href="#forward" class="headerlink" title="forward"></a>forward</h2><p>A typical method is to use <code>torch.allclose</code> to verify if two tensors are close to each other. We can create the reference answer by PyTorch’s implementation.</p>
<ul>
<li><code>matmul</code>:</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> mylinearops</span><br><span class="line"></span><br><span class="line">A = torch.randn(<span class="number">20</span>, <span class="number">30</span>, dtype=torch.float64).cuda().requires_grad_()</span><br><span class="line">B = torch.randn(<span class="number">30</span>, <span class="number">40</span>, dtype=torch.float64).cuda().requires_grad_()</span><br><span class="line"></span><br><span class="line">res_my = mylinearops.matmul(A, B)</span><br><span class="line">res_torch = torch.matmul(A, B)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.allclose(res_my, res_torch))</span><br></pre></td></tr></table></figure>

<ul>
<li><code>LinearLayer</code>:</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> mylinearops</span><br><span class="line"></span><br><span class="line">A = torch.randn(<span class="number">40</span>, <span class="number">30</span>, dtype=torch.float64).cuda().requires_grad_() * <span class="number">100</span></span><br><span class="line">linear = mylinearops.LinearLayer(<span class="number">30</span>, <span class="number">50</span>).cuda().double()</span><br><span class="line"></span><br><span class="line">res_my = linear(A)</span><br><span class="line">res_torch = torch.matmul(A, linear.weight) + linear.bias</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.allclose(res_my, res_torch))</span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">max</span>(torch.<span class="built_in">abs</span>(res_my - res_torch)))</span><br></pre></td></tr></table></figure>

<p>It is worthwhile that sometimes, because of the floating number error, the answer from PyTorch is not consistent with the answer from our implementations. We have three methods:</p>
<ol>
<li>Pass <code>atol=1e-5, rtol=1e-5</code> into the <code>torch.allclose</code> to increase the tolerance level.</li>
<li><strong>[Not very recommended]</strong> We can observe the absolute error by <code>torch.max(torch.abs(res_my - res_torch))</code> for reference. If the result is merely 0.01 ~ 0.1, That would be OK in most cases.</li>
</ol>
<h2 id="backward"><a href="#backward" class="headerlink" title="backward"></a>backward</h2><p>For backward calculation, we can use <code>torch.autograd.gradcheck</code> to verify the result. If some tensors are only <code>float</code>, an warning will occur:</p>
<blockquote>
<p>……&#x2F;torch&#x2F;autograd&#x2F;gradcheck.py:647: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. </p>
</blockquote>
<p>So it is recommended to use the <code>double</code> type. Otherwise the check will likely fail.</p>
<ul>
<li><code>matmul</code>:</li>
</ul>
<p>As mentioned above, for pure calculation functions, we can assign all tensor as <code>double</code> (<code>torch.float64</code>) type. We are ready to go: </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> mylinearops</span><br><span class="line"></span><br><span class="line">A = torch.randn(<span class="number">20</span>, <span class="number">30</span>, dtype=torch.float64).cuda().requires_grad_()</span><br><span class="line">B = torch.randn(<span class="number">30</span>, <span class="number">40</span>, dtype=torch.float64).cuda().requires_grad_()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.autograd.gradcheck(mylinearops.matmul, (A, B)))    <span class="comment"># pass</span></span><br></pre></td></tr></table></figure>

<ul>
<li><code>LinearLayer</code>:</li>
</ul>
<p>As mentioned above, we can use <code>model.double()</code>. We are ready to go: </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> mylinearops</span><br><span class="line"></span><br><span class="line"><span class="comment">## CHECK for Linear Layer with bias ##</span></span><br><span class="line">A = torch.randn(<span class="number">40</span>, <span class="number">30</span>, dtype=torch.float64).cuda().requires_grad_()</span><br><span class="line">linear = mylinearops.LinearLayer(<span class="number">30</span>, <span class="number">40</span>).cuda().double()</span><br><span class="line"><span class="built_in">print</span>(torch.autograd.gradcheck(linear, (A,)))    <span class="comment"># pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## CHECK for Linear Layer without bias ##</span></span><br><span class="line">A = torch.randn(<span class="number">40</span>, <span class="number">30</span>, dtype=torch.float64).cuda().requires_grad_()</span><br><span class="line">linear_nobias = mylinearops.LinearLayer(<span class="number">30</span>, <span class="number">40</span>, bias=<span class="literal">False</span>).cuda().double()</span><br><span class="line"><span class="built_in">print</span>(torch.autograd.gradcheck(linear_nobias, (A,)))    <span class="comment"># pass</span></span><br></pre></td></tr></table></figure>



<h1 id="Full-Example"><a href="#Full-Example" class="headerlink" title="Full Example"></a>Full Example</h1><p>Now, we use our linear module to build a three layer classic linear model <code>[784, 256, 10]</code>to classify the MNIST digits. See the <code>examples/main.py</code> file. </p>
<p>Just as the <code>nn.Linear</code>, we create the model by:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MLP, self).__init__()</span><br><span class="line">        self.linear1 = mylinearops.LinearLayer(<span class="number">784</span>, <span class="number">256</span>, bias=<span class="literal">True</span>)<span class="comment">#.cuda()</span></span><br><span class="line">        self.linear2 = mylinearops.LinearLayer(<span class="number">256</span>, <span class="number">256</span>, bias=<span class="literal">True</span>)<span class="comment">#.cuda()</span></span><br><span class="line">        self.linear3 = mylinearops.LinearLayer(<span class="number">256</span>, <span class="number">10</span>, bias=<span class="literal">True</span>)<span class="comment">#.cuda()</span></span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        <span class="comment"># self.softmax = nn.Softmax(dim=1)</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">784</span>)</span><br><span class="line">        x = self.relu(self.linear1(x))</span><br><span class="line">        x = self.relu(self.linear2(x))</span><br><span class="line">        <span class="comment"># x = self.softmax(self.linear3(x))</span></span><br><span class="line">        x = self.linear3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<p>After writing some basic things, we can run our model: <code>python examples/tests.py</code>.</p>
<p>We also build the model by PyTorch’s <code>nn.Linear</code>. The result logging is:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mylinearops</span></span><br><span class="line">...</span><br><span class="line">Epoch: [10/10], Step: [100/468], Loss: 0.0417, Acc: 0.9844</span><br><span class="line">Epoch: [10/10], Step: [200/468], Loss: 0.0971, Acc: 0.9609</span><br><span class="line">Epoch: [10/10], Step: [300/468], Loss: 0.0759, Acc: 0.9766</span><br><span class="line">Epoch: [10/10], Step: [400/468], Loss: 0.0777, Acc: 0.9766</span><br><span class="line">Time: 23.4661s</span><br><span class="line"></span><br><span class="line"><span class="comment"># torch</span></span><br><span class="line">...</span><br><span class="line">Epoch: [10/10], Step: [100/468], Loss: 0.1048, Acc: 0.9688</span><br><span class="line">Epoch: [10/10], Step: [200/468], Loss: 0.0412, Acc: 0.9844</span><br><span class="line">Epoch: [10/10], Step: [300/468], Loss: 0.0566, Acc: 0.9688</span><br><span class="line">Epoch: [10/10], Step: [400/468], Loss: 0.0217, Acc: 0.9922</span><br><span class="line">Time: 26.5896s</span><br></pre></td></tr></table></figure>

<p>It is surprising that our implementation is even faster than the torch’s one. (But relax, after trying for some repetitions, we find ours is just as fast as the torch’s one). This is because the data scale is relatively small, the computation proportion is small. When the data scale is larger, ours may be slower than torch’s. </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/02/Pytorch-Practical-Basics-8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Future">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Future's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/02/Pytorch-Practical-Basics-8/" class="post-title-link" itemprop="url">PyTorch Practical Hand-Written Modules Basics 8--Linear Layer Ops on C++/CUDA</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-08-02 12:03:42" itemprop="dateCreated datePublished" datetime="2023-08-02T12:03:42+08:00">2023-08-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-08-06 19:13:54" itemprop="dateModified" datetime="2023-08-06T19:13:54+08:00">2023-08-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>In the section 6 to 9, we’ll investigate how to use <code>torch.autograd.Function</code> to implement the hand-written operators. The tentative outline is:</p>
<ul>
<li><input checked disabled type="checkbox"> In the section 6, we talk about the basics of <code>torch.autograd.Function</code>. <strong>The operators defined by <code>torch.autograd.Function</code> can be automatically back-propagated.</strong></li>
<li><input checked disabled type="checkbox"> In the last section (7), we’ll talk about <strong>mathematic derivation</strong> for the “linear layer” operator. </li>
<li><input checked disabled type="checkbox"> In this section (8), we talk about <strong>writing C++ CUDA extension</strong> for the “linear layer” operator. </li>
<li><input disabled type="checkbox"> In the section 9, we talk details about building the extension to a module, as well as testing. Then we’ll conclude the things we’ve done so far.</li>
</ul>
<p><strong>Note:</strong></p>
<ul>
<li>This blog is written with following reference: <ul>
<li>PyTorch official tutorial about CUDA extension: <a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/advanced/cpp_extension.html">website</a>.</li>
<li>YouTube video about writing CUDA extension: <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=l_Rpk6CRJYI">video</a>, <a target="_blank" rel="noopener" href="https://github.com/kwea123/pytorch-cppcuda-tutorial">code</a>.</li>
</ul>
</li>
<li>For how to write CUDA code, you can follow <a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">official documentation</a>, <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/34587739">blogs</a> (In Chinese). You can search by yourself for English tutorials and video tutorials.</li>
<li>This blog only talk some important points in the matrix multiplication example. Code are picked by pieces for illustration. Whole code is at: <a target="_blank" rel="noopener" href="https://github.com/I-am-Future/PyTorch-Linear-Operator-CUDA">code</a>.</li>
</ul>
<h1 id="Overall-Structure"><a href="#Overall-Structure" class="headerlink" title="Overall Structure"></a>Overall Structure</h1><p>The general structure for our PyTorch C++ &#x2F; CUDA extension looks like following:</p>
<img src="/2023/08/02/Pytorch-Practical-Basics-8/cudaops-struct.png" alt="cudaops-struct" style="zoom:50%;">

<p>We mainly have three kinds of file: Library interface, Core code on CPU, and Core code on GPU. Let’s explain them in detail:</p>
<ul>
<li><p>Library interface (.cpp)</p>
<ul>
<li>Contains Functions Interface for Python to call. These functions usually have Tensor input and Tensor return value.</li>
<li>Contains a standard pybind declaration, since our extension uses pybind to bind the C++ functions for Python. It indicates which functions are needed to be bound.</li>
</ul>
</li>
<li><p>Core code on CPU (.cpp)</p>
<ul>
<li>Contains core function to do the calculation. </li>
<li>Contains wrapper for the core function, serves to creating the result tensor, checking the input shape, etc.</li>
</ul>
</li>
<li><p>Core code on GPU (.cu)</p>
<ul>
<li>Contains CUDA kernel function <code>__global__</code> to do the parallel calculation. </li>
<li>Contains wrapper for the core function, serves to creating the result tensor, checking the input shape, setting the launch configs, launching the kernel, etc.</li>
</ul>
</li>
</ul>
<p>Then, after we finishing the code, we can use Python build tools to compile the code into a static object library (.so file). Then, we can import them normally in the Python side. We can call the functions we declared in library interface by pybind11.</p>
<p>In our example <a target="_blank" rel="noopener" href="https://github.com/I-am-Future/PyTorch-Linear-Operator-CUDA">code</a>, we don’t provide code for CPU calculation. We only support GPU. So we only have two files (<code>src/linearops.cpp</code> and <code>src/addmul_kernel.cu</code>)</p>
<h1 id="Pybind-Interface"><a href="#Pybind-Interface" class="headerlink" title="Pybind Interface"></a>Pybind Interface</h1><p>This is the <code>src/linearops.cpp</code> file in our repo. </p>
<h2 id="1-Utils-function"><a href="#1-Utils-function" class="headerlink" title="1. Utils function"></a>1. Utils function</h2><p>We usually defines some utility macro functions in our code. They are in the <code>include/utils.h</code> header file. </p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// PyTorch CUDA Utils</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x <span class="string">&quot; must be a CUDA tensor&quot;</span>)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x <span class="string">&quot; must be contiguous&quot;</span>)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Kernel Config Utils</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> DIV_CEIL(a, b) (((a) + (b) - 1) / (b))</span></span><br></pre></td></tr></table></figure>

<p>The third macro will call first two macros, which are used to make sure the tenser is on the CUDA devices and is contiguous. </p>
<p>The last macro performs ceil division, which are often used in setting the CUDA kernel launch configurations. </p>
<h2 id="2-Interface-functions"><a href="#2-Interface-functions" class="headerlink" title="2. Interface functions"></a>2. Interface functions</h2><p>Benefited by pybind, we can simply define functions in C++ and use them in Python. A function looks like</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">torch::Tensor <span class="title">func</span><span class="params">(torch::Tensor a, torch::Tensor b, <span class="type">int</span> c)</span></span>&#123;</span><br><span class="line">    torch::Tensor res;</span><br><span class="line">    ......</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>is relatively same as the Python function below. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">func</span>(<span class="params">a: torch.Tensor, b: torch.Tensor, c: <span class="built_in">int</span></span>) -&gt; torch.Tensor</span><br><span class="line">	res = ... <span class="comment"># torch.Tensor</span></span><br><span class="line">    ......</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>

<p>Then, we can define our matrix multiplication interface as below. Note that we need to implement both the forward and backward functions!</p>
<ul>
<li>forward</li>
</ul>
<p>Check the input, input size, and then call the CUDA function wrapper.</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">torch::Tensor <span class="title">matmul_forward</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor &amp;A, </span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor &amp;B)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(A);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(B);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(A.<span class="built_in">size</span>(<span class="number">1</span>) == B.<span class="built_in">size</span>(<span class="number">0</span>), <span class="string">&quot;matmul_fast_forward: shape mismatch&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">matmul_cuda</span>(A, B);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>backward</li>
</ul>
<p>Also check the input, input size, and then call the CUDA function wrapper. Note that we calculate the backward of <code>A * B = C</code> for input matrix A, B in two different function. So that when someday we don’t need to calculate the gradient of A, we can just pass it. </p>
<p>The gradient function derivation is mentioned in last section <a target="_blank" rel="noopener" href="https://i-am-future.github.io/2023/08/01/Pytorch-Practical-Basics-7/#Matrix-multiplication-backward">here</a>. </p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* Backward for A gradient */</span></span><br><span class="line"><span class="function">torch::Tensor <span class="title">matmul_dA_backward</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor &amp;grad_output, </span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor &amp;A, </span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor &amp;B)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(grad_output);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(B);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// dL/dB = dL/dY * B^T</span></span><br><span class="line">    <span class="keyword">auto</span> grad_A = <span class="built_in">matmul_cuda</span>(grad_output, <span class="built_in">transpose_cuda</span>(B));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grad_A;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Backward for B gradient */</span></span><br><span class="line"><span class="function">torch::Tensor <span class="title">matmul_dB_backward</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor &amp;grad_output, </span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor &amp;A, </span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor &amp;B)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(grad_output);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(A);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// dL/dB = A^T * dL/dY</span></span><br><span class="line">    <span class="keyword">auto</span> grad_B = <span class="built_in">matmul_cuda</span>(<span class="built_in">transpose_cuda</span>(A), grad_output);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grad_B;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="3-Binding"><a href="#3-Binding" class="headerlink" title="3. Binding"></a>3. Binding</h2><p>At the last of the <code>src/linearops.cpp</code>, we use the following code to bind the functions. The first string is the function name in Python side, the second is a function pointer to the function be called, and the last is the docstring for that function in Python side. </p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">PYBIND11_MODULE</span>(TORCH_EXTENSION_NAME, m) &#123;</span><br><span class="line">    ......</span><br><span class="line">    m.<span class="built_in">def</span>(<span class="string">&quot;matmul_forward&quot;</span>, &amp;matmul_forward, <span class="string">&quot;Matmul forward&quot;</span>);</span><br><span class="line">    m.<span class="built_in">def</span>(<span class="string">&quot;matmul_dA_backward&quot;</span>, &amp;matmul_dA_backward, <span class="string">&quot;Matmul dA backward&quot;</span>);</span><br><span class="line">    m.<span class="built_in">def</span>(<span class="string">&quot;matmul_dB_backward&quot;</span>, &amp;matmul_dB_backward, <span class="string">&quot;Matmul dB backward&quot;</span>);</span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h1 id="CUDA-wrapper"><a href="#CUDA-wrapper" class="headerlink" title="CUDA wrapper"></a>CUDA wrapper</h1><p>This is the <code>src/addmul_kernel.cu</code> file in our repo. </p>
<p>The wrapper for matrix multiplication looks like below: </p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">torch::Tensor <span class="title">matmul_cuda</span><span class="params">(torch::Tensor A, torch::Tensor B)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 1. Get metadata</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> m = A.<span class="built_in">size</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> n = A.<span class="built_in">size</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> p = B.<span class="built_in">size</span>(<span class="number">1</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 2. Create output tensor</span></span><br><span class="line">    <span class="keyword">auto</span> result = torch::<span class="built_in">empty</span>(&#123;m, p&#125;, A.<span class="built_in">options</span>());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. Set launch configuration</span></span><br><span class="line">    <span class="type">const</span> dim3 blockSize = <span class="built_in">dim3</span>(BLOCK_SIZE, BLOCK_SIZE);</span><br><span class="line">    <span class="type">const</span> dim3 gridSize = <span class="built_in">dim3</span>(<span class="built_in">DIV_CEIL</span>(m, BLOCK_SIZE), <span class="built_in">DIV_CEIL</span>(p, BLOCK_SIZE));</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// 4. Call the cuda kernel launcher</span></span><br><span class="line">    <span class="built_in">AT_DISPATCH_FLOATING_TYPES</span>(A.<span class="built_in">type</span>(), <span class="string">&quot;matmul_cuda&quot;</span>, </span><br><span class="line">    ([&amp;] &#123;</span><br><span class="line">        matmul_fw_kernel&lt;<span class="type">scalar_t</span>&gt;&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(</span><br><span class="line">            A.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">            B.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">            result.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">            m, p</span><br><span class="line">        );</span><br><span class="line">    &#125;));</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 5. Return the value</span></span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>And here, we’ll talk in details: </p>
<h2 id="1-Get-metadata"><a href="#1-Get-metadata" class="headerlink" title="1. Get metadata"></a>1. Get metadata</h2><p>Just as the tensor in PyTorch, we can use <code>Tensor.size(0)</code> to axis the shape size of dimension 0.</p>
<p>Note that we have checked the dimension match at the interface side, we don’t need to check it here.</p>
<h2 id="2-Create-output-tensor"><a href="#2-Create-output-tensor" class="headerlink" title="2. Create output tensor"></a>2. Create output tensor</h2><p>We can do operation in-place or create a new tensor for output. Use the following code to create a tensor shape <code>m x p</code>, with same dtype &#x2F; device as <code>A</code>. </p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> result = torch::<span class="built_in">empty</span>(&#123;m, p&#125;, A.<span class="built_in">options</span>());</span><br></pre></td></tr></table></figure>

<p>In other situations, when we want special dtype &#x2F; device, we can follow the declaration as below:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch::<span class="built_in">empty</span>(&#123;m, p&#125;, torch::<span class="built_in">dtype</span>(torch::kInt32).<span class="built_in">device</span>(feats.<span class="built_in">device</span>()))</span><br></pre></td></tr></table></figure>

<p><code>torch.empty</code> only allocate the memory, but not initialize the entries to 0. Because sometimes, we’ll fill into the result tensors in the kernel functions, so it is not necessary to initialize as 0. </p>
<h2 id="3-Set-launch-configuration"><a href="#3-Set-launch-configuration" class="headerlink" title="3. Set launch configuration"></a>3. Set launch configuration</h2><p>You should know some basic CUDA knowledges before understand this part. Basically here, we are setting the launch configuration based on the input matrix size. We are using the macro functions defined before.</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">const</span> dim3 blockSize = <span class="built_in">dim3</span>(BLOCK_SIZE, BLOCK_SIZE);</span><br><span class="line"><span class="type">const</span> dim3 gridSize = <span class="built_in">dim3</span>(<span class="built_in">DIV_CEIL</span>(m, BLOCK_SIZE), <span class="built_in">DIV_CEIL</span>(p, BLOCK_SIZE));</span><br></pre></td></tr></table></figure>

<p>We set each thread block size to <code>16 x 16</code>. Then, we set the number of blocks according to the input size. </p>
<h2 id="4-Call-the-cuda-kernel-launcher"><a href="#4-Call-the-cuda-kernel-launcher" class="headerlink" title="4. Call the cuda kernel launcher"></a>4. Call the cuda kernel launcher</h2><p>Unlike normal cuda programs, we use <code>ATen</code>‘s function to start the kernel. This is a standard operation, and you can copy-paste it to anywhere. </p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AT_DISPATCH_FLOATING_TYPES</span>(A.<span class="built_in">type</span>(), <span class="string">&quot;matmul_cuda&quot;</span>, </span><br><span class="line">                           ([&amp;] &#123;</span><br><span class="line">                               matmul_fw_kernel&lt;<span class="type">scalar_t</span>&gt;&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(</span><br><span class="line">                                   A.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">                                   B.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">                                   result.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">                                   m, p</span><br><span class="line">                               );</span><br><span class="line">                           &#125;));</span><br></pre></td></tr></table></figure>

<ul>
<li><p>This function is named <code>AT_DISPATCH_FLOATING_TYPES</code>, meaning the inside kernel will support floating types, i.e., <code>float (32bit)</code> and <code>double (64bit)</code>.   For <code>float16</code>, you can use <code>AT_DISPATCH_ALL_TYPES_AND_HALF</code>. For int (<code>int (32bit)</code> and <code>long long (64 bit)</code> and more, use <code>AT_DISPATCH_INTEGRAL_TYPES</code>. </p>
</li>
<li><p>The first argument <code>A.type()</code>, indicates the actual chosen type in the runtime. </p>
</li>
<li><p>The second argument <code>matmul_cuda</code> can be used for error reporting.</p>
</li>
<li><p>The last argument, which is a lambda function, is the actual function to be called. Basically in this function, we start the kernel by the following statement:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">matmul_fw_kernel&lt;<span class="type">scalar_t</span>&gt;&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(</span><br><span class="line">A.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">B.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">result.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">m, p</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<ul>
<li><code>matmul_fw_kernel</code> is the kernel function name.</li>
<li><code>&lt;scalar_t&gt;</code> is the template parameter, will be replaced to all possible types in the outside <code>AT_DISPATCH_FLOATING_TYPES</code>.</li>
<li><code>&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;</code> passed in the launch configuration</li>
<li>In the parameter list, if that is a <code>Tensor</code>, we should pass in the packed accessor, which convenient indexing operation in the kernel.<ul>
<li><code>&lt;scalar_t&gt;</code> is the template parameter.</li>
<li><code>2</code> means the <code>Tensor.ndimension=2</code>.</li>
<li><code>torch::RestrictPtrTraits</code> means the pointer (tensor memory) would not not overlap. It enables some optimization. Usually not change.</li>
<li><code>size_t</code> indicates the index type. Usually not change.</li>
</ul>
</li>
<li>if the parameter is integer <code>m, p</code>, just pass it in as normal.</li>
</ul>
</li>
</ul>
<h2 id="5-Return-the-value"><a href="#5-Return-the-value" class="headerlink" title="5. Return the value"></a>5. Return the value</h2><p>If we have more then one return value, we can set the return type to <code>std::vector&lt;torch::Tensor&gt;</code>. Then we return with <code>&#123;xxx, yyy&#125;</code>. </p>
<h1 id="CUDA-kernel"><a href="#CUDA-kernel" class="headerlink" title="CUDA kernel"></a>CUDA kernel</h1><p>This is the <code>src/addmul_kernel.cu</code> file in our repo. </p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> <span class="type">scalar_t</span>&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">matmul_fw_kernel</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::PackedTensorAccessor&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt; A,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::PackedTensorAccessor&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt; B,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::PackedTensorAccessor&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt; result,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> m, <span class="type">const</span> <span class="type">int</span> p</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> row = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> col = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (row &gt;= m || col &gt;= p) <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">scalar_t</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; A.<span class="built_in">size</span>(<span class="number">1</span>); i++) &#123;</span><br><span class="line">        sum += A[row][i] * B[i][col];</span><br><span class="line">    &#125;</span><br><span class="line">    result[row][col] = sum;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>We define it as a template function <code>template &lt;typename scalar_t&gt;</code>, so that our kernel function can support different type of input tensor. </li>
<li>Usually we’ll set the input <code>PackedTensorAccessor</code> with <code>const</code>, to avoid some unexpected modification on them. </li>
<li>The main code is just a simple CUDA matrix multiplication example. This is very common, you can search online for explanation.</li>
</ul>
<h2 id="Ending"><a href="#Ending" class="headerlink" title="Ending"></a>Ending</h2><p>That’s too much things in this section. In the next section, we’ll talk about how to write the <code>setup.py</code> to <strong>compile</strong> the code, letting it be a module for python. </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/01/Pytorch-Practical-Basics-7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Future">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Future's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/01/Pytorch-Practical-Basics-7/" class="post-title-link" itemprop="url">PyTorch Practical Hand-Written Modules Basics 7--Mathematics Derivation of Linear Layer</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-08-01 20:15:44" itemprop="dateCreated datePublished" datetime="2023-08-01T20:15:44+08:00">2023-08-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-08-07 14:51:03" itemprop="dateModified" datetime="2023-08-07T14:51:03+08:00">2023-08-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>In the section 6 to 9, we’ll investigate how to use <code>torch.autograd.Function</code> to implement the hand-written operators. The tentative outline is:</p>
<ul>
<li><input checked disabled type="checkbox"> In the last section (6), we talk about the basics of <code>torch.autograd.Function</code>. <strong>The operators defined by <code>torch.autograd.Function</code> can be automatically back-propagated.</strong></li>
<li><input checked disabled type="checkbox"> In this section (7), we’ll talk about <strong>mathematic derivation</strong> for the “linear layer” operator. </li>
<li><input disabled type="checkbox"> In the section 8, we talk about writing C++ CUDA extension for the “linear layer” operator. </li>
<li><input disabled type="checkbox"> In the section 9, we talk details about building the extension to a module, as well as testing. Then we’ll conclude the things we’ve done so far.</li>
</ul>
<p>The linear layer is defined by <code>Y = XW + b</code>. There is a matrix multiplication operation, and a bias addition. <strong>We’ll talk about their forward&#x2F;backward derivation separately.</strong> </p>
<p>(I feel sorry that currently there is some problem with displaying mathematics formula here. I’ll use screenshot first.)</p>
<h1 id="Matrix-multiplication-forward"><a href="#Matrix-multiplication-forward" class="headerlink" title="Matrix multiplication: forward"></a>Matrix multiplication: forward</h1><p>The matrix multiplication operation is a common operator. Each entry in the result matrix is a vector dot product of two input matrixes. The <code>(i, j)</code> entry of the result is from multiplying first matrix’s row <code>i</code> vector and the second matrix’s column <code>j</code> vector. From this property, we know that number of columns in the first matrix should equal to number of rows in the second matrix. The shape should be: <code>[m, n] x [n, r] -&gt; [m, r]</code>. For more details, see the figure illustration below. </p>
<img src="/2023/08/01/Pytorch-Practical-Basics-7/matmul-forward.png" alt="matmul-forward" style="zoom: 67%;">



<h1 id="Matrix-multiplication-backward"><a href="#Matrix-multiplication-backward" class="headerlink" title="Matrix multiplication: backward"></a>Matrix multiplication: backward</h1><p>First, we should know what’s the <strong>goal</strong> of the backward propagation. In the upstream side, we would get the gradient of the answer matrix, C. (The gradient matrix has the same size as its corresponding matrix. i.e., if C is in shape <code>[m, r]</code>, then gradient of C is shape <code>[m, r]</code> as well.) <strong>In this step, we should get the gradient of matrix A and B.</strong> <u>Gradient of matrix A and B are functions in terms of matrix A and B and gradient of C.</u> Specially, by chain rule, we can formulate it as</p>
<img src="/2023/08/01/Pytorch-Practical-Basics-7/matmul-backward-math1.png" alt="matmul-backward-math1" style="zoom:50%;">

<!-- $$
\frac{\partial L}{\partial A} = \frac{\partial L}{\partial C} \cdot \frac{\partial C}{\partial A} \\
(\frac{\partial L}{\partial C} \text{ is the gradient of C, and we need to figure out } \frac{\partial C}{\partial A}.)
$$-->

<p>To figure out the gradient of A, we should first investigate how an entry <code>A[i, j]</code> contribute to the entries in the result matrix C. See the figure below:</p>
<img src="/2023/08/01/Pytorch-Practical-Basics-7/matmul-backward.png" alt="matmul-backward" style="zoom:67%;">

<p>As shown above, entry <code>A[i, j]</code> multiplies with entries in row <code>j</code> of matrix B, contributing to the entries in row <code>i</code> of matrix C. We can write the gradient down in mathematics formula below:</p>
<img src="/2023/08/01/Pytorch-Practical-Basics-7/matmul-backward-math2.png" alt="matmul-backward-math2" style="zoom:50%;">

<!--$$
\begin{align}
\frac{\partial L}{\partial A_{i, j}} &= \sum_{k=1}^r \frac{\partial L}{\partial C_{i, k}} \cdot \frac{\partial C_{i, k}}{\partial A_{i, j}} \\
&= \sum_{k=1}^r \frac{\partial L}{\partial C_{i, k}} \cdot B_{j, k}\\
&= \sum_{k=1}^r  \frac{\partial L}{\partial C}_{i, k}  \cdot  B^T_{k,j} \\
& = (\frac{\partial L}{\partial C})_{\text{row i}}  \cdot  (B^T)_{\text{col j}}\\
\end{align}
$$-->

<p>The result above is the gradient for one entry <code>A[i, j]</code>, and it’s a vector dot product between a matrix’s row <code>i</code> and another matrix’s column <code>j</code>. Observing this formula, we can naturally extend it to the gradient of the whole matrix A, and that will be a matrix product. </p>
<img src="/2023/08/01/Pytorch-Practical-Basics-7/matmul-backward-math3.png" alt="matmul-backward-math3" style="zoom:50%;">

<!--$$
\frac{\partial L}{\partial A} = \frac{\partial L}{\partial C} \cdot B^T \\
\text{At last, by the same principle, we know } \frac{\partial L}{\partial B} = A^T \cdot \frac{\partial L}{\partial C}.
$$-->

<p>Recall “Gradient of matrix A and B are functions in terms of matrix A and B and gradient of C” we said before. Our derivation indeed show that, uh?</p>
<h1 id="Add-bias-forward"><a href="#Add-bias-forward" class="headerlink" title="Add bias: forward"></a>Add bias: forward</h1><p>First, we should note that when doing the addition, we’re actually adding the <code>XW</code> matrix (shape <code>[n, r]</code>) with the bias vector (shape <code>[r]</code>). Indeed we have a <strong>broadcasting</strong> here. <strong>We add bias to each row of the <code>XW</code> matrix.</strong></p>
<img src="/2023/08/01/Pytorch-Practical-Basics-7/addbias-forward.png" alt="addbias-forward.drawio" style="zoom:67%;">

<h1 id="Add-bias-backward"><a href="#Add-bias-backward" class="headerlink" title="Add bias: backward"></a>Add bias: backward</h1><p>With the similar principle, we can get the gradient for the bias as well. </p>
<img src="/2023/08/01/Pytorch-Practical-Basics-7/addbias-backward.jpg" alt="addbias-backward" style="zoom:67%;">

<p>For each entry in vector <code>b</code>, the gradient is:</p>
<img src="/2023/08/01/Pytorch-Practical-Basics-7/addbias-backward-math1.png" alt="addbias-backward-math1" style="zoom:50%;">

<!--$$
\begin{align}
\frac{\partial L}{\partial b_{i}} &= \sum_{k=1}^m \frac{\partial L}{\partial C_{k, i}} \cdot \frac{\partial C_{k, i}}{\partial b_i} \\
&= \sum_{k=1}^m \frac{\partial L}{\partial C_{k, i}} \cdot 1 \\
&= \sum_{k=1}^m \frac{\partial L}{\partial C_{k, i}} \\
\end{align}
$$-->
<p>That is, the gradient of entry <code>b_i</code> is the summation of the i-th column. In total, the gradient will be the summation along each column (i.e., axis&#x3D;0). In programming, we write:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grad_b = torch.<span class="built_in">sum</span>(grad_C, axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>



<h1 id="PyTorch-Verification"><a href="#PyTorch-Verification" class="headerlink" title="PyTorch Verification"></a>PyTorch Verification</h1><p>Finally, we can write a PyTorch program to verify if our derivation is correct: we will compare our calculated gradients with the gradients calculated by the PyTorch. If they are same, our derivation would be correct. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">A = torch.randn(<span class="number">10</span>, <span class="number">20</span>).requires_grad_()</span><br><span class="line">B = torch.randn(<span class="number">20</span>, <span class="number">30</span>).requires_grad_()</span><br><span class="line"></span><br><span class="line">res = torch.mm(A, B)</span><br><span class="line">res.retain_grad()</span><br><span class="line">res.<span class="built_in">sum</span>().backward()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.allclose(A.grad, torch.mm(res.grad, B.t()))) <span class="comment"># grad_A = grad_res * B^T</span></span><br><span class="line"><span class="built_in">print</span>(torch.allclose(B.grad, torch.mm(A.t(), res.grad))) <span class="comment"># grad_B = A^T * grad_res</span></span><br></pre></td></tr></table></figure>

<p>Finally, the output is:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>

<p>Which means that our derivation is correct. </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/30/Pytorch-Practical-Basics-6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Future">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Future's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/30/Pytorch-Practical-Basics-6/" class="post-title-link" itemprop="url">PyTorch Practical Hand-Written Modules Basics 6--torch.autograd.Function</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-07-30 20:31:20" itemprop="dateCreated datePublished" datetime="2023-07-30T20:31:20+08:00">2023-07-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-08-01 20:19:16" itemprop="dateModified" datetime="2023-08-01T20:19:16+08:00">2023-08-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>In this section (and also three sections in the future), we investigate how to use <code>torch.autograd.Function</code> to implement the hand-written operators. The tentative outline is:</p>
<ul>
<li><input checked disabled type="checkbox"> This section (6), we talk about the basics of <code>torch.autograd.Function</code>.</li>
<li><input disabled type="checkbox"> In the next section (7), we’ll talk about mathematic derivation for the “linear layer” operator. </li>
<li><input disabled type="checkbox"> In the section 8, we talk about writing C++ CUDA extension for the “linear layer” operator. </li>
<li><input disabled type="checkbox"> In the section 9, we talk details about building the extension to a module, as well as testing. Then we’ll conclude the things we’ve done so far.</li>
</ul>
<h1 id="Backgrounds"><a href="#Backgrounds" class="headerlink" title="Backgrounds"></a>Backgrounds</h1><p>This article mainly takes reference of the <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd">Official tutorial</a> and summarizes, explains the important points. </p>
<p>By defining an operator with <code>torch.autograd.Function</code> and implement its forward &#x2F; backward function, we can use this operator with other PyTorch built-in operators together. <strong>The operators defined by <code>torch.autograd.Function</code> can be automatically back-propagated.</strong></p>
<p>As mentioned in the tutorial, we should use the <code>torch.autograd.Function</code> in the following scenes:</p>
<ul>
<li>The computation is from other libraries, so they don’t support differential natively. We should explicitly define its backward functions. </li>
<li>The PyTorch’s implementation of an operator cannot take benefits from the parallelization. We utilize the PyTorch C++&#x2F;CUDA extension for the better performance.</li>
</ul>
<h1 id="Basic-Structure"><a href="#Basic-Structure" class="headerlink" title="Basic Structure"></a>Basic Structure</h1><p>The following is the basic structure of the Function:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Function</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearFunction</span>(<span class="title class_ inherited__">Function</span>):</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, input0, input1, ... , inputN</span>):</span><br><span class="line">        <span class="comment"># Save the input for the backward use. </span></span><br><span class="line">        ctx.save_for_backward(input1, input1, ... , inputN)</span><br><span class="line">        <span class="comment"># Calculate the output0, ... outputM given the inputs.</span></span><br><span class="line">        ......</span><br><span class="line">        <span class="keyword">return</span> output0, ... , outputM</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, grad_output0, ... , grad_outputM</span>):</span><br><span class="line">        <span class="comment"># Get and unpack the input for the backward use. </span></span><br><span class="line">        input0, input1, ... , inputN = ctx.saved_tensors</span><br><span class="line">        </span><br><span class="line">        grad_input0 = grad_input1 = grad_inputN = <span class="literal">None</span></span><br><span class="line">        <span class="comment"># These needs_input_grad records whether each input need to calculate the gradient. This can improve the efficiency.</span></span><br><span class="line">        <span class="keyword">if</span> ctx.needs_input_grad[<span class="number">0</span>]:</span><br><span class="line">            grad_input0 = ...  <span class="comment"># backward calculation</span></span><br><span class="line">        <span class="keyword">if</span> ctx.needs_input_grad[<span class="number">1</span>]:</span><br><span class="line">            grad_input1 = ...  <span class="comment"># backward calculation</span></span><br><span class="line">        ......</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> grad_input0, grad_input1, grad_inputN</span><br></pre></td></tr></table></figure>

<ol>
<li><p>The <code>forward</code> and <code>backward</code> functions are <code>staticmethod</code>. The forward function is <code>o0, ..., oM = forward(i0, ..., iN)</code>, calculate the output0 ~ outputM by the input0 ~ inputN. Then the backward function is <code>g_i0, ..., g_iN = backward(g_o0, ..., g_M)</code>, calculate the gradient of input0 ~ gradient of inputM by the gradient of output0 ~ outputN.</p>
</li>
<li><p>Since forward and backward are merely functions. We need store the input tensors to the <code>ctx</code> in the forward pass, so that we can get them in the backward functions. See <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/notes/extending.html#combining-forward-context">here</a> to use the alternative way to define <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function"><code>Function</code></a>.</p>
</li>
<li><p><code>ctx.needs_input_grad</code> is a tuple of Booleans. It records whether one input needs to calculate the gradient or not. Therefore, we can save computation resources if one tensor doesn’t need gradients. In that case, the return value of backward function for that tensor is <code>None</code>.</p>
</li>
</ol>
<h1 id="Use-it"><a href="#Use-it" class="headerlink" title="Use it"></a>Use it</h1><h2 id="Pure-functions"><a href="#Pure-functions" class="headerlink" title="Pure functions"></a>Pure functions</h2><p>After defining the class, we can use the <code>.apply</code> method to use it. Simply</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Option 1: alias</span></span><br><span class="line">linear = LinearFunction.apply</span><br></pre></td></tr></table></figure>
<p>or, </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Option 2: wrap in a function, to support default args and keyword args.</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linear</span>(<span class="params"><span class="built_in">input</span>, weight, bias=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">return</span> LinearFunction.apply(<span class="built_in">input</span>, weight, bias)</span><br></pre></td></tr></table></figure>

<p>Then call as</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output = linear(<span class="built_in">input</span>, weight, bias) <span class="comment"># input, weight, bias are all tensors!</span></span><br></pre></td></tr></table></figure>

<h2 id="nn-Module"><a href="#nn-Module" class="headerlink" title="nn.Module"></a>nn.Module</h2><p>In most cases, the weight and bias are parameters that are trainable during the process. We can further wrap this linear function to a Linear module: </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Linear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_features, output_features, bias=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.input_features = input_features</span><br><span class="line">        self.output_features = output_features</span><br><span class="line"></span><br><span class="line">        <span class="comment"># nn.Parameters require gradients by default.</span></span><br><span class="line">        self.weight = nn.Parameter(torch.empty(output_features, input_features))</span><br><span class="line">        <span class="keyword">if</span> bias:</span><br><span class="line">            self.bias = nn.Parameter(torch.empty(output_features))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># You should always register all possible parameters, but the</span></span><br><span class="line">            <span class="comment"># optional ones can be None if you want.</span></span><br><span class="line">            self.register_parameter(<span class="string">&#x27;bias&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Not a very smart way to initialize weights</span></span><br><span class="line">        nn.init.uniform_(self.weight, -<span class="number">0.1</span>, <span class="number">0.1</span>)</span><br><span class="line">        <span class="keyword">if</span> self.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            nn.init.uniform_(self.bias, -<span class="number">0.1</span>, <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="comment"># See the autograd section for explanation of what happens here.</span></span><br><span class="line">        <span class="keyword">return</span> LinearFunction.apply(<span class="built_in">input</span>, self.weight, self.bias)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">extra_repr</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># (Optional)Set the extra information about this module. You can test</span></span><br><span class="line">        <span class="comment"># it by printing an object of this class.</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;input_features=&#123;&#125;, output_features=&#123;&#125;, bias=&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">            self.input_features, self.output_features, self.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">        )</span><br></pre></td></tr></table></figure>

<p>As mentioned in section 3, 4 of this series, the weight and bias should be <code>nn.Parameter</code> so that they can be recognized correctly. Then we initialize the weights with random variables. </p>
<p>In the <code>forward</code> functions, we use the defined <code>LinearFunction.apply</code> functions. The backward process will be automatically done just as other PyTorch modules. </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/26/An-Obscure-RuntimeError-for-CUDA-error-out-of-memory/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Future">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Future's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/26/An-Obscure-RuntimeError-for-CUDA-error-out-of-memory/" class="post-title-link" itemprop="url">An Obscure RuntimeError for CUDA error: out of memory</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-07-26 21:02:18" itemprop="dateCreated datePublished" datetime="2023-07-26T21:02:18+08:00">2023-07-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-07-28 13:12:30" itemprop="dateModified" datetime="2023-07-28T13:12:30+08:00">2023-07-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Problem-Solving/" itemprop="url" rel="index"><span itemprop="name">Problem Solving</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h2><p>Today when I was running PyTorch scripts, I met a strange problem: </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.rand(<span class="number">2</span>, <span class="number">2</span>).to(<span class="string">&#x27;cuda:1&#x27;</span>)</span><br><span class="line">......</span><br><span class="line">torch.cuda.synchronize()</span><br></pre></td></tr></table></figure>

<p>but result in the following error:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">  File <span class="string">&quot;....../test.py&quot;</span>, line 67, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    torch.cuda.synchronize()</span><br><span class="line">  File <span class="string">&quot;....../miniconda3/envs/py39/lib/python3.9/site-packages/torch/cuda/__init__.py&quot;</span>, line 495, <span class="keyword">in</span> synchronize</span><br><span class="line">    <span class="built_in">return</span> torch._C._cuda_synchronize()</span><br><span class="line">RuntimeError: CUDA error: out of memory</span><br></pre></td></tr></table></figure>

<p>but It’s clear that GPU1 has enough memory (we only need to allocate 16 bytes!):</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">|===============================+======================+======================|</span><br><span class="line">|   0  NVIDIA GeForce ...  Off  | 00000000:1A:00.0 Off |                  N/A |</span><br><span class="line">| 75%   73C    P2   303W / 350W |  24222MiB / 24268MiB |     64%      Default |</span><br><span class="line">|                               |                      |                  N/A |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line">|   1  NVIDIA GeForce ...  Off  | 00000000:1B:00.0 Off |                  N/A |</span><br><span class="line">| 90%   80C    P2   328W / 350W |  15838MiB / 24268MiB |     92%      Default |</span><br><span class="line">|                               |                      |                  N/A |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br></pre></td></tr></table></figure>

<p>And normally, when we fail to allocate the memory for tensors, the error is:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 6.00 GiB total capacity; 4.54 GiB already allocated; 14.94 MiB free; 4.64 GiB reserved <span class="keyword">in</span> total by PyTorch)</span><br></pre></td></tr></table></figure>

<p>But our error message is much “simpler”. So what happened?</p>
<h2 id="Possible-Answer"><a href="#Possible-Answer" class="headerlink" title="Possible Answer"></a>Possible Answer</h2><p>This confused me for some time. According to this <a target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/pytorch-cuda-synchronize-out-of-memory/9502">website</a>:</p>
<blockquote>
<p>When you initially do a CUDA call, it’ll create a cuda context and a THC context on the primary GPU (GPU0), and for that i think it needs 200 MB or so. That’s right at the edge of how much memory you have left. </p>
</blockquote>
<p>Surprisingly, in my case, GPU0 has occupied <code>24222MiB / 24268MiB</code> memory. So there is no more memory for the context. In addition, this makes sense that out error message is <code>RuntimeError: CUDA error: out of memory</code>, not the message that tensallocation failed. </p>
<h2 id="Possible-Solution"><a href="#Possible-Solution" class="headerlink" title="Possible Solution"></a>Possible Solution</h2><p>Set the <code>CUDA_VISIBLE_DEVICES</code> environment variable. We need to change primary GPU (GPU0) to other one.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do this before `import torch`</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;CUDA_VISIBLE_DEVICES&#x27;</span>] = <span class="string">&#x27;1&#x27;</span> <span class="comment"># set to what you like, e.g., &#x27;1,2,3,4,5,6,7&#x27;</span></span><br></pre></td></tr></table></figure>

<p>And then, our program is ready to go. </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/09/Pytorch-Practical-Basics-5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Future">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Future's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/09/Pytorch-Practical-Basics-5/" class="post-title-link" itemprop="url">PyTorch Practical Hand-Written Modules Basics 5--Implement a ResNet</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-07-09 09:47:01" itemprop="dateCreated datePublished" datetime="2023-07-09T09:47:01+08:00">2023-07-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-07-30 23:59:13" itemprop="dateModified" datetime="2023-07-30T23:59:13+08:00">2023-07-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>In this section, we’ll utilize knowledge we learnt from the last section (<a href>see here</a>), to implement a ResNet Network (<a href>paper</a>). </p>
<p>Note that we follow the original paper’s work. Our implementation is a simper version of the official <code>torchvision</code> implementation. (That is, we only implement the key structure, and the random weight init. We don’t consider dilation or other things). </p>
<h2 id="Preliminaries-Calculate-the-feature-map-size"><a href="#Preliminaries-Calculate-the-feature-map-size" class="headerlink" title="Preliminaries: Calculate the feature map size"></a>Preliminaries: Calculate the feature map size</h2><ul>
<li>Basic formula</li>
</ul>
<p>Given a convolution kernel with size <em>K</em>, and the padding <em>P</em>, the stride <em>S</em>, feature map size <em>I</em>, we can calculate the output size as <em>O</em> &#x3D; ( <em>I</em> - <em>K</em> + 2<em>P</em> ) &#x2F; <em>S</em> + 1. </p>
<ul>
<li>Corollary</li>
</ul>
<p>Based on the formula above, we know that when <em>S</em>&#x3D;1: </p>
<ol>
<li><em>K</em>&#x3D;3, <em>P</em>&#x3D;1 makes the input size and output size same.</li>
<li><em>K</em>&#x3D;1, <em>P</em>&#x3D;0 makes the input size and output size same.</li>
</ol>
<h2 id="Overall-Structure"><a href="#Overall-Structure" class="headerlink" title="Overall Structure"></a>Overall Structure</h2><p>The Table 1 in the original paper illustrates the overall structure of the ResNet:</p>
<img src="/2023/07/09/Pytorch-Practical-Basics-5/resnet_table1.png" alt="resnet_table1" style="zoom: 67%;">

<p>We know that from <code>conv2</code>, each layer consists of many blocks. And the blocks in <code>18, 34 layers</code> is different from blocks in <code>50, 101, 152 layers</code>. </p>
<p>We have several deductions: </p>
<ol>
<li>When the feature map enters the next layer, the first block need to do a down sampling operation. This is done by setting the one of the convolution kernel’s <code>stride=2</code>. </li>
<li>At other convolution kernels, the feature map’s size is same. So the convolution settings is same as the one referred in Preliminaries.</li>
</ol>
<h2 id="Basic-Block-Implementation"><a href="#Basic-Block-Implementation" class="headerlink" title="Basic Block Implementation"></a>Basic Block Implementation</h2><p>The basic block’s structure looks like this: </p>
<img src="/2023/07/09/Pytorch-Practical-Basics-5/basic.png" alt="basic">

<p>Please see the code below. Here, apart from <code>channels</code> defining the channels in the block, we have three additional parameters, <code>in_channels</code>, <code>stride</code>, and <code>downsample</code> to make this block versatile in the FIRST block in each layer. </p>
<p>According to the ResNet structure, for example, the first block in layer3 has the input <code>64*56*56</code>. The first block in layer3 has two tasks: </p>
<ol>
<li>Make the feature map size to <code>28*28</code>. Thus we need to set its stride to <code>2</code>. </li>
<li>Make the number of channels from <code>64</code> to <code>128</code>. Thus the <code>in_channel</code> should be <code>64</code>. </li>
<li>In addition, since the input is <code>64*56*56</code>, while the output is <code>128*28*28</code>, we need a down sample convolution to match the shortcut input to the output size.</li>
</ol>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualBasicBlock</span>(nn.Module):</span><br><span class="line">    expansion: <span class="built_in">int</span> = <span class="number">1</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels: <span class="built_in">int</span>, channels: <span class="built_in">int</span>, stride: <span class="built_in">int</span> = <span class="number">1</span>, downsample: nn.Module = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels, channels, <span class="number">3</span>, stride, <span class="number">1</span>)</span><br><span class="line">        self.batchnorm1 = nn.BatchNorm2d(channels)</span><br><span class="line">        self.relu1 = nn.ReLU()</span><br><span class="line">        self.conv2 = nn.Conv2d(channels, channels, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.batchnorm2 = nn.BatchNorm2d(channels)</span><br><span class="line">        self.relu2 = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        residual = x</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.batchnorm1(x)</span><br><span class="line">        x = self.relu1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.batchnorm2(x)</span><br><span class="line">        <span class="keyword">if</span> self.downsample:</span><br><span class="line">            residual = self.downsample(residual)</span><br><span class="line">        x += residual</span><br><span class="line">        x = self.relu2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>



<h2 id="Bottleneck-Block-Implementation"><a href="#Bottleneck-Block-Implementation" class="headerlink" title="Bottleneck Block Implementation"></a>Bottleneck Block Implementation</h2><p>The bottleneck block’s structure looks like this: </p>
<img src="/2023/07/09/Pytorch-Practical-Basics-5/bottleneck.png" alt="bottleneck">

<p>To reduce the computation cost, the Bottleneck block use <code>1x1</code> kernel to map the high number of channels (e.g., 256) to a low one (e.g., 64), and do the <code>3x3</code> convolution. Then, it maps the <code>64</code> channels to <code>256</code> again. </p>
<p>Please see the code below. Same as the basic block, We have three additional parameters, <code>in_channels</code>, <code>stride</code>, and <code>downsample</code> to make this block versatile in the FIRST block in each layer. The reasons are same as above. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualBottleNeck</span>(nn.Module):</span><br><span class="line">    expansion: <span class="built_in">int</span> = <span class="number">4</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels: <span class="built_in">int</span>, channels: <span class="built_in">int</span>, stride: <span class="built_in">int</span> = <span class="number">1</span>, downsample: nn.Module = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels, channels, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.batchnorm1 = nn.BatchNorm2d(channels)</span><br><span class="line">        self.relu1 = nn.ReLU()</span><br><span class="line">        self.conv2 = nn.Conv2d(channels, channels, <span class="number">3</span>, stride, <span class="number">1</span>)</span><br><span class="line">        self.batchnorm2 = nn.BatchNorm2d(channels)</span><br><span class="line">        self.relu2 = nn.ReLU()</span><br><span class="line">        self.conv3 = nn.Conv2d(channels, channels*<span class="number">4</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.batchnorm3 = nn.BatchNorm2d(channels*<span class="number">4</span>)</span><br><span class="line">        self.relu3 = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        residual = x</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.batchnorm1(x)</span><br><span class="line">        x = self.relu1(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.batchnorm2(x)</span><br><span class="line">        x = self.relu2(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        x = self.batchnorm3(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.downsample:</span><br><span class="line">            residual = self.downsample(residual)</span><br><span class="line"></span><br><span class="line">        x += residual</span><br><span class="line">        x = self.relu3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>



<h2 id="ResNet-Base-Implementation"><a href="#ResNet-Base-Implementation" class="headerlink" title="ResNet Base Implementation"></a>ResNet Base Implementation</h2><p>Then we can put thing together to form the ResNet model! The whole structure is straight-forward. We define the submodules one by one, and implement the <code>forward()</code> function. </p>
<p>There is only two tricky point: </p>
<ol>
<li>To support the <code>ResNetBase</code> for two different base blocks, the base block can be passed to this initializer. Since two base blocks have slightly differences in setting the channels, <code>ResidualBasicBlock</code> and <code>ResidualBottleNeck</code> have an attribute called <code>expansion</code>, which convenient the procedure in setting the correct number of channels and outputs. </li>
<li>See the <code>_make_layer</code> function below. It need to determine whether we need to do the down sample. And the condition and explanation is described below.</li>
</ol>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResNetBase</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, block, layer_blocks: <span class="built_in">list</span>, input_channels=<span class="number">3</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.block = block</span><br><span class="line">        <span class="comment"># conv1: 7x7</span></span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(input_channels, <span class="number">64</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>), </span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># max pool</span></span><br><span class="line">        self.maxpool = nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># conv2 ~ conv5_x</span></span><br><span class="line">        self.in_channels = <span class="number">64</span></span><br><span class="line">        self.conv2 = self._make_layer(<span class="number">64</span>, layer_blocks[<span class="number">0</span>])</span><br><span class="line">        self.conv3 = self._make_layer(<span class="number">128</span>, layer_blocks[<span class="number">1</span>], <span class="number">2</span>)</span><br><span class="line">        self.conv4 = self._make_layer(<span class="number">256</span>, layer_blocks[<span class="number">2</span>], <span class="number">2</span>)</span><br><span class="line">        self.conv5 = self._make_layer(<span class="number">512</span>, layer_blocks[<span class="number">3</span>], <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.downsample = nn.AvgPool2d(<span class="number">7</span>)</span><br><span class="line">        output_numel = <span class="number">512</span> * self.block.expansion</span><br><span class="line">        self.fc = nn.Linear(output_numel, <span class="number">1000</span>)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># init the weights</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(m.weight, mode=<span class="string">&quot;fan_out&quot;</span>, nonlinearity=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">                nn.init.normal_(m.weight, <span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">                nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, (nn.BatchNorm2d, nn.GroupNorm)):</span><br><span class="line">                nn.init.constant_(m.weight, <span class="number">1</span>)</span><br><span class="line">                nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_make_layer</span>(<span class="params">self, channel, replicates, stride=<span class="number">1</span></span>):</span><br><span class="line">        modules = []</span><br><span class="line"></span><br><span class="line">        downsample = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> self.in_channels != channel*self.block.expansion:</span><br><span class="line">            <span class="comment"># Use downsample to match the dimension in two cases: </span></span><br><span class="line">            <span class="comment"># 1. stride != 1, meaning we should downsample H, W in this layer. </span></span><br><span class="line">            <span class="comment">#   Then we need to match the residual&#x27;s H, W and the output&#x27;s H, W of this layer.</span></span><br><span class="line">            <span class="comment"># 2. self.in_channels != channel*block.expansion, meaning we should increase C in this layer.</span></span><br><span class="line">            <span class="comment">#   Then we need to match the residual&#x27;s C and the output&#x27;s C of this layer.</span></span><br><span class="line"></span><br><span class="line">            downsample = nn.Sequential(</span><br><span class="line">                nn.Conv2d(self.in_channels, channel*self.block.expansion, <span class="number">1</span>, stride),</span><br><span class="line">                nn.BatchNorm2d(channel*self.block.expansion)</span><br><span class="line">            )</span><br><span class="line">        </span><br><span class="line">        modules.append(self.block(self.in_channels, channel, stride, downsample))</span><br><span class="line"></span><br><span class="line">        self.in_channels = channel * self.block.expansion</span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, replicates):</span><br><span class="line">            modules.append(self.block(self.in_channels, channel))</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*modules)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x: shape Bx3x224x224</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.maxpool(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        x = self.conv4(x)</span><br><span class="line">        x = self.conv5(x)</span><br><span class="line"></span><br><span class="line">        x = self.downsample(x)</span><br><span class="line">        x = torch.flatten(x, start_dim=<span class="number">1</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>



<h2 id="Encapsulate-the-Constructors"><a href="#Encapsulate-the-Constructors" class="headerlink" title="Encapsulate the Constructors"></a>Encapsulate the Constructors</h2><p>Finally, we can encapsulate the constructors by functions:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">my_resnet18</span>(<span class="params">in_channels=<span class="number">3</span></span>):</span><br><span class="line">    <span class="keyword">return</span> ResNetBase(ResidualBasicBlock, [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>], in_channels)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">my_resnet34</span>(<span class="params">in_channels=<span class="number">3</span></span>):</span><br><span class="line">    <span class="keyword">return</span> ResNetBase(ResidualBasicBlock, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>], in_channels)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">my_resnet50</span>(<span class="params">in_channels=<span class="number">3</span></span>):</span><br><span class="line">    <span class="keyword">return</span> ResNetBase(ResidualBottleNeck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>], in_channels)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">my_resnet101</span>(<span class="params">in_channels=<span class="number">3</span></span>):</span><br><span class="line">    <span class="keyword">return</span> ResNetBase(ResidualBottleNeck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">3</span>], in_channels)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">my_resnet152</span>(<span class="params">in_channels=<span class="number">3</span></span>):</span><br><span class="line">    <span class="keyword">return</span> ResNetBase(ResidualBottleNeck, [<span class="number">3</span>, <span class="number">8</span>, <span class="number">36</span>, <span class="number">3</span>], in_channels)</span><br></pre></td></tr></table></figure>

<p>Then, we can use it as normal models:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">img = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line">model_my = my_resnet50()</span><br><span class="line">res_my = model_my(img)</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/18/Pytorch-Practical-Basics-4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Future">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Future's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/18/Pytorch-Practical-Basics-4/" class="post-title-link" itemprop="url">PyTorch Practical Hand-Written Modules Basics 4--Hand-written modules basics</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-06-18 10:01:32" itemprop="dateCreated datePublished" datetime="2023-06-18T10:01:32+08:00">2023-06-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-07-09 09:49:04" itemprop="dateModified" datetime="2023-07-09T09:49:04+08:00">2023-07-09</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>After three articles talking about tensors, in this article, we will talk about something to the PyTorch Hand Written Modules Basics. You can see the outline on the left sidebar.</p>
<h2 id="Basic-structure"><a href="#Basic-structure" class="headerlink" title="Basic structure"></a>Basic structure</h2><p>The model must inherit the <code>nn.Module</code> class. Basically, according to the <a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/nn_tutorial.html#closing-thoughts">official tutorial</a>, <code>nn.Module</code> “creates a callable which behaves like a function, but can also contain state(such as neural net layer weights).”</p>
<p>The following is an example from the <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">docs</a>:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">20</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        <span class="keyword">return</span> F.relu(self.conv2(x))</span><br></pre></td></tr></table></figure>

<p><strong>Some details</strong></p>
<ul>
<li>First, our model has Name <code>Model</code>, and inherits the <code>nn.Module</code> class.</li>
<li><code>super().__init__()</code> must be called at the first line of the <code>__init__</code> function.</li>
<li>The <code>Model</code> contains two submodules as attributes, <code>conv1</code> and <code>conv2</code>. They’re <code>nn.Conv2d</code> (The PyTorch implementation for 2-D convolution)</li>
<li>The <code>forward()</code> function do the forward-propagation of the model. It receives a tensor <code>x</code> and do two convolution-with-relu operation. And then return the result. </li>
<li>As for the backward-propagation, that step is calculated automatically by the powerful PyTorch’s auto-gradient technique. You don’t need to care about that.</li>
</ul>
<h2 id="load-store-the-model-state-dict"><a href="#load-store-the-model-state-dict" class="headerlink" title="load &#x2F; store the model.state_dict()"></a>load &#x2F; store the model.state_dict()</h2><p>Only model’s attributes that are subclass of <code>nn.Module</code> can be regarded as a valid registered parameters. These parameters are in the <code>model.state_dict()</code>, and can be load and store from&#x2F;to the disk.</p>
<ul>
<li><code>model.state_dict()</code>:</li>
</ul>
<p>The <code>state_dict()</code> is an <code>OrderedDict</code>. It contains the key value pair like “Parameter Name: Tensor”</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model.state_dict()</span><br><span class="line"></span><br><span class="line">model.state_dict().keys()</span><br><span class="line"><span class="comment"># OUTPUT:</span></span><br><span class="line"><span class="comment"># odict_keys([&#x27;conv1.weight&#x27;, &#x27;conv1.bias&#x27;, &#x27;conv2.weight&#x27;, &#x27;conv2.bias&#x27;])</span></span><br><span class="line"></span><br><span class="line">model.state_dict().values()</span><br><span class="line"><span class="comment"># OUTPUT:</span></span><br><span class="line"><span class="comment"># odict_values([tensor([[[[ 1.0481e-01, -2.3481e-02,  9.1083e-02,  1.9955e-01,  1.0437e-01], ... ...</span></span><br></pre></td></tr></table></figure>

<ul>
<li>Use the following code to <strong>store</strong> the parameters of the model <code>Model</code> above to the disk:</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(), <span class="string">&#x27;model.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>Use the following code to <strong>load</strong> the parameters from the disk:</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;model.pth&#x27;</span>))</span><br></pre></td></tr></table></figure>



<h2 id="Common-Submodules"><a href="#Common-Submodules" class="headerlink" title="Common Submodules"></a>Common Submodules</h2><p>This subsection introduces some common submodules used. As mentioned above, to make them as valid registered parameters, they are subclass of <code>nn.Module</code> or are type <code>nn.Parameter</code>.</p>
<h3 id="clone-the-module"><a href="#clone-the-module" class="headerlink" title="clone the module"></a>clone the module</h3><p>The module should be copied (cloned) by the <code>copy.deepcopy</code> method. </p>
<ul>
<li>Shallow copy (wrong!)</li>
</ul>
<p>The model is only shallow copied. We can see that the two models’ <code>conv1</code> Tensor are the same one!!!</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">model = Model()</span><br><span class="line">model2 = copy.copy(model) <span class="comment"># shallow copy</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(model.conv1), <span class="built_in">id</span>(model2.conv1))</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line"><span class="number">2755774917472</span> <span class="number">2755774917472</span></span><br></pre></td></tr></table></figure>

<ul>
<li>Deep copy (right!)</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">model = Model()</span><br><span class="line">model2 = copy.deepcopy(model) <span class="comment"># deep copy</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(model.conv1), <span class="built_in">id</span>(model2.conv1))</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line"><span class="number">2755774915552</span> <span class="number">2755774916272</span></span><br></pre></td></tr></table></figure>

<ul>
<li>Example:</li>
</ul>
<p>This is the code from <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/detr/blob/main/models/transformer.py#L272">DETR</a>. This copies <code>module</code> for N times, resulting in an <code>nn.ModuleList</code>. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_get_clones</span>(<span class="params">module, N</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N)])</span><br></pre></td></tr></table></figure>



<h3 id="nn-ModuleList"><a href="#nn-ModuleList" class="headerlink" title="nn.ModuleList"></a>nn.ModuleList</h3><p><code>nn.ModuleList</code> is a list, but inherited the <code>nn.Module</code>. It can be recognized by the model correctly. </p>
<ul>
<li>Wrong example: from the output, we can see the submodule is not registered correctly.</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model2</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args, **kwargs</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__(*args, **kwargs)</span><br><span class="line">        self.mlp = [nn.Linear(<span class="number">10</span>, <span class="number">10</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]</span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(Model2().state_dict().keys())</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">odict_keys([])</span><br></pre></td></tr></table></figure>

<ul>
<li>Correct example: from the output, we can see the submodule is registered correctly.</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model3</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args, **kwargs</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__(*args, **kwargs)</span><br><span class="line">        self.mlp = nn.ModuleList([nn.Linear(<span class="number">10</span>, <span class="number">10</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]) </span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(Model3().state_dict().keys())</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">odict_keys([<span class="string">&#x27;mlp.0.weight&#x27;</span>, <span class="string">&#x27;mlp.0.bias&#x27;</span>, ..., <span class="string">&#x27;mlp.9.weight&#x27;</span>, <span class="string">&#x27;mlp.9.bias&#x27;</span>])</span><br></pre></td></tr></table></figure>

<p><code>nn.ModuleDict</code> is similar to <code>nn.ModuleList</code>, but a dictionary. </p>
<h3 id="nn-Parameter"><a href="#nn-Parameter" class="headerlink" title="nn.Parameter"></a>nn.Parameter</h3><p>A plain tensor attributes can not be registered to the model. We need to wrap it with <code>nn.Parameter</code>, to make the model save the tensor’s state correctly. </p>
<p>The following is modified from the <a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/nn_tutorial.html#refactor-using-nn-module">official tutorial</a>. In this example, <code>self.weights</code> is merely a <code>torch.Tensor</code>, which cannot be regarded as a model’s <code>state_dict</code>. The <code>self.bias</code> would works normally, because it’s a <code>nn.Parameter</code>.  </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Mnist_Logistic</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weights = torch.randn(<span class="number">784</span>, <span class="number">10</span>) / math.sqrt(<span class="number">784</span>) <span class="comment"># WRONG</span></span><br><span class="line">        self.bias = nn.Parameter(torch.zeros(<span class="number">10</span>)) <span class="comment"># CORRECT</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, xb</span>):</span><br><span class="line">        <span class="keyword">return</span> xb @ self.weights + self.bias</span><br></pre></td></tr></table></figure>

<p>Check if submodules is correctly regiestered:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(Mnist_Logistic().state_dict().keys())</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">odict_keys([<span class="string">&#x27;bias&#x27;</span>]) <span class="comment"># only `bias` regiestered! no `weights` here</span></span><br></pre></td></tr></table></figure>



<h3 id="nn-Sequential"><a href="#nn-Sequential" class="headerlink" title="nn.Sequential"></a>nn.Sequential</h3><p>This is a sequential container. Data will flow by the submodules contained one by one. An example is shown below.  </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line">model =nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">784</span>, <span class="number">128</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">128</span>, <span class="number">64</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>



<h2 id="model-apply-weight-init"><a href="#model-apply-weight-init" class="headerlink" title="model.apply() &amp; weight init"></a>model.apply() &amp; weight init</h2><p>Applies <code>fn</code> recursively to every submodule (as returned by <code>model.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.init.html#nn-init-doc">torch.nn.init</a>).</p>
<p>A typical example can be:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">20</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        <span class="keyword">return</span> F.relu(self.conv2(x))</span><br><span class="line">    </span><br><span class="line">model = Model()   </span><br><span class="line"><span class="comment"># do init params with model.apply():</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        torch.nn.init.xavier_uniform_(m.weight)</span><br><span class="line">        m.bias.data.fill_(<span class="number">0.01</span>)</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">        torch.nn.init.xavier_uniform_(m.weight)</span><br><span class="line">        m.bias.data.fill_(<span class="number">0.01</span>)</span><br><span class="line">model.apply(init_weights)</span><br></pre></td></tr></table></figure>



<h2 id="model-eval-model-train-training"><a href="#model-eval-model-train-training" class="headerlink" title="model.eval() &#x2F; model.train() &#x2F; .training"></a>model.eval() &#x2F; model.train() &#x2F; .training</h2><p>The modules such as <code>BatchNorm</code> and <code>DropOut</code> performs differently on the training and evaluating stage. </p>
<p>We can use <code>model.train()</code> to set the model to the training stage. Use <code>model.eval()</code> to set the model to the training stage. </p>
<p>But, what if our <u>own written modules</u> need to perform differently in two stages? The answer is that, <code>nn.Module</code> has an attribute called <code>training</code>. It’s <code>True</code> when training, <code>False</code> otherwise. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># skipped in this example</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            ... <span class="comment"># write the code in training stage here</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            ... <span class="comment"># write the code in evaluating/inferencing stage here</span></span><br></pre></td></tr></table></figure>

<p>As we can see, when we called <code>model.train()</code>, actually, all submodules from <code>model</code> would set the <code>training</code> attribute to <code>True</code>, and <code>False</code> otherwise. </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Future</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">28</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Future</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
