<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/time-machine.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Future&#39;s blog">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Future&#39;s blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Future">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Future's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Future's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/21/%E6%9C%AC%E7%AB%99%E9%A3%9F%E7%94%A8%E6%8C%87%E5%8D%97-How-to-use-this-blog/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Future">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Future's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/03/21/%E6%9C%AC%E7%AB%99%E9%A3%9F%E7%94%A8%E6%8C%87%E5%8D%97-How-to-use-this-blog/" class="post-title-link" itemprop="url">How to use this blog 本站食用指南</a>
        </h2>

        <div class="post-meta">
          
              <i class="fa fa-thumbtack"></i>
              <font color=7D26CD>Pinned</font>
              <span class="post-meta-divider">|</span>
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-03-21 16:14:47" itemprop="dateCreated datePublished" datetime="2023-03-21T16:14:47+08:00">2023-03-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-05-29 11:23:24" itemprop="dateModified" datetime="2023-05-29T11:23:24+08:00">2023-05-29</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Hello! 你好！</p>
<p>This blog is established on 2023&#x2F;3&#x2F;21. The aim to build such a blog website which is used to share my programming &#x2F; other knowledges.</p>
<p>本站建立于2023.3.21，建立之初是为了拥有一个自己的知识内容分享平台。</p>
<p>This is the home page, and this article is pinned. If you want to search for some past articles, use “Categories”, “Archives” or “Search” on the top of the page.</p>
<p>这是主页。这篇文章置顶于此。如果你想找指定文章内容，可以在页面上方“Categories”或者“Archives”寻找。</p>
<p><strong>Future and Plans 计划与憧憬</strong></p>
<p>On the first day of our blog, let’s plan it: 建博客第一天，就先来立一些这个博客的计划：</p>
<ul>
<li>Language 语言：We will mainly use English in this blog. 网站主要使用英文。（单词也比较简单，对于中文用户其实阅读难度不大啦）</li>
<li>Contents 内容：The content should mainly be my original content, or a comprehensive content that combines other existing materials with my annotations or supplements. 内容主要应该是我的原创内容，或者是结合了其他现有资料，配合我的注释或是增补的综合内容。</li>
<li>Categories 类别：<ul>
<li>Deep Learning: Things related to PyTorch, deep learning。和torch，深度学习相关的会在这。</li>
<li>QRH: Quick Reference Handbook of tools&#x2F;programming languages&#x2F;packages, 一些工具、软件、包、语言的快速参考手册。</li>
<li>QuickIntro: Quick Introduction to a tools&#x2F;programming languages&#x2F;packages, 快速入门某一工具、软件、包、语言。</li>
<li>Techniques: Techniques and tricks in something. 技艺，一些环境配置、瞎折腾的技术、搭建过程会写在这。</li>
<li>Problem Solving: Record the problems I met and how did I solve it out. 记录我遇到的问题以及我的解决方案。</li>
<li>Others: 其它。</li>
</ul>
</li>
</ul>
<p><strong>TODO list：</strong></p>
<p>在下面列一些近期可能想做的topic（立flag中）</p>
<ul>
<li>QRH</li>
</ul>
<ul>
<li><input checked disabled type="checkbox"> Matplotlib common functions</li>
<li><input disabled type="checkbox"> conda common commands</li>
<li><input disabled type="checkbox"> Linux shell common commands</li>
<li><input disabled type="checkbox"> vim common keys</li>
<li><input checked disabled type="checkbox"> latex common code blocks</li>
<li><input disabled type="checkbox"> </li>
</ul>
<ul>
<li>QuickIntro</li>
</ul>
<ul>
<li><input checked disabled type="checkbox"> NumPy quick intro</li>
<li><input disabled type="checkbox"> 15 min regex</li>
<li><input disabled type="checkbox"> 15 min  Makefile</li>
<li><input disabled type="checkbox"> 15 min  MySQL</li>
<li><input disabled type="checkbox"> </li>
</ul>
<ul>
<li>Techniques</li>
</ul>
<ul>
<li><input checked disabled type="checkbox"> C++ complex type declaration rules (In Chinese, 2&#x2F;2)</li>
<li><input disabled type="checkbox"> </li>
</ul>
<ul>
<li>Others</li>
</ul>
<ul>
<li><input disabled type="checkbox"> Emm…</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/18/Pytorch-Practical-Basics-4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Future">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Future's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/18/Pytorch-Practical-Basics-4/" class="post-title-link" itemprop="url">PyTorch Practical Hand-Written Modules Basics 4--Hand-written modules basics</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2023-06-18 10:01:32 / Modified: 20:23:22" itemprop="dateCreated datePublished" datetime="2023-06-18T10:01:32+08:00">2023-06-18</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="PyTorch-Practical-Hand-Written-Modules-Basics-4–Hand-written-modules-basics"><a href="#PyTorch-Practical-Hand-Written-Modules-Basics-4–Hand-written-modules-basics" class="headerlink" title="PyTorch Practical Hand-Written Modules Basics 4–Hand-written modules basics"></a>PyTorch Practical Hand-Written Modules Basics 4–Hand-written modules basics</h1><p>After three articles talking about tensors, in this article, we will talk about something to the PyTorch Hand Written Modules Basics. You can see the outline on the left sidebar.</p>
<h2 id="Basic-structure"><a href="#Basic-structure" class="headerlink" title="Basic structure"></a>Basic structure</h2><p>The model must inherit the <code>nn.Module</code> class. Basically, according to the <a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/nn_tutorial.html#closing-thoughts">official tutorial</a>, <code>nn.Module</code> “creates a callable which behaves like a function, but can also contain state(such as neural net layer weights).”</p>
<p>The following is an example from the <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">docs</a>:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">20</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        <span class="keyword">return</span> F.relu(self.conv2(x))</span><br></pre></td></tr></table></figure>

<p><strong>Some details</strong></p>
<ul>
<li>First, our model has Name <code>Model</code>, and inherits the <code>nn.Module</code> class.</li>
<li><code>super().__init__()</code> must be called at the first line of the <code>__init__</code> function.</li>
<li>The <code>Model</code> contains two submodules as attributes, <code>conv1</code> and <code>conv2</code>. They’re <code>nn.Conv2d</code> (The PyTorch implementation for 2-D convolution)</li>
<li>The <code>forward()</code> function do the forward-propagation of the model. It receives a tensor <code>x</code> and do two convolution-with-relu operation. And then return the result. </li>
<li>As for the backward-propagation, that step is calculated automatically by the powerful PyTorch’s auto-gradient technique. You don’t need to care about that.</li>
</ul>
<h2 id="load-x2F-store-the-model-state-dict"><a href="#load-x2F-store-the-model-state-dict" class="headerlink" title="load &#x2F; store the model.state_dict()"></a>load &#x2F; store the model.state_dict()</h2><p>Only model’s attributes that are subclass of <code>nn.Module</code> can be regarded as a valid registered parameters. These parameters are in the <code>model.state_dict()</code>, and can be load and store from&#x2F;to the disk.</p>
<ul>
<li><code>model.state_dict()</code>:</li>
</ul>
<p>The <code>state_dict()</code> is an <code>OrderedDict</code>. It contains the key value pair like “Parameter Name: Tensor”</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model.state_dict()</span><br><span class="line"></span><br><span class="line">model.state_dict().keys()</span><br><span class="line"><span class="comment"># OUTPUT:</span></span><br><span class="line"><span class="comment"># odict_keys([&#x27;conv1.weight&#x27;, &#x27;conv1.bias&#x27;, &#x27;conv2.weight&#x27;, &#x27;conv2.bias&#x27;])</span></span><br><span class="line"></span><br><span class="line">model.state_dict().values()</span><br><span class="line"><span class="comment"># OUTPUT:</span></span><br><span class="line"><span class="comment"># odict_values([tensor([[[[ 1.0481e-01, -2.3481e-02,  9.1083e-02,  1.9955e-01,  1.0437e-01], ... ...</span></span><br></pre></td></tr></table></figure>

<ul>
<li>Use the following code to <strong>store</strong> the parameters of the model <code>Model</code> above to the disk:</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(), <span class="string">&#x27;model.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>Use the following code to <strong>load</strong> the parameters from the disk:</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;model.pth&#x27;</span>))</span><br></pre></td></tr></table></figure>



<h2 id="Common-Submodules"><a href="#Common-Submodules" class="headerlink" title="Common Submodules"></a>Common Submodules</h2><p>This subsection introduces some common submodules used. As mentioned above, to make them as valid registered parameters, they are subclass of <code>nn.Module</code> or are type <code>nn.Parameter</code>.</p>
<h3 id="clone-the-module"><a href="#clone-the-module" class="headerlink" title="clone the module"></a>clone the module</h3><p>The module should be copied (cloned) by the <code>copy.deepcopy</code> method. </p>
<ul>
<li>Shallow copy (wrong!)</li>
</ul>
<p>The model is only shallow copied. We can see that the two models’ <code>conv1</code> Tensor are the same one!!!</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">model = Model()</span><br><span class="line">model2 = copy.copy(model) <span class="comment"># shallow copy</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(model.conv1), <span class="built_in">id</span>(model2.conv1))</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line"><span class="number">2755774917472</span> <span class="number">2755774917472</span></span><br></pre></td></tr></table></figure>

<ul>
<li>Deep copy (right!)</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">model = Model()</span><br><span class="line">model2 = copy.deepcopy(model) <span class="comment"># deep copy</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(model.conv1), <span class="built_in">id</span>(model2.conv1))</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line"><span class="number">2755774915552</span> <span class="number">2755774916272</span></span><br></pre></td></tr></table></figure>

<ul>
<li>Example:</li>
</ul>
<p>This is the code from <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/detr/blob/main/models/transformer.py#L272">DETR</a>. This copies <code>module</code> for N times, resulting in an <code>nn.ModuleList</code>. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_get_clones</span>(<span class="params">module, N</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N)])</span><br></pre></td></tr></table></figure>



<h3 id="nn-ModuleList"><a href="#nn-ModuleList" class="headerlink" title="nn.ModuleList"></a>nn.ModuleList</h3><p><code>nn.ModuleList</code> is a list, but inherited the <code>nn.Module</code>. It can be recognized by the model correctly. </p>
<ul>
<li>Wrong example: from the output, we can see the submodule is not registered correctly.</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model2</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args, **kwargs</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__(*args, **kwargs)</span><br><span class="line">        self.mlp = [nn.Linear(<span class="number">10</span>, <span class="number">10</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]</span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(Model2().state_dict().keys())</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">odict_keys([])</span><br></pre></td></tr></table></figure>

<ul>
<li>Correct example: from the output, we can see the submodule is registered correctly.</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model3</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args, **kwargs</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__(*args, **kwargs)</span><br><span class="line">        self.mlp = nn.ModuleList([nn.Linear(<span class="number">10</span>, <span class="number">10</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]) </span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(Model3().state_dict().keys())</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">odict_keys([<span class="string">&#x27;mlp.0.weight&#x27;</span>, <span class="string">&#x27;mlp.0.bias&#x27;</span>, ..., <span class="string">&#x27;mlp.9.weight&#x27;</span>, <span class="string">&#x27;mlp.9.bias&#x27;</span>])</span><br></pre></td></tr></table></figure>

<p><code>nn.ModuleDict</code> is similar to <code>nn.ModuleList</code>, but a dictionary. </p>
<h3 id="nn-Parameter"><a href="#nn-Parameter" class="headerlink" title="nn.Parameter"></a>nn.Parameter</h3><p>A plain tensor attributes can not be registered to the model. We need to wrap it with <code>nn.Parameter</code>, to make the model save the tensor’s state correctly. </p>
<p>The following is modified from the <a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/nn_tutorial.html#refactor-using-nn-module">official tutorial</a>. In this example, <code>self.weights</code> is merely a <code>torch.Tensor</code>, which cannot be regarded as a model’s <code>state_dict</code>. The <code>self.bias</code> would works normally, because it’s a <code>nn.Parameter</code>.  </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Mnist_Logistic</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weights = torch.randn(<span class="number">784</span>, <span class="number">10</span>) / math.sqrt(<span class="number">784</span>) <span class="comment"># WRONG</span></span><br><span class="line">        self.bias = nn.Parameter(torch.zeros(<span class="number">10</span>)) <span class="comment"># CORRECT</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, xb</span>):</span><br><span class="line">        <span class="keyword">return</span> xb @ self.weights + self.bias</span><br></pre></td></tr></table></figure>

<p>Check if submodules is correctly regiestered:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(Mnist_Logistic().state_dict().keys())</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">odict_keys([<span class="string">&#x27;bias&#x27;</span>]) <span class="comment"># only `bias` regiestered! no `weights` here</span></span><br></pre></td></tr></table></figure>



<h3 id="nn-Sequential"><a href="#nn-Sequential" class="headerlink" title="nn.Sequential"></a>nn.Sequential</h3><p>This is a sequential container. Data will flow by the submodules contained one by one. An example is shown below.  </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line">model =nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">784</span>, <span class="number">128</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">128</span>, <span class="number">64</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>



<h2 id="model-apply-amp-weight-init"><a href="#model-apply-amp-weight-init" class="headerlink" title="model.apply() &amp; weight init"></a>model.apply() &amp; weight init</h2><p>Applies <code>fn</code> recursively to every submodule (as returned by <code>model.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.init.html#nn-init-doc">torch.nn.init</a>).</p>
<p>A typical example can be:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">20</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        <span class="keyword">return</span> F.relu(self.conv2(x))</span><br><span class="line">    </span><br><span class="line">model = Model()   </span><br><span class="line"><span class="comment"># do init params with model.apply():</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        torch.nn.init.xavier_uniform_(m.weight)</span><br><span class="line">        m.bias.data.fill_(<span class="number">0.01</span>)</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">        torch.nn.init.xavier_uniform_(m.weight)</span><br><span class="line">        m.bias.data.fill_(<span class="number">0.01</span>)</span><br><span class="line">model.apply(init_weights)</span><br></pre></td></tr></table></figure>



<h2 id="model-eval-x2F-model-train-x2F-training"><a href="#model-eval-x2F-model-train-x2F-training" class="headerlink" title="model.eval() &#x2F; model.train() &#x2F; .training"></a>model.eval() &#x2F; model.train() &#x2F; .training</h2><p>The modules such as <code>BatchNorm</code> and <code>DropOut</code> performs differently on the training and evaluating stage. </p>
<p>We can use <code>model.train()</code> to set the model to the training stage. Use <code>model.eval()</code> to set the model to the training stage. </p>
<p>But, what if our <u>own written modules</u> need to perform differently in two stages? The answer is that, <code>nn.Module</code> has an attribute called <code>training</code>. It’s <code>True</code> when training, <code>False</code> otherwise. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># skipped in this example</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            ... <span class="comment"># write the code in training stage here</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            ... <span class="comment"># write the code in evaluating/inferencing stage here</span></span><br></pre></td></tr></table></figure>

<p>As we can see, when we called <code>model.train()</code>, actually, all submodules from <code>model</code> would set the <code>training</code> attribute to <code>True</code>, and <code>False</code> otherwise. </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/17/Pytorch-Practical-Basics-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Future">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Future's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/17/Pytorch-Practical-Basics-3/" class="post-title-link" itemprop="url">PyTorch Practical Hand-Written Modules Basics 3--Tensor-wise operations</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-06-17 18:07:05" itemprop="dateCreated datePublished" datetime="2023-06-17T18:07:05+08:00">2023-06-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-06-23 19:32:20" itemprop="dateModified" datetime="2023-06-23T19:32:20+08:00">2023-06-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="PyTorch-Practical-Hand-Written-Modules-Basics-3–Tensor-wise-operations"><a href="#PyTorch-Practical-Hand-Written-Modules-Basics-3–Tensor-wise-operations" class="headerlink" title="PyTorch Practical Hand-Written Modules Basics 3–Tensor-wise operations"></a>PyTorch Practical Hand-Written Modules Basics 3–Tensor-wise operations</h1><p>In this section we will talk about some PyTorch functions that operates the tensors. </p>
<h2 id="torch-Tensor-expand"><a href="#torch-Tensor-expand" class="headerlink" title="torch.Tensor.expand"></a>torch.Tensor.expand</h2><p>Signature: <code>Tensor.expand(*sizes) -&gt; Tensor</code></p>
<p>The <code>expand</code> function returns a new view of the <code>self</code> tensor, with singleton dimensions expanded to a larger size. The passing parameter indicates the destination size. (“singleton dimensions” means the dimension with shape <code>1</code>)</p>
<p><strong>Basic Usage</strong></p>
<p>Passing <code>-1</code> as the size for a dimension means not changing the size of that dimension.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]]) <span class="comment"># torch.Size([3, 1])</span></span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(x.expand(<span class="number">3</span>, <span class="number">4</span>))   <span class="comment"># torch.Size([3, 4])</span></span><br><span class="line"><span class="built_in">print</span>(x.expand(-<span class="number">1</span>, <span class="number">4</span>))  <span class="comment"># torch.Size([3, 4])</span></span><br><span class="line"><span class="built_in">print</span>(x.expand(<span class="number">3</span>, -<span class="number">1</span>))  <span class="comment"># torch.Size([3, 1])</span></span><br><span class="line"><span class="built_in">print</span>(x.expand(-<span class="number">1</span>, -<span class="number">1</span>)) <span class="comment"># torch.Size([3, 1])</span></span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">tensor([[<span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>]])</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>]])</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>]])</span><br><span class="line">tensor([[<span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>]])</span><br><span class="line">tensor([[<span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>]])</span><br></pre></td></tr></table></figure>

<p><strong>Wrong Usage</strong></p>
<p>Only the dimension with shape 1 can be expanded:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]]) <span class="comment"># torch.Size([3, 1])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.expand(<span class="number">2</span>, <span class="number">2</span>))   <span class="comment"># ERROR! can&#x27;t expand axis 0 shape from 3 (not 1)</span></span><br></pre></td></tr></table></figure>

<p><strong>Why use it?</strong></p>
<p>The return is only a view, not a new tensor. <strong>Therefore, if you only want to only read (not write) to an expanded tensor, use expand() will save much GPU memory.</strong> Note that modifying on the expanded tensor would make modification on the original as well. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]]) <span class="comment"># torch.Size([3, 1])</span></span><br><span class="line">x.expand(<span class="number">3</span>, <span class="number">4</span>)[<span class="number">0</span>, <span class="number">1</span>] = <span class="number">100</span></span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">tensor([[<span class="number">100</span>],</span><br><span class="line">        [  <span class="number">2</span>],</span><br><span class="line">        [  <span class="number">3</span>]])</span><br></pre></td></tr></table></figure>



<h2 id="torch-Tensor-repeat"><a href="#torch-Tensor-repeat" class="headerlink" title="torch.Tensor.repeat"></a>torch.Tensor.repeat</h2><p>Signature: <code>Tensor.repeat(*sizes) -&gt; Tensor)</code></p>
<p>Repeats this tensor along the specified dimensions. It is somewhat similar to <code>torch.Tensor.expand()</code>, but the passing in parameter indicates the repeat times. Also, this is a deep copy.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) <span class="comment"># torch.Size([3])</span></span><br><span class="line"><span class="built_in">print</span>(x.repeat(<span class="number">4</span>, <span class="number">2</span>)) <span class="comment"># torch.Size([4, 6])</span></span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]])</span><br></pre></td></tr></table></figure>

<p><strong>More than the given ndimension</strong></p>
<p>If the <code>size</code> has <strong>more</strong> dimension than the self tensor, like the example below, the <code>x</code> only have shape 3x1, while we have more than two input parameters, then additional dimensions will be added at the front. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]]) <span class="comment"># torch.Size([3, 1])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.repeat(<span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>).shape)    </span><br><span class="line"><span class="comment"># torch.Size([4, 6, 1])  first 1: same. last 2 dim: [3,1]*[2,1]=[6,1]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.repeat(<span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>).shape)    </span><br><span class="line"><span class="comment"># torch.Size([4, 2, 3, 1])  first 2: same. last 2 dim: [3,1]*[1,1]=[3,1]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.repeat(<span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>).shape)    </span><br><span class="line"><span class="comment"># torch.Size([1, 4, 6, 1])  first 2: same. last 2 dim: [3,1]*[2,1]=[6,1]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.repeat(<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>).shape)    </span><br><span class="line"><span class="comment"># torch.Size([1, 1, 12, 2])  first 2: same. last 2 dim: [3,1]*[4,2]=[12,2]</span></span><br></pre></td></tr></table></figure>



<h2 id="torch-Tensor-transpose"><a href="#torch-Tensor-transpose" class="headerlink" title="torch.Tensor.transpose"></a>torch.Tensor.transpose</h2><p>Signature: <code>torch.transpose(input, dim0, dim1) -&gt; Tensor</code></p>
<p>Signature: <code>torch.Tensor.transpose(dim0, dim1) -&gt; Tensor</code></p>
<p>Returns a tensor that is a transposed version of <code>input</code>. The given dimensions <code>dim0</code> and <code>dim1</code> are swapped. </p>
<p>Therefore, like the examples below, <code>x.transpose(0, 1)</code> and <code>x.transpose(1, 0)</code> are same. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x) <span class="comment"># shape: torch.Size([2, 3])</span></span><br><span class="line"><span class="built_in">print</span>(x.transpose(<span class="number">0</span>, <span class="number">1</span>)) <span class="comment"># shape: torch.Size([3, 2])</span></span><br><span class="line"><span class="built_in">print</span>(x.transpose(<span class="number">1</span>, <span class="number">0</span>)) <span class="comment"># shape: torch.Size([3, 2])</span></span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">y = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(y) <span class="comment"># shape: torch.Size([2, 3, 4])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(y.transpose(<span class="number">0</span>, <span class="number">1</span>)) <span class="comment"># shape: torch.Size([3, 2, 4])</span></span><br><span class="line"><span class="built_in">print</span>(y.transpose(<span class="number">1</span>, <span class="number">0</span>)) <span class="comment"># shape: torch.Size([3, 2, 4])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(y.transpose(<span class="number">0</span>, <span class="number">2</span>)) <span class="comment"># shape: torch.Size([4, 3, 2])</span></span><br><span class="line"><span class="built_in">print</span>(y.transpose(<span class="number">2</span>, <span class="number">0</span>)) <span class="comment"># shape: torch.Size([4, 3, 2])</span></span><br></pre></td></tr></table></figure>



<h2 id="torch-Tensor-permute"><a href="#torch-Tensor-permute" class="headerlink" title="torch.Tensor.permute"></a>torch.Tensor.permute</h2><p>Signature: <code>torch.Tensor.permute(dims) -&gt; Tensor</code></p>
<p>Signature: <code>torch.permute(input, dims) -&gt; Tensor</code></p>
<p>This function reorder the dimensions. See the example below. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>) <span class="comment"># Shape: torch.Size([2, 3, 4])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(y.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>)) <span class="comment"># Shape: torch.Size([2, 3, 4])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(y.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)) <span class="comment"># Shape: torch.Size([2, 4, 3])</span></span><br></pre></td></tr></table></figure>

<p>Let’s have a close look to the third line as an example. </p>
<ul>
<li><p>The first argument <code>0</code> means that the new tensor’s first dimension is the original dimension at <code>0</code>, so the shape is 2.</p>
</li>
<li><p>The second argument <code>2</code> means that the new tensor’s second dimension is the original dimension at <code>2</code>, so the shape is 4. </p>
</li>
<li><p>The third argument <code>1</code> means that the new tensor’s third dimension is the original dimension at <code>1</code>, so the shape is 3.</p>
</li>
</ul>
<p>Finally, the result shape is <code>torch.Size([2, 4, 3])</code>. </p>
<h2 id="torch-Tensor-view-x2F-torch-Tensor-reshape"><a href="#torch-Tensor-view-x2F-torch-Tensor-reshape" class="headerlink" title="torch.Tensor.view &#x2F; torch.Tensor.reshape"></a>torch.Tensor.view &#x2F; torch.Tensor.reshape</h2><p>Signature: <code>Tensor.view(*shape) -&gt; Tensor</code></p>
<p>Signature: <code>Tensor.reshape(*shape) -&gt; Tensor</code></p>
<p>Reshape the Tensor to <code>shape</code>. </p>
<p>The function <code>shape()</code> always return a new copy of the tensor. </p>
<p>For function <code>view()</code>, if the <code>shape</code> satisfies some conditions (see <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.Tensor.view.html">here</a>), deep copy can be avoided to save the GPU memory. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x) <span class="comment"># Shape: torch.Size([4, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.reshape(<span class="number">3</span>, <span class="number">4</span>)) <span class="comment"># Shape: torch.Size([3, 4])</span></span><br><span class="line"><span class="built_in">print</span>(x.reshape(-<span class="number">1</span>, <span class="number">4</span>)) <span class="comment"># Shape: torch.Size([3, 4])</span></span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x) <span class="comment"># Shape: torch.Size([4, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.view(<span class="number">3</span>, <span class="number">4</span>)) <span class="comment"># Shape: torch.Size([3, 4])</span></span><br><span class="line"><span class="built_in">print</span>(x.view(-<span class="number">1</span>, <span class="number">4</span>)) <span class="comment"># Shape: torch.Size([3, 4])</span></span><br></pre></td></tr></table></figure>

<h2 id="torch-cat"><a href="#torch-cat" class="headerlink" title="torch.cat"></a>torch.cat</h2><p>Signature: <code>torch.cat(tensors, dim=0, out=None) -&gt; Tensor</code></p>
<p>Concatenates the given sequence of <code>tensors</code> in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty. For how to determine the <code>dim</code>, please refer to my previous <a target="_blank" rel="noopener" href="https://i-am-future.github.io/2023/05/27/Pytorch-Practical-Basics-2/#Key-What-is-the-%E2%80%9Cdim%E2%80%9D-parameter">article</a>. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x) <span class="comment"># Shape: torch.Size([2, 3])</span></span><br><span class="line"></span><br><span class="line">y = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(y) <span class="comment"># Shape: torch.Size([2, 3])</span></span><br><span class="line"></span><br><span class="line">z = torch.cat((x, y), dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(z) <span class="comment"># Shape: torch.Size([4, 3]) [2+2, 3]</span></span><br><span class="line"></span><br><span class="line">z = torch.cat((x, y), dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(z) <span class="comment"># Shape: torch.Size([2, 6]) [2, 3+3]</span></span><br></pre></td></tr></table></figure>



<h2 id="torch-stack"><a href="#torch-stack" class="headerlink" title="torch.stack"></a>torch.stack</h2><p>Signature: <code>torch.stack(tensors, dim=0, out=None) -&gt; Tensor</code></p>
<p>Concatenates a sequence of tensors along a new dimension. See example below. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>) <span class="comment"># Shape: torch.Size([2, 3])</span></span><br><span class="line">y = torch.randn(<span class="number">2</span>, <span class="number">3</span>) <span class="comment"># Shape: torch.Size([2, 3])</span></span><br><span class="line"></span><br><span class="line">z = torch.stack((x, y), dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(z) <span class="comment"># Shape: torch.Size([*2, 2, 3]) The first 2 is the new dimension</span></span><br><span class="line"></span><br><span class="line">z = torch.stack((x, y), dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(z) <span class="comment"># Shape: torch.Size([2, *2, 3]) The second 2 is the new dimension</span></span><br><span class="line"></span><br><span class="line">z = torch.stack((x, y), dim=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(z) <span class="comment"># Shape: torch.Size([2, 3, *2]) The last 2 is the new dimension</span></span><br></pre></td></tr></table></figure>



<h2 id="torch-vstack-x2F-hstack"><a href="#torch-vstack-x2F-hstack" class="headerlink" title="torch.vstack&#x2F;hstack"></a>torch.vstack&#x2F;hstack</h2><p><code>torch.vsplit(...)</code> is spliting the tensors vertically, which is equivalent to <code>torch.split(..., dim=0)</code>. </p>
<p> <code>torch.hsplit(...)</code> is spliting the tensors horizontally, which is equivalent to <code>torch.split(..., dim=1)</code>. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>) <span class="comment"># Shape: torch.Size([2, 3])</span></span><br><span class="line">y = torch.randn(<span class="number">2</span>, <span class="number">3</span>) <span class="comment"># Shape: torch.Size([2, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> torch.vstack((x, y)).shape == torch.cat((x, y), dim=<span class="number">0</span>).shape</span><br><span class="line"><span class="keyword">assert</span> torch.hstack((x, y)).shape == torch.cat((x, y), dim=<span class="number">1</span>).shape</span><br></pre></td></tr></table></figure>



<h2 id="torch-split"><a href="#torch-split" class="headerlink" title="torch.split"></a>torch.split</h2><p>Signature: <code>torch.split(tensor, split_size_or_sections, dim=0)</code></p>
<ul>
<li>If <code>split_size_or_sections</code> is an integer, then tensor will be split into equally sized chunks (if possible, ptherwise, last would be smaller).</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">3</span>) <span class="comment"># Shape: torch.Size([4, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.split(x, <span class="number">2</span>, dim=<span class="number">0</span>)) <span class="comment"># 2-item tuple, each Shape: (2, 3)</span></span><br><span class="line"><span class="built_in">print</span>(torch.split(x, <span class="number">1</span>, dim=<span class="number">1</span>)) <span class="comment"># 3-item tuple, each Shape: (4, 1)</span></span><br></pre></td></tr></table></figure>

<ul>
<li>If <code>split_size_or_sections</code> is a list, then tensor will be split into <code>len(split_size_or_sections)</code> chunks with sizes in <code>dim</code> according to <code>split_size_or_sections</code>.</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">3</span>) <span class="comment"># Shape: torch.Size([4, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.split(x, (<span class="number">1</span>, <span class="number">3</span>), dim=<span class="number">0</span>)) </span><br><span class="line"><span class="comment"># 2-item tuple, each Shape: (1, 3) and (3, 3)</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.split(x, (<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>), dim=<span class="number">1</span>)) </span><br><span class="line"><span class="comment"># 3-item tuple, each Shape: (4, 1) and (4, 1) and (4, 1)</span></span><br></pre></td></tr></table></figure>



<h2 id="torch-vsplit-x2F-hsplit"><a href="#torch-vsplit-x2F-hsplit" class="headerlink" title="torch.vsplit&#x2F;hsplit"></a>torch.vsplit&#x2F;hsplit</h2><p>This is actually similar to <code>torch.vstack</code> and <code>torch.hstack</code>. v means vertically, along dim&#x3D;0, and h means horizontally, along dim&#x3D;1.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The followings are equivalent:</span></span><br><span class="line"><span class="comment"># pair 1</span></span><br><span class="line"><span class="built_in">print</span>(torch.vsplit(x, <span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(torch.split(x, <span class="number">1</span>, dim=<span class="number">0</span>))</span><br><span class="line"><span class="comment"># pair 2</span></span><br><span class="line"><span class="built_in">print</span>(torch.hsplit(x, <span class="number">4</span>))</span><br><span class="line"><span class="built_in">print</span>(torch.split(x, <span class="number">1</span>, dim=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>



<h2 id="torch-flatten"><a href="#torch-flatten" class="headerlink" title="torch.flatten"></a>torch.flatten</h2><p>Signature: <code>torch.flatten(input, start_dim=0, end_dim=-1) -&gt; Tensor</code></p>
<p>flatten the given dimension from <code>start_dim</code> to <code>end_dim</code>. This is especially useful when converting a 3D (image) tensor to a linear vector.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(x) <span class="comment"># Shape: torch.Size([2, 4, 4])</span></span><br><span class="line"></span><br><span class="line">flattened = torch.flatten(x, start_dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(flattened) <span class="comment"># Shape: torch.Size([2, 16])</span></span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/05/30/CMake-QRH/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Future">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Future's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/05/30/CMake-QRH/" class="post-title-link" itemprop="url">CMake Quick Reference</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-05-30 10:28:18" itemprop="dateCreated datePublished" datetime="2023-05-30T10:28:18+08:00">2023-05-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-05-31 09:25:56" itemprop="dateModified" datetime="2023-05-31T09:25:56+08:00">2023-05-31</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/QRH/" itemprop="url" rel="index"><span itemprop="name">QRH</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Learned and made up from <a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1fa411r7zp">video</a> and <a target="_blank" rel="noopener" href="https://github.com/parallel101/course">code</a>.</p>
<p>Prerequisite: basic knowledge in C&#x2F;C++.</p>
<h2 id="Basic"><a href="#Basic" class="headerlink" title="Basic"></a>Basic</h2><p><a target="_blank" rel="noopener" href="https://github.com/parallel101/course/tree/master/01/05">See code here</a></p>
<p>A project root directory must contain a file called <code>CMakeLists.txt</code>, describing the build procedure of the project. </p>
<p>A typical simple <code>CMakeLists.txt</code> contains the following (assuming we have two source files in the current directory, <code>main.cpp</code> and <code>hello.cpp</code>):</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.12</span>) <span class="comment"># describe the minimum cmake version</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">project</span>(hellocmake LANGUAGES CXX)    <span class="comment"># describe the project name, and lan</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">add_executable</span>(a.out main.cpp hello.cpp)</span><br></pre></td></tr></table></figure>

<p>The <code>add_executable</code> function’s signature is <code>add_executable(target, [src files...])</code>, meaning to use all <code>src files</code> to compile the <code>target</code>.</p>
<p>To build the program, run in the shell:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cmake -B build</span><br><span class="line">cmake --build build</span><br><span class="line"><span class="comment"># run with</span></span><br><span class="line">./build/&lt;program_name&gt;</span><br></pre></td></tr></table></figure>

<p>To clean and rebuild from scratch, just</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">rm</span> -rf build</span><br></pre></td></tr></table></figure>



<h2 id="Compile-a-Library-amp-Link-it"><a href="#Compile-a-Library-amp-Link-it" class="headerlink" title="Compile a Library &amp; Link it"></a>Compile a Library &amp; Link it</h2><p><a target="_blank" rel="noopener" href="https://github.com/parallel101/course/tree/master/01/06">See code here</a></p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># compile static OR dynamic library</span></span><br><span class="line"><span class="keyword">add_library</span>(hellolib STATIC hello.cpp)</span><br><span class="line"><span class="keyword">add_library</span>(hellolib SHARED hello.cpp)</span><br><span class="line"></span><br><span class="line"><span class="keyword">add_executable</span>(a.out main.cpp)</span><br><span class="line"></span><br><span class="line"><span class="keyword">target_link_libraries</span>(a.out PUBLIC hellolib)</span><br></pre></td></tr></table></figure>

<ul>
<li>The <code>add_library</code> function’s signature is <code>add_library(target, STATIC/SHARED [src files...])</code>, meaning to use all <code>src files</code> to compile the static&#x2F;dynamic <code>target</code> library.</li>
<li>Then, <code>target_link_libraries(a.out PUBLIC hellolib)</code> links the <code>hellolib</code>‘s source to the <code>a.out</code>.</li>
</ul>
<h2 id="Compile-a-subdirectory"><a href="#Compile-a-subdirectory" class="headerlink" title="Compile a subdirectory"></a>Compile a subdirectory</h2><p><a target="_blank" rel="noopener" href="https://github.com/parallel101/course/tree/master/01/10">See code here</a></p>
<p>The sub-directory could contain a set of source codes to compile a library&#x2F;executable.</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># main CMakeLists.txt</span></span><br><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.12</span>)</span><br><span class="line"><span class="keyword">project</span>(hellocmake LANGUAGES CXX)</span><br><span class="line"></span><br><span class="line"><span class="keyword">add_subdirectory</span>(hellolib)  <span class="comment"># the name of subdirectory</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">add_executable</span>(a.out main.cpp)</span><br><span class="line"><span class="keyword">target_link_libraries</span>(a.out PUBLIC hellolib)</span><br></pre></td></tr></table></figure>

<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sub-directory CMakeLists.txt</span></span><br><span class="line"><span class="keyword">add_library</span>(hellolib STATIC hello.cpp)</span><br></pre></td></tr></table></figure>



<p>If the <code>main.cpp</code> uses the headers in the subdirectory <code>hellolib</code>, then <code>main.cpp</code> should write <code>#include &quot;hellolib/hello.h&quot;</code>. To simplify the <code>#include</code> statement, we could add the following to main’s <code>CMakeLists.txt</code>:</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">add_executable</span>(a.out main.cpp)</span><br><span class="line"><span class="keyword">target_include_directories</span>(a.out PUBLIC hellolib)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>This is still some complex. If we want to build two executable, we need write the following, with repeated code:</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">add_executable</span>(a.out main.cpp)</span><br><span class="line"><span class="keyword">target_include_directories</span>(a.out PUBLIC hellolib)</span><br><span class="line"><span class="keyword">add_executable</span>(b.out main.cpp)</span><br><span class="line"><span class="keyword">target_include_directories</span>(b.out PUBLIC hellolib)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>A solution is to move the <code>target_include_directories()</code> to the subdirectory. Then all the further library&#x2F;executable relied on the <code>hellolib</code> will include this subdirectory. </p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sub-directory</span></span><br><span class="line"><span class="keyword">target_include_directories</span>(hellolib PUBLIC .)</span><br></pre></td></tr></table></figure>

<p>If we change the <code>PUBLIC</code> to <code>PRIVATE</code>, then the further dependent would not have the effects.</p>
<h2 id="Further-options"><a href="#Further-options" class="headerlink" title="Further options"></a>Further options</h2><ul>
<li>Set C++ standard:</li>
</ul>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SET</span>(CMAKE_CXX_STANDARD <span class="number">17</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>Set global &#x2F; special macros:</li>
</ul>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># global</span></span><br><span class="line"><span class="keyword">add_definitions</span>(-DDEBUG)</span><br><span class="line"><span class="keyword">add_definitions</span>(DEBUG)</span><br><span class="line"><span class="comment"># special target</span></span><br><span class="line"><span class="keyword">target_compile_definitions</span>(a.out PUBLIC -DDEBUG)</span><br><span class="line"><span class="keyword">target_compile_definitions</span>(a.out PUBLIC DEBUG)</span><br><span class="line"></span><br><span class="line"><span class="comment"># They have the same effect as</span></span><br><span class="line">g++ xx.cpp -DDEBUG <span class="comment"># (define a `DEBUG` macro to the file)</span></span><br></pre></td></tr></table></figure>

<ul>
<li>Set global &#x2F; special compiling options:</li>
</ul>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># global</span></span><br><span class="line"><span class="keyword">add_compile_options</span>(-O2)</span><br><span class="line"><span class="comment"># special target</span></span><br><span class="line"><span class="keyword">target_compile_options</span>(a.out PUBLIC -O0)</span><br><span class="line"></span><br><span class="line"><span class="comment"># They have the same effect as</span></span><br><span class="line">g++ xx.cpp -O0 <span class="comment"># (add a `-O0` option in the compilation)</span></span><br></pre></td></tr></table></figure>

<ul>
<li>Set global &#x2F; special include directories:</li>
</ul>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># global</span></span><br><span class="line"><span class="keyword">include_directories</span>(hellolib)</span><br><span class="line"><span class="comment"># special target</span></span><br><span class="line"><span class="keyword">target_include_directories</span>(a.out PUBLIC hellolib)</span><br></pre></td></tr></table></figure>



<h2 id="CUDA-with-CMake"><a href="#CUDA-with-CMake" class="headerlink" title="CUDA with CMake"></a>CUDA with CMake</h2><p>A common template can be:</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CMAKE_MINIMUM_REQUIRED</span>(VERSION <span class="number">3.12</span>)</span><br><span class="line"><span class="keyword">PROJECT</span>(adder)</span><br><span class="line"></span><br><span class="line"><span class="keyword">SET</span>(CMAKE_CXX_STANDARD <span class="number">14</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">FIND_PACKAGE</span>(CUDA REQUIRED)</span><br><span class="line"></span><br><span class="line">CUDA_ADD_EXECUTABLE(adder main.cu)</span><br><span class="line"><span class="keyword">TARGET_LINK_LIBRARIES</span>(adder)</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/05/27/Pytorch-Practical-Basics-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Future">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Future's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/05/27/Pytorch-Practical-Basics-2/" class="post-title-link" itemprop="url">PyTorch Practical Hand-Written Modules Basics 2--Tensor basics (Tensor functions, "axis" and indexing)</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-05-27 13:32:58" itemprop="dateCreated datePublished" datetime="2023-05-27T13:32:58+08:00">2023-05-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-06-24 18:55:04" itemprop="dateModified" datetime="2023-06-24T18:55:04+08:00">2023-06-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>In this section, we will briefly talk about the arithmetic functions in the PyTorch. Then, we will introduce the <code>axis</code> parameter in most of these functions in detail. </p>
<p>Finally, we talk about indexing the tensor, which is very tricky in manipulating the tensors as well. </p>
<h2 id="Tensor-functions"><a href="#Tensor-functions" class="headerlink" title="Tensor functions"></a>Tensor functions</h2><p>PyTorch supports many arithmetic functions for tensor. They are vectorized and acts very similar to <code>numpy</code>. (So if you are not familiar with <code>numpy</code>, learn it first). In the following, I’ll introduce some functions with the official docs.</p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#pointwise-ops">binary arithmetic functions</a>, such as <code>+, -, *, /, @</code> etc. Entry-wise operations, supports <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/notes/broadcasting.html">broadcasting</a>.</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#pointwise-ops">binary logical functions</a>, such as <code>torch.bitwise_and()</code>, <code>torch.bitwise_or</code>…</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#pointwise-ops">math functions</a>, such as <code>exp, log, sigmoid</code> etc.</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#comparison-ops">comparison functions</a>, such as <code>torch.eq</code>, <code>torch.ge</code>. The <code>==</code> and <code>&gt;=</code> operators are overloaded, so they have the same effect.</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#reduction-ops">reduction functions</a>. They are usually very useful. e.g., <code>mean</code>, <code>median</code>, <code>argmax</code>, <code>sum</code>… They do the corresponding operations on a specific dimension, requiring the “dim” parameter (See below). </p>
</li>
<li><p>…… For more functions, please visit the <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#module-torch">docs</a>.</p>
</li>
</ul>
<h3 id="Key-What-is-the-“dim”-parameter"><a href="#Key-What-is-the-“dim”-parameter" class="headerlink" title="Key: What is the “dim” parameter?"></a>Key: What is the “dim” parameter?</h3><p>For the reduction functions such as <code>argmax</code>, we need to pass a parameter called <code>dim</code>. What does it mean?</p>
<ul>
<li><p>The default value or <code>dim</code> is <code>None</code>, indicates that do the <code>argmax</code> for all the entries. </p>
</li>
<li><p>On the other hand, if we specifies the <code>dim</code> parameter, that means, we apply the function <code>argmax</code> on each vector along a specific “axis”. For all of the example below, we use a <code>4x3x4</code> 3D tensor.</p>
</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create a 4x3x4 tensor</span></span><br><span class="line">a = torch.randn(<span class="number">4</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>



<ol>
<li>Then, in the first case, we do:</li>
</ol>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a1 = torch.argmax(a, dim=<span class="number">0</span>)</span><br><span class="line">a1.shape</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure>

<p>See the gif below. If we set <code>dim=0</code>, that means, we apply the <code>argmax</code> function on each yellow vector (they are in the direction of dim0). The original tensor’s shape is 4x3x4, we reduce on the dim0, so now it’s 3x4, containing all results from <code>argmax</code> on the yellow vectors. </p>
<img src="/2023/05/27/Pytorch-Practical-Basics-2/dim0.gif" alt="dim0" style="zoom: 67%;">



<ol start="2">
<li>Then, in the second case, we do:</li>
</ol>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a2 = torch.argmax(a, dim=<span class="number">1</span>)</span><br><span class="line">a2.shape</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure>

<p>See the gif below. If we set <code>dim=1</code>, that means, we apply the <code>argmax</code> function on each yellow vector (they are in the direction of dim1). The original tensor’s shape is 4x3x4, we reduce on the dim1, so now we will have a result with 4x4 shape. </p>
<img src="/2023/05/27/Pytorch-Practical-Basics-2/dim1.gif" alt="dim1" style="zoom: 67%;">



<ol start="3">
<li>Then, in the third case, we do:</li>
</ol>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a3 = torch.argmax(a, dim=<span class="number">2</span>)</span><br><span class="line">a3.shape</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>

<p>See the gif below. If we set <code>dim=2</code>, that means, we apply the <code>argmax</code> function on each yellow vector (they are in the direction of dim2). The original tensor’s shape is 4x3x4, we reduce on the dim2, so now we will have a result with 4x3 shape. </p>
<img src="/2023/05/27/Pytorch-Practical-Basics-2/dim2.gif" alt="dim2" style="zoom: 67%;">

<h3 id="As-member-function"><a href="#As-member-function" class="headerlink" title="As member function"></a>As member function</h3><p>Many functions mentioned above has member function style. For example, the following pairs are equivalent.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment"># pair1</span></span><br><span class="line">_ = torch.<span class="built_in">sum</span>(a)</span><br><span class="line">_ = a.<span class="built_in">sum</span>()</span><br><span class="line"><span class="comment"># pair2</span></span><br><span class="line">_ = torch.argmax(a, dim=<span class="number">0</span>)</span><br><span class="line">_ = a.argmax(dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<h3 id="As-in-place-function"><a href="#As-in-place-function" class="headerlink" title="As in-place function"></a>As in-place function</h3><p>The functions mentioned above returns a new result tensor, keeping the original one same. In some cases, we can do in-place operation on the tensor. The in-place functions are terminated with a <code>_</code>. </p>
<p>For example, the following pairs are equivalent.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment"># pair 1</span></span><br><span class="line">a = torch.cos(a)</span><br><span class="line">a = a.cos()</span><br><span class="line">a.cos_()</span><br><span class="line"><span class="comment"># pair 2</span></span><br><span class="line">a = torch.clamp(a, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">a = a.clamp(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">a.clamp_(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">a.clamp(<span class="number">1</span>, <span class="number">2</span>) <span class="comment"># Wrong: this line has no effect. The a remains same; the return value was assigned to nothing.</span></span><br></pre></td></tr></table></figure>





<h2 id="Tensor-indexing"><a href="#Tensor-indexing" class="headerlink" title="Tensor indexing"></a>Tensor indexing</h2><p>Indexing is very powerful in torch. They are very similar to the one in <code>numpy</code>.  Learn <code>numpy</code> first if you are not familiar with it.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># a is</span></span><br><span class="line">tensor([[ <span class="number">1.1351</span>,  <span class="number">0.7592</span>, -<span class="number">3.5945</span>],</span><br><span class="line">        [ <span class="number">0.0192</span>,  <span class="number">0.1052</span>,  <span class="number">0.9603</span>],</span><br><span class="line">        [-<span class="number">0.5672</span>, -<span class="number">0.5706</span>,  <span class="number">1.5980</span>],</span><br><span class="line">        [ <span class="number">0.1115</span>, -<span class="number">0.0392</span>,  <span class="number">1.4112</span>]])</span><br></pre></td></tr></table></figure>

<p>The indexing supports many types, you can pass:</p>
<ul>
<li><p>An integer. <code>a[1, 2]</code> returns just one value 0-D tensor <code>tensor(0.9603)</code>, one element at (row 1, col 2).</p>
</li>
<li><p>A Slice. <code>a[1::2, 2]</code> returns 1-D tensor <code>tensor([0.9603, 1.4112])</code>, two elements at (row 1, col 2) and (row 3, col 2).</p>
</li>
<li><p>A colon. colon means everything on this dim.<code>a[:, 2]</code> returns 1-D tensor <code>tensor([-3.5945,  0.9603,  1.5980,  1.4112])</code>, a column of 4 elements at col 2.</p>
</li>
<li><p>A None. None is used to create a new dim on the given axis. E.g., <code>a[:, None, :]</code> has the shape of <code>torch.Size([4, 1, 3])</code>. A further example:</p>
</li>
</ul>
<p>​		<code>a[:, 2]</code> returns 1-D vector <code>tensor([-3.5945,  0.9603,  1.5980,  1.4112])</code> of col 2.</p>
<p>​		<code>a[:, 2, None]</code> returns 2-D vector <code>tensor([[-3.5945],  [0.9603],  [1.5980],  [1.4112]])</code> of col 2, which the original shape is kept.</p>
<ul>
<li><p>A <code>...</code> (Ellipsis). Ellipsis can be used as multiple <code>:</code>. E.g., </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">16</span>).reshape(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"><span class="comment"># The following returns the same value</span></span><br><span class="line">a[..., <span class="number">1</span>]</span><br><span class="line">a[:, :, :, <span class="number">1</span>]</span><br></pre></td></tr></table></figure></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/05/27/Pytorch-Practical-Basics-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Future">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Future's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/05/27/Pytorch-Practical-Basics-1/" class="post-title-link" itemprop="url">PyTorch Practical Hand-Written Modules Basics 1--Tensor basics (attributes, creation)</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-05-27 11:47:00" itemprop="dateCreated datePublished" datetime="2023-05-27T11:47:00+08:00">2023-05-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-05-29 00:30:27" itemprop="dateModified" datetime="2023-05-29T00:30:27+08:00">2023-05-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>This series would not be a general PyTorch introduction or detailed tutorials. Instead, this would be a very practical introduction to some common basics needed for Implementing Hand-Written Modules. </p>
<p>This is the First Section of this series, we would like to introduce some tensor basics, including: tensor attributes, tensor creation, and some other things. All the things I mentioned will be practical, but not exhaustive. </p>
<h2 id="Tensor-attributes"><a href="#Tensor-attributes" class="headerlink" title="Tensor attributes"></a>Tensor attributes</h2><p>We introduce 5 key attributes for <code>torch.tensor</code> <code>a</code> here:</p>
<ul>
<li><code>a.shape</code>: Returns the shape of <code>a</code>. The return type is <code>torch.Size</code>. Example:</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">10</span>, <span class="number">20</span>) <span class="comment"># create a 10x20 tensor</span></span><br><span class="line">a.shape</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">torch.Size([<span class="number">10</span>, <span class="number">20</span>])</span><br></pre></td></tr></table></figure>

<p>The <code>torch.Size</code> object supports some tricks:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># unpack</span></span><br><span class="line">h, w = a.shape</span><br><span class="line">h, w</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">(<span class="number">10</span>, <span class="number">20</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># unpack in function calls</span></span><br><span class="line"><span class="built_in">print</span>(*a.shape)</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line"><span class="number">10</span> <span class="number">20</span></span><br></pre></td></tr></table></figure>



<ul>
<li><code>a.ndim</code>: Returns number of dimensions of <code>a</code>.</li>
</ul>
<p>It looks like <code>len(a.shape)</code>. It also has a function version, called <code>a.ndimension()</code></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a.ndim</span><br><span class="line">a.ndimension()</span><br><span class="line"><span class="built_in">len</span>(a.shape)</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">2</span></span><br></pre></td></tr></table></figure>



<ul>
<li><code>a.device</code>: Returns where the <code>a</code> locates.</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a.device</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">device(<span class="built_in">type</span>=<span class="string">&#x27;cpu&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>Convert to CUDA by using <code>a = a.to(&#39;cuda:0&#39;)</code>. Convert back to CPU by using <code>a = a.to(&#39;cpu&#39;)</code> or <code>a = a.cpu()</code>.</p>
<ul>
<li><code>a.dtype</code>: Returns the data type of <code>a</code>.</li>
</ul>
<p>The data type of tensor <code>a</code>. It’s very important in PyTorch! Usually, the data type would be <code>torch.float32</code> or <code>torch.int64</code>. Some data type convert method:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># to float32</span></span><br><span class="line">f = a.<span class="built_in">float</span>()</span><br><span class="line">f.dtype</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">torch.float32</span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># to int64</span></span><br><span class="line">l = a.long()</span><br><span class="line">l.dtype</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">torch.int64</span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># to int32</span></span><br><span class="line">i = a.<span class="built_in">int</span>()</span><br><span class="line">i.dtype</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">torch.int32</span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Also, we can use .to() as well:</span></span><br><span class="line">f = a.to(torch.float32)</span><br><span class="line">f.dtype</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">torch.float32</span><br></pre></td></tr></table></figure>



<ul>
<li><code>a.numel()</code>: Returns <strong>num</strong>ber of <strong>el</strong>ements in <code>a</code>. Usually used in counting number of parameters in the model.</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a.numel()</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line"><span class="number">200</span>  <span class="comment"># it&#x27;s 10*20!</span></span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">model = torchvision.models.resnet50()</span><br><span class="line"><span class="built_in">sum</span>([p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters()])</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line"><span class="number">25557032</span></span><br></pre></td></tr></table></figure>



<h2 id="Tensor-creation"><a href="#Tensor-creation" class="headerlink" title="Tensor creation"></a>Tensor creation</h2><p>PyTorch tensors plays key role in writing deep learning programs. Usually, tensor are from two types: data and auxiliary variables (e.g., masks).</p>
<ol>
<li>For the data tensor, they are usually converted from other packages, such as <code>numpy</code>. We have several methods to convert it to <code>torch.tensor</code>.</li>
</ol>
<ul>
<li><p><code>torch.tensor(arr)</code> Returns a deep copy of <code>arr</code>, i.e., the storage data is independent with <code>arr</code>.. (Very memory and time consuming, not recommended for most cases)</p>
</li>
<li><p><code>torch.from_numpy(arr)</code> Returns a shallow copy tensor, i.e., the storage data is shared with <code>arr</code>. </p>
</li>
<li><p><code>torch.as_tensor(arr, dtype=..., device=...)</code> If <code>dtype</code> and <code>device</code> is same as <code>arr</code>, then it behaves like <code>torch.from_numpy()</code> function, shallow copy. Otherwise, it acts like <code>torch.tensor()</code>, deep copy. So using the function is recommended.</p>
</li>
</ul>
<ol start="2">
<li>For the auxiliary variables, PyTorch provides some common methods:</li>
</ol>
<ul>
<li>Linear:</li>
</ul>
<p>We have <code>torch.linspace</code> and <code>torch.arange</code>. They are easy to understand. Please see the docs <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace">linspace</a> and <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.arange.html#torch.arange">arange</a>.</p>
<ul>
<li>Random:</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.randn(<span class="number">1</span>, <span class="number">2</span>) <span class="comment"># normal distribution, shape 1x2</span></span><br><span class="line">torch.rand(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># uniform[0, 1) distribution, shape 1x2</span></span><br><span class="line">torch.randint(<span class="number">0</span>, <span class="number">100</span>, (<span class="number">1</span>, <span class="number">2</span>)) <span class="comment"># uniform[0, 100) distribution, shape 1x2</span></span><br></pre></td></tr></table></figure>

<p>These functions also support passing in <code>torch.Size()</code> or a sequence as the size parameter. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">10</span>, <span class="number">10</span>) <span class="comment"># a is in shape 10x10</span></span><br><span class="line">torch.randn(a.shape)    <span class="comment"># good!</span></span><br><span class="line">torch.randn([<span class="number">10</span>, <span class="number">10</span>])   <span class="comment"># good!</span></span><br></pre></td></tr></table></figure>

<ul>
<li>Special tensors:</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.zeros(<span class="number">10</span>, <span class="number">10</span>) <span class="comment"># all zero tensor, shape 10x10</span></span><br><span class="line">torch.ones(<span class="number">10</span>, <span class="number">10</span>)  <span class="comment"># all one tensor, shape 10x10</span></span><br><span class="line"><span class="comment"># By default, the dtype is float32.</span></span><br></pre></td></tr></table></figure>

<ul>
<li><code>xxx_like()</code></li>
</ul>
<p>PyTorch has a series of function looks like <code>xxx_like()</code>, such as <code>ones_like()</code>, <code>zeros_like()</code>, <code>randn_like()</code>. These functions generates the tensor with the name, and the <code>dtype</code> and <code>device</code> and <code>layout</code> is same as the passing-in tensor. </p>
<p><code>torch.rand_like(input)</code> is equivalent to <code>torch.rand(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)</code>.</p>
<p>An example:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">arr = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], dtype=torch.float64)</span><br><span class="line"><span class="built_in">print</span>(arr.shape) <span class="comment"># torch.Size([3])</span></span><br><span class="line"><span class="built_in">print</span>(arr.dtype) <span class="comment"># torch.float64</span></span><br><span class="line"></span><br><span class="line">z = torch.zeros_like(arr)</span><br><span class="line"><span class="built_in">print</span>(z.shape)   <span class="comment"># torch.Size([3])</span></span><br><span class="line"><span class="built_in">print</span>(z.dtype)   <span class="comment"># torch.float64</span></span><br></pre></td></tr></table></figure>






      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/05/07/My-Setup-to-New-Linux-Server-Account/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Future">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Future's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/05/07/My-Setup-to-New-Linux-Server-Account/" class="post-title-link" itemprop="url">My Setup to New Linux Server Account</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-05-07 13:47:38" itemprop="dateCreated datePublished" datetime="2023-05-07T13:47:38+08:00">2023-05-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-05-27 16:52:48" itemprop="dateModified" datetime="2023-05-27T16:52:48+08:00">2023-05-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Techniques/" itemprop="url" rel="index"><span itemprop="name">Techniques</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>In this article, I list some procedures I do when setting up new Linux server account.</p>
<h2 id="1-Install-conda"><a href="#1-Install-conda" class="headerlink" title="1. Install conda"></a>1. Install conda</h2><p>Please refer to the official docs:</p>
<p><a target="_blank" rel="noopener" href="https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html">https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html</a></p>
<h2 id="2-Setup-ssh-key-password-free-login"><a href="#2-Setup-ssh-key-password-free-login" class="headerlink" title="2. Setup ssh key (password-free login)"></a>2. Setup ssh key (password-free login)</h2><p>Modified from <a target="_blank" rel="noopener" href="https://github.com/bokesyo/CSC4005_2022Fall_Demo/blob/main/docs/Instruction%20on%20Passwordlessly%20Connecting%20to%20Cluster%20by%20Setting%20Up%20SSH%20Key%20Authentification.md">here</a>.</p>
<p>(Replace <code>&#123;KeyName&#125;</code> to any name you like)</p>
<h3 id="2-1-Generate-SSH-Key-Pair-on-Your-Local-Machine"><a href="#2-1-Generate-SSH-Key-Pair-on-Your-Local-Machine" class="headerlink" title="2.1. Generate SSH Key Pair on Your Local Machine"></a>2.1. Generate SSH Key Pair on Your Local Machine</h3><p>(macOS)</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Execute the following commands on your local machine</span></span><br><span class="line"><span class="built_in">cd</span> ~/.ssh</span><br><span class="line">ssh-keygen -t rsa -b 1024 -f <span class="string">&quot;&#123;KeyName&#125;&quot;</span> -C <span class="string">&quot;&#123;Put Any Comment You Like&#125;&quot;</span></span><br><span class="line">ssh-add -K ./&#123;KeyName&#125;</span><br></pre></td></tr></table></figure>
<p>To check if you make it right, type the following command and you should see a string as the output.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> ~/.ssh/&#123;KeyName&#125;.pub</span><br></pre></td></tr></table></figure>

<p>(Windows)</p>
<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Execute the following commands on your local machine</span></span><br><span class="line"><span class="built_in">cd</span> C:\Users\&#123;UserName&#125;\.ssh</span><br><span class="line">ssh<span class="literal">-keygen</span> <span class="literal">-t</span> rsa <span class="literal">-b</span> <span class="number">1024</span> <span class="operator">-f</span> <span class="string">&quot;&#123;KeyName&#125;&quot;</span> <span class="literal">-C</span> <span class="string">&quot;&#123;Put Any Comment You Like&#125;&quot;</span></span><br></pre></td></tr></table></figure>
<p>To check if you make it right, double click the key file and you should see a string.</p>
<p><strong>Notes:</strong></p>
<ul>
<li>Execute ssh-keygen on your own computer instead of the cluster.</li>
<li>It is fine to use the SSH key file generated before (eg. id_rsa.pub), if you have.</li>
<li>ssh-keygen will ask you to set a paraphrase, which improves the security of using SSH key authentication. Type “Enter” for no paraphrase.</li>
</ul>
<p><strong>Parameters for ssh-keygen command:</strong></p>
<ul>
<li><strong>-t</strong>: type for ssh key generation. Here we use rsa</li>
<li><strong>-b</strong>: bits</li>
<li><strong>-f</strong>: name of your ssh key file. You are recommended to set this parameter in case it<br>is conflict with the ssh key file you generated before.</li>
<li><strong>-C</strong>: comments to distinguish the ssh key file from others</li>
</ul>
<h3 id="2-2-Transfer-Your-SSH-Public-Key-File-to-the-Server"><a href="#2-2-Transfer-Your-SSH-Public-Key-File-to-the-Server" class="headerlink" title="2.2. Transfer Your SSH Public Key File to the Server"></a>2.2. Transfer Your SSH Public Key File to the Server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Execute the following commands on your local machine</span></span><br><span class="line">scp ~/.ssh/&#123;KeyName&#125;.pub &#123;Your Account Name&#125;@&#123;Server ip&#125;:~</span><br></pre></td></tr></table></figure>
<p><strong>Notes:</strong></p>
<ul>
<li>This command will ask you for password for data transfer. Please make sure that you type in the correct password. If you are Windows user and you want to copy &amp; paste the password for convenience, try “Ctrl + Shift + V” if you fail to paste the password with “Ctrl + V”.</li>
</ul>
<h3 id="2-3-Configure-authorized-keys-on-the-Server"><a href="#2-3-Configure-authorized-keys-on-the-Server" class="headerlink" title="2.3. Configure authorized_keys on the Server"></a>2.3. Configure authorized_keys on the Server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Login the cluster with password</span></span><br><span class="line">ssh &#123;Your Account Name&#125;@&#123;Server ip&#125;</span><br><span class="line"><span class="comment">## All the following commands should be executed on the cluster</span></span><br><span class="line"><span class="built_in">mkdir</span> -p ~/.ssh</span><br><span class="line"><span class="built_in">cat</span> ./&#123;KeyName&#125;.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class="line"><span class="built_in">rm</span> -f ~/&#123;KeyName&#125;.pub</span><br><span class="line"><span class="built_in">chmod</span> 700 ~/.ssh</span><br><span class="line"><span class="built_in">chmod</span> 600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure>
<p>To check if you make it right, execute the following command and you should see a string as the output that is the same as in Step-1.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure>

<h3 id="2-4-Prepare-for-SSH-Connection-with-Key-Authentication"><a href="#2-4-Prepare-for-SSH-Connection-with-Key-Authentication" class="headerlink" title="2.4. Prepare for SSH Connection with Key Authentication"></a>2.4. Prepare for SSH Connection with Key Authentication</h3><p>Add the following content to <code>~/.ssh/config</code> file on your local machine:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Host &#123;Any Name You Like&#125;</span><br><span class="line">    HostName &#123;ip&#125;</span><br><span class="line">    IdentityFile ~/.ssh/&#123;KeyName&#125;</span><br><span class="line">    User &#123;Your Account Name&#125;</span><br></pre></td></tr></table></figure>
<p><strong>Notes:</strong></p>
<ul>
<li>It is recommended to configure with the Remote SSH extension on VS Code. Please refer to <a target="_blank" rel="noopener" href="https://code.visualstudio.com/docs/remote/ssh">Remote Development using SSH</a> for more information.</li>
</ul>
<h3 id="2-5-Login-the-Server-Password-free-with-SSH-key-Authentication"><a href="#2-5-Login-the-Server-Password-free-with-SSH-key-Authentication" class="headerlink" title="2.5. Login the Server Password-free with SSH key Authentication"></a>2.5. Login the Server Password-free with SSH key Authentication</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh &#123;Your Account Name&#125;@&#123;Server ip&#125;</span><br></pre></td></tr></table></figure>
<p>or</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh &#123;Your Student ID&#125;@&#123;Any Name You Like&#125; <span class="comment"># above in Sec. 4</span></span><br></pre></td></tr></table></figure>

<p>or </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh &#123;Any Name You Like&#125; <span class="comment"># above in Sec. 4</span></span><br></pre></td></tr></table></figure>



<h2 id="3-Set-some-alias"><a href="#3-Set-some-alias" class="headerlink" title="3. Set some alias"></a>3. Set some alias</h2><p>Add the following to the end of <code>~/.bashrc</code>:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> cf=<span class="string">&quot;ls -l | grep &quot;</span>^-<span class="string">&quot; | wc -l&quot;</span></span><br><span class="line"><span class="built_in">alias</span> ns=<span class="string">&quot;nvidia-smi&quot;</span></span><br></pre></td></tr></table></figure>

<p>The first alias, <code>cf</code> means count files, can count number of visible files (excluding directories) under the current working directory.</p>
<p>The second alias, <code>ns</code> is short for <code>nvidia-smi</code> to check for the GPU information.</p>
<p>To execute these commands, you just need to type <code>cf</code> <code>ns</code> in the command line.</p>
<h2 id="4-Configure-the-CUDA-compiler-version"><a href="#4-Configure-the-CUDA-compiler-version" class="headerlink" title="4. Configure the CUDA compiler version"></a>4. Configure the CUDA compiler version</h2><p>Add the following to the end of <code>~/.bashrc</code>:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=/usr/local/cuda-11.1/bin:<span class="variable">$PATH</span></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=/usr/local/cuda-11.1/lib64:<span class="variable">$LD_LIBRARY_PATH</span></span><br></pre></td></tr></table></figure>

<p>Change the CUDA directory (<code>/usr/local/cuda-11.1</code> above) to the one you actually want to use.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/05/07/Conda-common-commands/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Future">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Future's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/05/07/Conda-common-commands/" class="post-title-link" itemprop="url">Conda Common Commands Quick Reference</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-05-07 13:46:58" itemprop="dateCreated datePublished" datetime="2023-05-07T13:46:58+08:00">2023-05-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-05-31 17:28:41" itemprop="dateModified" datetime="2023-05-31T17:28:41+08:00">2023-05-31</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/QRH/" itemprop="url" rel="index"><span itemprop="name">QRH</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h2><p>Please refer to the official <a target="_blank" rel="noopener" href="https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html">docs</a>. </p>
<p>Mirrors: to download faster in China, it’s recommended to add some mirror configs:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ </span><br><span class="line">conda config --<span class="built_in">set</span> show_channel_urls <span class="built_in">yes</span></span><br></pre></td></tr></table></figure>



<h2 id="Common-commands"><a href="#Common-commands" class="headerlink" title="Common commands"></a>Common commands</h2><h3 id="Create-env"><a href="#Create-env" class="headerlink" title="Create env"></a>Create env</h3><p>Create an environment with <code>&lt;env_name&gt;</code> and python with <code>3.9</code>. </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n &lt;env_name&gt; python=3.9</span><br></pre></td></tr></table></figure>

<h3 id="Activate-env"><a href="#Activate-env" class="headerlink" title="Activate env"></a>Activate env</h3><p>Activate &#x2F; Deactivate the environment <code>&lt;env_name&gt;</code>.</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># activate</span></span><br><span class="line">conda activate &lt;env_name&gt;</span><br><span class="line"><span class="comment"># deactivate</span></span><br><span class="line">conda deactivate &lt;env_name&gt;</span><br></pre></td></tr></table></figure>

<h3 id="Remove-env"><a href="#Remove-env" class="headerlink" title="Remove env"></a>Remove env</h3><p>Remove the environment <code>&lt;env_name&gt;</code>.</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda remove -n &lt;env_name&gt; --all</span><br></pre></td></tr></table></figure>

<h3 id="Install-packages"><a href="#Install-packages" class="headerlink" title="Install packages"></a>Install packages</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># with conda</span></span><br><span class="line">conda install &lt;packages&gt;</span><br><span class="line"><span class="comment"># with pip</span></span><br><span class="line">pip install &lt;packages&gt;</span><br><span class="line"><span class="comment"># with pip and mirrors</span></span><br><span class="line">pip install &lt;packages&gt; -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>

<h3 id="Update-packages"><a href="#Update-packages" class="headerlink" title="Update packages"></a>Update packages</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda update &lt;packages&gt;</span><br></pre></td></tr></table></figure>

<h3 id="List-packages-in-env"><a href="#List-packages-in-env" class="headerlink" title="List packages in env"></a>List packages in env</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda list</span><br></pre></td></tr></table></figure>

<h3 id="List-all-envs"><a href="#List-all-envs" class="headerlink" title="List all envs"></a>List all envs</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda info --envs</span><br></pre></td></tr></table></figure>

<h3 id="Clean-caches"><a href="#Clean-caches" class="headerlink" title="Clean caches"></a>Clean caches</h3><p>The downloaded caches occupies the storage data. We may clean them by the following commands:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda clean --all</span><br></pre></td></tr></table></figure>

<h3 id="Export-env-configs"><a href="#Export-env-configs" class="headerlink" title="Export env configs"></a>Export env configs</h3><p>Export the current environment configs to a yaml file, then we can follow the yaml file to create another environment.</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># change to the desired env</span></span><br><span class="line">conda activate &lt;env_name&gt;</span><br><span class="line"><span class="comment"># export </span></span><br><span class="line">conda <span class="built_in">env</span> <span class="built_in">export</span> &gt; environment.yml</span><br><span class="line"><span class="comment"># import and create</span></span><br><span class="line">conda <span class="built_in">env</span> create -f environment.yml</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/05/06/CUDA-No-process-but-GPU-memory-occupied/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Future">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Future's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/05/06/CUDA-No-process-but-GPU-memory-occupied/" class="post-title-link" itemprop="url">CUDA No process found but GPU memory occupied</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-05-06 01:07:19" itemprop="dateCreated datePublished" datetime="2023-05-06T01:07:19+08:00">2023-05-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-05-27 16:43:05" itemprop="dateModified" datetime="2023-05-27T16:43:05+08:00">2023-05-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Problem-Solving/" itemprop="url" rel="index"><span itemprop="name">Problem Solving</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h2><p>when typing <code>nvidia-smi</code>, we find there is GPU memory occupied (See Red Boxes), but we cannot see any relevant process on that GPU (See Orange Boxes).</p>
<img src="/2023/05/06/CUDA-No-process-but-GPU-memory-occupied/problem.png" alt="problem" style="zoom:50%;">

<h2 id="Possible-Answer"><a href="#Possible-Answer" class="headerlink" title="Possible Answer"></a>Possible Answer</h2><p>This can be caused by <code>torch.distributed</code> and other multi-processing CUDA programs. When the main process terminated, the background process still alive, not killed.</p>
<ol>
<li>To figure which processes used the GPU, we can use the following command:</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">fuser -v /dev/nvidia&lt;<span class="built_in">id</span>&gt;</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">                     Users     PID         Command</span><br><span class="line">/dev/nvidia5:        XXXXXX    14701 F...m python</span><br><span class="line">                     XXXXXX    14703 F...m python</span><br><span class="line">                     XXXXXX    14705 F...m python</span><br><span class="line">                     XXXXXX    14706 F...m python</span><br><span class="line">                     XXXXXX    37041 F...m python</span><br><span class="line">                     XXXXXX    37053 F...m python</span><br></pre></td></tr></table></figure>

<p>This will list all of the processes that use GPU. Note that if this is executed from a <strong>normal user</strong>, then only the user’s processes displayed. If this is executed from <strong>root</strong>, then all user’s relevant processes will be displayed.</p>
<ol start="2">
<li>Then use the following command to kill the process shown above.</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">kill</span> -9 [PID]</span><br></pre></td></tr></table></figure>

<p>That will kill the process on the GPU. After killing the processes, you will find the GPU memory is freed. If still occupied, this may be caused by other users. You need to ask other users&#x2F;administrators to kill it manually.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/05/02/common-latex-templates/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Future">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Future's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/05/02/common-latex-templates/" class="post-title-link" itemprop="url">Common LaTeX Blocks Templates</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-05-02 16:19:21" itemprop="dateCreated datePublished" datetime="2023-05-02T16:19:21+08:00">2023-05-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-05-07 13:42:42" itemprop="dateModified" datetime="2023-05-07T13:42:42+08:00">2023-05-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Techniques/" itemprop="url" rel="index"><span itemprop="name">Techniques</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Set-geometries"><a href="#Set-geometries" class="headerlink" title="Set geometries"></a>Set geometries</h2><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\usepackage</span>&#123;geometry&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">\geometry</span>&#123;a4paper, top = 1.25in, bottom = 1.25in, left = 1.25in, right = 1.25in, headheight = 1.25in&#125;</span><br></pre></td></tr></table></figure>



<h2 id="Figures"><a href="#Figures" class="headerlink" title="Figures"></a>Figures</h2><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\usepackage</span>&#123;float&#125;</span><br><span class="line"><span class="keyword">\usepackage</span>&#123;subcaption&#125;</span><br><span class="line"><span class="keyword">\usepackage</span>&#123;graphicx&#125; </span><br></pre></td></tr></table></figure>

<img src="/2023/05/02/common-latex-templates/figures.png" alt="figures" style="zoom:60%;">

<h3 id="Single-Figure"><a href="#Single-Figure" class="headerlink" title="Single Figure:"></a>Single Figure:</h3><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\begin</span>&#123;figure&#125;[H]</span><br><span class="line">    <span class="keyword">\centering</span></span><br><span class="line">    <span class="keyword">\includegraphics</span>[width=0.35<span class="keyword">\textwidth</span>]&#123;blankfig.png&#125;</span><br><span class="line">    <span class="keyword">\caption</span>&#123;XXXXXX&#125;</span><br><span class="line">    <span class="comment">% \label&#123;XXXX&#125;</span></span><br><span class="line"><span class="keyword">\end</span>&#123;figure&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Subfigures"><a href="#Subfigures" class="headerlink" title="Subfigures:"></a>Subfigures:</h3><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\begin</span>&#123;figure&#125;[H]</span><br><span class="line">     <span class="keyword">\centering</span></span><br><span class="line">     <span class="keyword">\begin</span>&#123;subfigure&#125;[b]&#123;0.47<span class="keyword">\textwidth</span>&#125;</span><br><span class="line">         <span class="keyword">\centering</span></span><br><span class="line">         <span class="keyword">\includegraphics</span>[width=<span class="keyword">\textwidth</span>]&#123;blankfig.png&#125;</span><br><span class="line">         <span class="keyword">\caption</span>&#123;xxxx&#125;</span><br><span class="line">         <span class="comment">% \label&#123;xxxx&#125;</span></span><br><span class="line">     <span class="keyword">\end</span>&#123;subfigure&#125;</span><br><span class="line">     <span class="comment">% \hfill</span></span><br><span class="line">     <span class="keyword">\begin</span>&#123;subfigure&#125;[b]&#123;0.47<span class="keyword">\textwidth</span>&#125;</span><br><span class="line">         <span class="keyword">\centering</span></span><br><span class="line">         <span class="keyword">\includegraphics</span>[width=<span class="keyword">\textwidth</span>]&#123;blankfig.png&#125;</span><br><span class="line">         <span class="keyword">\caption</span>&#123;xxxx&#125;</span><br><span class="line">         <span class="comment">% \label&#123;xxxx&#125;</span></span><br><span class="line">     <span class="keyword">\end</span>&#123;subfigure&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">\caption</span>&#123;XXXX&#125;</span><br><span class="line">    <span class="comment">% \label&#123;xxxx&#125;</span></span><br><span class="line"><span class="keyword">\end</span>&#123;figure&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Parallel-Figures"><a href="#Parallel-Figures" class="headerlink" title="Parallel Figures"></a>Parallel Figures</h3><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\begin</span>&#123;figure&#125;[H]<span class="comment">%[htp]</span></span><br><span class="line"><span class="keyword">\centering</span></span><br><span class="line">    <span class="keyword">\begin</span>&#123;minipage&#125;[t]&#123;0.47<span class="keyword">\textwidth</span>&#125;</span><br><span class="line">        <span class="keyword">\centering</span></span><br><span class="line">        <span class="keyword">\includegraphics</span>[width=1<span class="keyword">\textwidth</span>]&#123;blankfig.png&#125;</span><br><span class="line">        <span class="keyword">\caption</span>&#123;XXXX&#125;</span><br><span class="line">    <span class="comment">% \label&#123;XXXX&#125;</span></span><br><span class="line">    <span class="keyword">\end</span>&#123;minipage&#125;</span><br><span class="line">    <span class="keyword">\begin</span>&#123;minipage&#125;[t]&#123;0.47<span class="keyword">\textwidth</span>&#125;</span><br><span class="line">        <span class="keyword">\centering</span></span><br><span class="line">        <span class="keyword">\includegraphics</span>[width=1<span class="keyword">\textwidth</span>]&#123;blankfig.png&#125;</span><br><span class="line">        <span class="keyword">\caption</span>&#123;XXXX&#125;</span><br><span class="line">        <span class="comment">% \label&#123;XXXX&#125;</span></span><br><span class="line">    <span class="keyword">\end</span>&#123;minipage&#125;</span><br><span class="line"><span class="keyword">\end</span>&#123;figure&#125;</span><br></pre></td></tr></table></figure>



<h2 id="Pseudo-codes"><a href="#Pseudo-codes" class="headerlink" title="Pseudo-codes"></a>Pseudo-codes</h2><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\usepackage</span>[ruled, vlined, linesnumbered]&#123;algorithm2e&#125;</span><br></pre></td></tr></table></figure>

<img src="/2023/05/02/common-latex-templates/pseudo-codes.png" alt="pseudo-codes" style="zoom:60%;">

<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\begin</span>&#123;algorithm&#125;[H]</span><br><span class="line">	<span class="keyword">\caption</span>&#123;Residual&#x27;s Distribution Simulation&#125;</span><br><span class="line">	<span class="keyword">\BlankLine</span></span><br><span class="line">    <span class="keyword">\SetAlgoLined</span></span><br><span class="line">	<span class="keyword">\KwIn</span>&#123;Number of sample needed (<span class="built_in">$</span>num<span class="built_in">_</span>&#123;sample&#125;<span class="built_in">$</span>), <span class="built_in">$</span>w<span class="built_in">_</span>1<span class="built_in">$</span>,<span class="built_in">$</span>w<span class="built_in">_</span>2<span class="built_in">$</span>,<span class="built_in">$</span>w<span class="built_in">_</span>3<span class="built_in">$</span>,<span class="built_in">$</span><span class="keyword">\mu</span><span class="built_in">_</span>1<span class="built_in">$</span>,<span class="built_in">$</span><span class="keyword">\mu</span><span class="built_in">_</span>2<span class="built_in">$</span>,<span class="built_in">$</span><span class="keyword">\mu</span><span class="built_in">_</span>3<span class="built_in">$</span>,<span class="built_in">$</span><span class="keyword">\sigma</span><span class="built_in">_</span>1<span class="built_in">$</span>,<span class="built_in">$</span><span class="keyword">\sigma</span><span class="built_in">_</span>2<span class="built_in">$</span>,<span class="built_in">$</span><span class="keyword">\sigma</span><span class="built_in">_</span>3<span class="built_in">$</span>.&#125;</span><br><span class="line">	<span class="keyword">\KwOut</span>&#123;A sequence of random variables <span class="built_in">$</span><span class="keyword">\&#123;</span>X<span class="built_in">_</span>i<span class="keyword">\&#125;</span><span class="built_in">_</span>&#123;i = 1&#125;<span class="built_in">^</span>&#123;num<span class="built_in">_</span>&#123;sample&#125;&#125;<span class="built_in">$</span> following target distribution.&#125; </span><br><span class="line"></span><br><span class="line">	<span class="keyword">\BlankLine</span></span><br><span class="line">    i = 0<span class="keyword">\\</span></span><br><span class="line">    <span class="keyword">\While</span>&#123;<span class="built_in">$</span> i &lt; num<span class="built_in">_</span>&#123;sample&#125; <span class="built_in">$</span>&#125;&#123;</span><br><span class="line">        <span class="built_in">$</span>v<span class="built_in">_</span>i<span class="built_in">$</span> <span class="built_in">$</span><span class="keyword">\sim</span> <span class="keyword">\mathcal</span>&#123;U&#125;[0,1]<span class="built_in">$</span><span class="keyword">\\</span></span><br><span class="line">        <span class="keyword">\uIf</span>&#123;<span class="built_in">$</span>0&lt;v<span class="built_in">_</span>i <span class="keyword">\leq</span> w<span class="built_in">_</span>1<span class="built_in">$</span>&#125;&#123;</span><br><span class="line">            <span class="built_in">$</span>X<span class="built_in">_</span>i <span class="keyword">\sim</span> <span class="keyword">\mathcal</span>&#123;N&#125;(<span class="keyword">\mu</span><span class="built_in">_</span>1,<span class="keyword">\sigma</span><span class="built_in">_</span>1<span class="built_in">^</span>2)<span class="built_in">$</span> <span class="keyword">\;</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">\uElseIf</span>&#123;<span class="built_in">$</span>w<span class="built_in">_</span>1&lt;v<span class="built_in">_</span>i<span class="keyword">\leq</span> w<span class="built_in">_</span>2<span class="built_in">$</span>&#125;&#123;</span><br><span class="line">            <span class="built_in">$</span>X<span class="built_in">_</span>i <span class="keyword">\sim</span> <span class="keyword">\mathcal</span>&#123;N&#125;(<span class="keyword">\mu</span><span class="built_in">_</span>2,<span class="keyword">\sigma</span><span class="built_in">_</span>2<span class="built_in">^</span>2)<span class="built_in">$</span> <span class="keyword">\;</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">\Else</span>&#123;</span><br><span class="line">            <span class="built_in">$</span>X<span class="built_in">_</span>i <span class="keyword">\sim</span> <span class="keyword">\mathcal</span>&#123;N&#125;(<span class="keyword">\mu</span><span class="built_in">_</span>3,<span class="keyword">\sigma</span><span class="built_in">_</span>3<span class="built_in">^</span>2)<span class="built_in">$</span> <span class="keyword">\;</span></span><br><span class="line">        &#125;</span><br><span class="line">        i = i + 1</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">\Return</span>&#123;<span class="built_in">$</span><span class="keyword">\&#123;</span>X<span class="built_in">_</span>i<span class="keyword">\&#125;</span><span class="built_in">_</span>&#123;i = 1&#125;<span class="built_in">^</span>&#123;num<span class="built_in">_</span>&#123;sample&#125;&#125;<span class="built_in">$</span>    &#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">\BlankLine</span></span><br><span class="line"><span class="keyword">\end</span>&#123;algorithm&#125;</span><br></pre></td></tr></table></figure>



<h2 id="Tables"><a href="#Tables" class="headerlink" title="Tables"></a>Tables</h2><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\usepackage</span>&#123;tabularx&#125;</span><br><span class="line"><span class="keyword">\usepackage</span>&#123;booktabs&#125;</span><br></pre></td></tr></table></figure>

<img src="/2023/05/02/common-latex-templates/tables.png" alt="tables" style="zoom:60%;">

<h3 id="Narrow-table"><a href="#Narrow-table" class="headerlink" title="Narrow table"></a>Narrow table</h3><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\begin</span>&#123;table&#125;[H]</span><br><span class="line">    <span class="keyword">\centering</span><span class="keyword">\small</span></span><br><span class="line">    <span class="keyword">\setlength</span>&#123;<span class="keyword">\tabcolsep</span>&#125;&#123;3mm&#125;&#123;</span><br><span class="line">        <span class="keyword">\caption</span>&#123;XXXX&#125;</span><br><span class="line">        <span class="keyword">\begin</span>&#123;tabular&#125;&#123;cccc&#125;</span><br><span class="line">            <span class="keyword">\specialrule</span>&#123;0.05em&#125;&#123;3pt&#125;&#123;3pt&#125;</span><br><span class="line">            <span class="keyword">\toprule</span></span><br><span class="line">            X <span class="built_in">&amp;</span> X <span class="built_in">&amp;</span> X <span class="built_in">&amp;</span> X  <span class="keyword">\\</span></span><br><span class="line">            <span class="keyword">\midrule</span></span><br><span class="line">            XXX <span class="built_in">&amp;</span> 0.928 <span class="built_in">&amp;</span> 0.2935 <span class="built_in">&amp;</span> 1.000 <span class="keyword">\\</span></span><br><span class="line">            XXX <span class="built_in">&amp;</span> 0.747 <span class="built_in">&amp;</span> 0.0526 <span class="built_in">&amp;</span> 1.301 <span class="keyword">\\</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">\specialrule</span>&#123;0.05em&#125;&#123;3pt&#125;&#123;3pt&#125; </span><br><span class="line">        <span class="keyword">\bottomrule</span></span><br><span class="line">        <span class="keyword">\label</span>&#123;tab:compare2&#125;</span><br><span class="line">    	<span class="keyword">\end</span>&#123;tabular&#125;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">\end</span>&#123;table&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Text-width-table"><a href="#Text-width-table" class="headerlink" title="Text-width table"></a>Text-width table</h3><p>The width may be adjusted manually.</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\begin</span>&#123;table&#125;[H]</span><br><span class="line"><span class="keyword">\centering</span></span><br><span class="line">    <span class="keyword">\caption</span>&#123;Experiment results for different &quot;joins&quot;&#125;</span><br><span class="line">    <span class="keyword">\label</span>&#123;X&#125;</span><br><span class="line">    <span class="keyword">\begin</span>&#123;tabularx&#125;&#123;<span class="keyword">\textwidth</span>&#125;&#123;X X X&#125;</span><br><span class="line">    <span class="keyword">\toprule</span></span><br><span class="line">    Header 1 <span class="built_in">&amp;</span> Header 2 <span class="built_in">&amp;</span> Header 3 <span class="keyword">\\</span></span><br><span class="line">    <span class="keyword">\midrule</span></span><br><span class="line">    Data 1   <span class="built_in">&amp;</span> Data 2   <span class="built_in">&amp;</span> Data 3   <span class="keyword">\\</span></span><br><span class="line">    Data 4   <span class="built_in">&amp;</span> Data 5   <span class="built_in">&amp;</span> Data 6   <span class="keyword">\\</span></span><br><span class="line">    <span class="keyword">\bottomrule</span></span><br><span class="line">    <span class="keyword">\end</span>&#123;tabularx&#125;</span><br><span class="line">    </span><br><span class="line"><span class="keyword">\end</span>&#123;table&#125;</span><br></pre></td></tr></table></figure>



<h2 id="Listing"><a href="#Listing" class="headerlink" title="Listing"></a>Listing</h2><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\usepackage</span>&#123;listings&#125;</span><br></pre></td></tr></table></figure>

<img src="/2023/05/02/common-latex-templates/listings.png" alt="listings" style="zoom:60%;">

<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% outside of document</span></span><br><span class="line"><span class="keyword">\lstset</span>&#123;</span><br><span class="line">    language=SQL,    </span><br><span class="line">    basicstyle = <span class="keyword">\tiny</span><span class="keyword">\ttfamily</span>,</span><br><span class="line">    breaklines=true,</span><br><span class="line">    numberstyle=<span class="keyword">\tiny</span>,keywordstyle=<span class="keyword">\color</span>&#123;blue!70&#125;,</span><br><span class="line">    commentstyle=<span class="keyword">\color</span>&#123;red!50!green!50!blue!50&#125;,frame=shadowbox,</span><br><span class="line">    columns=flexible,</span><br><span class="line">    rulesepcolor=<span class="keyword">\color</span>&#123;red!20!green!20!blue!20&#125;,basicstyle=<span class="keyword">\ttfamily</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">% inside of document</span></span><br><span class="line"><span class="keyword">\begin</span>&#123;lstlisting&#125;</span><br><span class="line">    SELECT u.province, c.chip<span class="built_in">_</span>name AS ChipName, SUM(p.budget) AS revenue</span><br><span class="line">    FROM user AS u NATURAL JOIN package AS p, chip AS c</span><br><span class="line">    WHERE p.package<span class="built_in">_</span>id=c.package<span class="built_in">_</span>id AND province IN (<span class="comment">%s)</span></span><br><span class="line">    GROUP BY c.chip<span class="built_in">_</span>name</span><br><span class="line">    ORDER BY SUM(p.budget) DESC; </span><br><span class="line"><span class="keyword">\end</span>&#123;lstlisting&#125;</span><br></pre></td></tr></table></figure>

<img src="/2023/05/02/common-latex-templates/listing2.png" alt="listing2" style="zoom:60%;">

<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% outside of document</span></span><br><span class="line"><span class="keyword">\lstset</span>&#123;</span><br><span class="line">  language=Python,</span><br><span class="line">  basicstyle=<span class="keyword">\small</span><span class="keyword">\ttfamily</span>,</span><br><span class="line">  commentstyle=<span class="keyword">\color</span>&#123;gray&#125;,</span><br><span class="line">  keywordstyle=<span class="keyword">\color</span>&#123;blue&#125;<span class="keyword">\bfseries</span>,</span><br><span class="line">  stringstyle=<span class="keyword">\color</span>&#123;red&#125;,</span><br><span class="line">  showstringspaces=false,</span><br><span class="line">  numbers=left,</span><br><span class="line">  numberstyle=<span class="keyword">\tiny</span><span class="keyword">\color</span>&#123;gray&#125;,</span><br><span class="line">  stepnumber=1,</span><br><span class="line">  numbersep=10pt,</span><br><span class="line">  tabsize=4,</span><br><span class="line">  showspaces=false,</span><br><span class="line">  showtabs=false,</span><br><span class="line">  breaklines=true,</span><br><span class="line">  breakatwhitespace=true,</span><br><span class="line">  aboveskip=<span class="keyword">\bigskipamount</span>,</span><br><span class="line">  belowskip=<span class="keyword">\bigskipamount</span>,</span><br><span class="line">  frame=single</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">% inside of document</span></span><br><span class="line"><span class="keyword">\begin</span>&#123;lstlisting&#125;</span><br><span class="line">    print(&quot;hello world!&quot;)</span><br><span class="line">    for qid in range(hs.shape[0]):</span><br><span class="line">    if qid &lt; 1:</span><br><span class="line">        lvl = 0</span><br><span class="line">    elif qid &gt;= 1 and qid &lt; 3:</span><br><span class="line">        lvl = 1</span><br><span class="line">    elif qid &gt;= 3 and qid &lt; 6:</span><br><span class="line">        lvl = 2</span><br><span class="line">    elif qid &gt;= 6 and qid &lt; 11:</span><br><span class="line">        lvl = 3</span><br><span class="line"><span class="keyword">\end</span>&#123;lstlisting&#125;</span><br></pre></td></tr></table></figure>








      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Future</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">20</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Future</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
