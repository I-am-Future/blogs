<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/time-machine.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Future&#39;s blog">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Future&#39;s blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Future">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Future's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Future's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/21/%E6%9C%AC%E7%AB%99%E9%A3%9F%E7%94%A8%E6%8C%87%E5%8D%97-How-to-use-this-blog/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Future">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Future's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/03/21/%E6%9C%AC%E7%AB%99%E9%A3%9F%E7%94%A8%E6%8C%87%E5%8D%97-How-to-use-this-blog/" class="post-title-link" itemprop="url">How to use this blog 本站食用指南</a>
        </h2>

        <div class="post-meta">
          
              <i class="fa fa-thumbtack"></i>
              <font color=7D26CD>Pinned</font>
              <span class="post-meta-divider">|</span>
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-03-21 16:14:47" itemprop="dateCreated datePublished" datetime="2023-03-21T16:14:47+08:00">2023-03-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-05-29 11:23:24" itemprop="dateModified" datetime="2023-05-29T11:23:24+08:00">2023-05-29</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Hello! 你好！</p>
<p>This blog is established on 2023&#x2F;3&#x2F;21. The aim to build such a blog website which is used to share my programming &#x2F; other knowledges.</p>
<p>本站建立于2023.3.21，建立之初是为了拥有一个自己的知识内容分享平台。</p>
<p>This is the home page, and this article is pinned. If you want to search for some past articles, use “Categories”, “Archives” or “Search” on the top of the page.</p>
<p>这是主页。这篇文章置顶于此。如果你想找指定文章内容，可以在页面上方“Categories”或者“Archives”寻找。</p>
<p><strong>Future and Plans 计划与憧憬</strong></p>
<p>On the first day of our blog, let’s plan it: 建博客第一天，就先来立一些这个博客的计划：</p>
<ul>
<li>Language 语言：We will mainly use English in this blog. 网站主要使用英文。（单词也比较简单，对于中文用户其实阅读难度不大啦）</li>
<li>Contents 内容：The content should mainly be my original content, or a comprehensive content that combines other existing materials with my annotations or supplements. 内容主要应该是我的原创内容，或者是结合了其他现有资料，配合我的注释或是增补的综合内容。</li>
<li>Categories 类别：<ul>
<li>Deep Learning: Things related to PyTorch, deep learning。和torch，深度学习相关的会在这。</li>
<li>QRH: Quick Reference Handbook of tools&#x2F;programming languages&#x2F;packages, 一些工具、软件、包、语言的快速参考手册。</li>
<li>QuickIntro: Quick Introduction to a tools&#x2F;programming languages&#x2F;packages, 快速入门某一工具、软件、包、语言。</li>
<li>Techniques: Techniques and tricks in something. 技艺，一些环境配置、瞎折腾的技术、搭建过程会写在这。</li>
<li>Problem Solving: Record the problems I met and how did I solve it out. 记录我遇到的问题以及我的解决方案。</li>
<li>Others: 其它。</li>
</ul>
</li>
</ul>
<p><strong>TODO list：</strong></p>
<p>在下面列一些近期可能想做的topic（立flag中）</p>
<ul>
<li>QRH</li>
</ul>
<ul>
<li><input checked disabled type="checkbox"> Matplotlib common functions</li>
<li><input disabled type="checkbox"> conda common commands</li>
<li><input disabled type="checkbox"> Linux shell common commands</li>
<li><input disabled type="checkbox"> vim common keys</li>
<li><input checked disabled type="checkbox"> latex common code blocks</li>
<li><input disabled type="checkbox"> </li>
</ul>
<ul>
<li>QuickIntro</li>
</ul>
<ul>
<li><input checked disabled type="checkbox"> NumPy quick intro</li>
<li><input disabled type="checkbox"> 15 min regex</li>
<li><input disabled type="checkbox"> 15 min  Makefile</li>
<li><input disabled type="checkbox"> 15 min  MySQL</li>
<li><input disabled type="checkbox"> </li>
</ul>
<ul>
<li>Techniques</li>
</ul>
<ul>
<li><input checked disabled type="checkbox"> C++ complex type declaration rules (In Chinese, 2&#x2F;2)</li>
<li><input disabled type="checkbox"> </li>
</ul>
<ul>
<li>Others</li>
</ul>
<ul>
<li><input disabled type="checkbox"> Emm…</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/02/Pytorch-Practical-Basics-8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Future">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Future's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/02/Pytorch-Practical-Basics-8/" class="post-title-link" itemprop="url">PyTorch Practical Hand-Written Modules Basics 8--Linear Layer Ops on C++/CUDA</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-08-02 12:03:42" itemprop="dateCreated datePublished" datetime="2023-08-02T12:03:42+08:00">2023-08-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-08-06 19:13:54" itemprop="dateModified" datetime="2023-08-06T19:13:54+08:00">2023-08-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>In the section 6 to 9, we’ll investigate how to use <code>torch.autograd.Function</code> to implement the hand-written operators. The tentative outline is:</p>
<ul>
<li><input checked disabled type="checkbox"> In the section 6, we talk about the basics of <code>torch.autograd.Function</code>. <strong>The operators defined by <code>torch.autograd.Function</code> can be automatically back-propagated.</strong></li>
<li><input checked disabled type="checkbox"> In the last section (7), we’ll talk about <strong>mathematic derivation</strong> for the “linear layer” operator. </li>
<li><input checked disabled type="checkbox"> In this section (8), we talk about <strong>writing C++ CUDA extension</strong> for the “linear layer” operator. </li>
<li><input disabled type="checkbox"> In the section 9, we talk details about building the extension to a module, as well as testing. Then we’ll conclude the things we’ve done so far.</li>
</ul>
<p><strong>Note:</strong></p>
<ul>
<li>This blog is written with following reference: <ul>
<li>PyTorch official tutorial about CUDA extension: <a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/advanced/cpp_extension.html">website</a>.</li>
<li>YouTube video about writing CUDA extension: <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=l_Rpk6CRJYI">video</a>, <a target="_blank" rel="noopener" href="https://github.com/kwea123/pytorch-cppcuda-tutorial">code</a>.</li>
</ul>
</li>
<li>For how to write CUDA code, you can follow <a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">official documentation</a>, <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/34587739">blogs</a> (In Chinese). You can search by yourself for English tutorials and video tutorials.</li>
<li>This blog only talk some important points in the matrix multiplication example. Code are picked by pieces for illustration. Whole code is at: <a target="_blank" rel="noopener" href="https://github.com/I-am-Future/PyTorch-Linear-Operator-CUDA">code</a>.</li>
</ul>
<h1 id="Overall-Structure"><a href="#Overall-Structure" class="headerlink" title="Overall Structure"></a>Overall Structure</h1><p>The general structure for our PyTorch C++ &#x2F; CUDA extension looks like following:</p>
<img src="/2023/08/02/Pytorch-Practical-Basics-8/cudaops-struct.png" alt="cudaops-struct" style="zoom:50%;">

<p>We mainly have three kinds of file: Library interface, Core code on CPU, and Core code on GPU. Let’s explain them in detail:</p>
<ul>
<li><p>Library interface (.cpp)</p>
<ul>
<li>Contains Functions Interface for Python to call. These functions usually have Tensor input and Tensor return value.</li>
<li>Contains a standard pybind declaration, since our extension uses pybind to bind the C++ functions for Python. It indicates which functions are needed to be bound.</li>
</ul>
</li>
<li><p>Core code on CPU (.cpp)</p>
<ul>
<li>Contains core function to do the calculation. </li>
<li>Contains wrapper for the core function, serves to creating the result tensor, checking the input shape, etc.</li>
</ul>
</li>
<li><p>Core code on GPU (.cu)</p>
<ul>
<li>Contains CUDA kernel function <code>__global__</code> to do the parallel calculation. </li>
<li>Contains wrapper for the core function, serves to creating the result tensor, checking the input shape, setting the launch configs, launching the kernel, etc.</li>
</ul>
</li>
</ul>
<p>Then, after we finishing the code, we can use Python build tools to compile the code into a static object library (.so file). Then, we can import them normally in the Python side. We can call the functions we declared in library interface by pybind11.</p>
<p>In our example <a target="_blank" rel="noopener" href="https://github.com/I-am-Future/PyTorch-Linear-Operator-CUDA">code</a>, we don’t provide code for CPU calculation. We only support GPU. So we only have two files (<code>src/linearops.cpp</code> and <code>src/addmul_kernel.cu</code>)</p>
<h1 id="Pybind-Interface"><a href="#Pybind-Interface" class="headerlink" title="Pybind Interface"></a>Pybind Interface</h1><p>This is the <code>src/linearops.cpp</code> file in our repo. </p>
<h2 id="1-Utils-function"><a href="#1-Utils-function" class="headerlink" title="1. Utils function"></a>1. Utils function</h2><p>We usually defines some utility macro functions in our code. They are in the <code>include/utils.h</code> header file. </p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// PyTorch CUDA Utils</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x <span class="string">&quot; must be a CUDA tensor&quot;</span>)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x <span class="string">&quot; must be contiguous&quot;</span>)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Kernel Config Utils</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> DIV_CEIL(a, b) (((a) + (b) - 1) / (b))</span></span><br></pre></td></tr></table></figure>

<p>The third macro will call first two macros, which are used to make sure the tenser is on the CUDA devices and is contiguous. </p>
<p>The last macro performs ceil division, which are often used in setting the CUDA kernel launch configurations. </p>
<h2 id="2-Interface-functions"><a href="#2-Interface-functions" class="headerlink" title="2. Interface functions"></a>2. Interface functions</h2><p>Benefited by pybind, we can simply define functions in C++ and use them in Python. A function looks like</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">torch::Tensor <span class="title">func</span><span class="params">(torch::Tensor a, torch::Tensor b, <span class="type">int</span> c)</span></span>&#123;</span><br><span class="line">    torch::Tensor res;</span><br><span class="line">    ......</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>is relatively same as the Python function below. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">func</span>(<span class="params">a: torch.Tensor, b: torch.Tensor, c: <span class="built_in">int</span></span>) -&gt; torch.Tensor</span><br><span class="line">	res = ... <span class="comment"># torch.Tensor</span></span><br><span class="line">    ......</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>

<p>Then, we can define our matrix multiplication interface as below. Note that we need to implement both the forward and backward functions!</p>
<ul>
<li>forward</li>
</ul>
<p>Check the input, input size, and then call the CUDA function wrapper.</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">torch::Tensor <span class="title">matmul_forward</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor &amp;A, </span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor &amp;B)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(A);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(B);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(A.<span class="built_in">size</span>(<span class="number">1</span>) == B.<span class="built_in">size</span>(<span class="number">0</span>), <span class="string">&quot;matmul_fast_forward: shape mismatch&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">matmul_cuda</span>(A, B);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>backward</li>
</ul>
<p>Also check the input, input size, and then call the CUDA function wrapper. Note that we calculate the backward of <code>A * B = C</code> for input matrix A, B in two different function. So that when someday we don’t need to calculate the gradient of A, we can just pass it. </p>
<p>The gradient function derivation is mentioned in last section <a target="_blank" rel="noopener" href="https://i-am-future.github.io/2023/08/01/Pytorch-Practical-Basics-7/#Matrix-multiplication-backward">here</a>. </p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* Backward for A gradient */</span></span><br><span class="line"><span class="function">torch::Tensor <span class="title">matmul_dA_backward</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor &amp;grad_output, </span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor &amp;A, </span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor &amp;B)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(grad_output);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(B);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// dL/dB = dL/dY * B^T</span></span><br><span class="line">    <span class="keyword">auto</span> grad_A = <span class="built_in">matmul_cuda</span>(grad_output, <span class="built_in">transpose_cuda</span>(B));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grad_A;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Backward for B gradient */</span></span><br><span class="line"><span class="function">torch::Tensor <span class="title">matmul_dB_backward</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor &amp;grad_output, </span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor &amp;A, </span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor &amp;B)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(grad_output);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(A);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// dL/dB = A^T * dL/dY</span></span><br><span class="line">    <span class="keyword">auto</span> grad_B = <span class="built_in">matmul_cuda</span>(<span class="built_in">transpose_cuda</span>(A), grad_output);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grad_B;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="3-Binding"><a href="#3-Binding" class="headerlink" title="3. Binding"></a>3. Binding</h2><p>At the last of the <code>src/linearops.cpp</code>, we use the following code to bind the functions. The first string is the function name in Python side, the second is a function pointer to the function be called, and the last is the docstring for that function in Python side. </p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">PYBIND11_MODULE</span>(TORCH_EXTENSION_NAME, m) &#123;</span><br><span class="line">    ......</span><br><span class="line">    m.<span class="built_in">def</span>(<span class="string">&quot;matmul_forward&quot;</span>, &amp;matmul_forward, <span class="string">&quot;Matmul forward&quot;</span>);</span><br><span class="line">    m.<span class="built_in">def</span>(<span class="string">&quot;matmul_dA_backward&quot;</span>, &amp;matmul_dA_backward, <span class="string">&quot;Matmul dA backward&quot;</span>);</span><br><span class="line">    m.<span class="built_in">def</span>(<span class="string">&quot;matmul_dB_backward&quot;</span>, &amp;matmul_dB_backward, <span class="string">&quot;Matmul dB backward&quot;</span>);</span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h1 id="CUDA-wrapper"><a href="#CUDA-wrapper" class="headerlink" title="CUDA wrapper"></a>CUDA wrapper</h1><p>This is the <code>src/addmul_kernel.cu</code> file in our repo. </p>
<p>The wrapper for matrix multiplication looks like below: </p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">torch::Tensor <span class="title">matmul_cuda</span><span class="params">(torch::Tensor A, torch::Tensor B)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 1. Get metadata</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> m = A.<span class="built_in">size</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> n = A.<span class="built_in">size</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> p = B.<span class="built_in">size</span>(<span class="number">1</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 2. Create output tensor</span></span><br><span class="line">    <span class="keyword">auto</span> result = torch::<span class="built_in">empty</span>(&#123;m, p&#125;, A.<span class="built_in">options</span>());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. Set launch configuration</span></span><br><span class="line">    <span class="type">const</span> dim3 blockSize = <span class="built_in">dim3</span>(BLOCK_SIZE, BLOCK_SIZE);</span><br><span class="line">    <span class="type">const</span> dim3 gridSize = <span class="built_in">dim3</span>(<span class="built_in">DIV_CEIL</span>(m, BLOCK_SIZE), <span class="built_in">DIV_CEIL</span>(p, BLOCK_SIZE));</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// 4. Call the cuda kernel launcher</span></span><br><span class="line">    <span class="built_in">AT_DISPATCH_FLOATING_TYPES</span>(A.<span class="built_in">type</span>(), <span class="string">&quot;matmul_cuda&quot;</span>, </span><br><span class="line">    ([&amp;] &#123;</span><br><span class="line">        matmul_fw_kernel&lt;<span class="type">scalar_t</span>&gt;&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(</span><br><span class="line">            A.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">            B.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">            result.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">            m, p</span><br><span class="line">        );</span><br><span class="line">    &#125;));</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 5. Return the value</span></span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>And here, we’ll talk in details: </p>
<h2 id="1-Get-metadata"><a href="#1-Get-metadata" class="headerlink" title="1. Get metadata"></a>1. Get metadata</h2><p>Just as the tensor in PyTorch, we can use <code>Tensor.size(0)</code> to axis the shape size of dimension 0.</p>
<p>Note that we have checked the dimension match at the interface side, we don’t need to check it here.</p>
<h2 id="2-Create-output-tensor"><a href="#2-Create-output-tensor" class="headerlink" title="2. Create output tensor"></a>2. Create output tensor</h2><p>We can do operation in-place or create a new tensor for output. Use the following code to create a tensor shape <code>m x p</code>, with same dtype &#x2F; device as <code>A</code>. </p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> result = torch::<span class="built_in">empty</span>(&#123;m, p&#125;, A.<span class="built_in">options</span>());</span><br></pre></td></tr></table></figure>

<p>In other situations, when we want special dtype &#x2F; device, we can follow the declaration as below:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch::<span class="built_in">empty</span>(&#123;m, p&#125;, torch::<span class="built_in">dtype</span>(torch::kInt32).<span class="built_in">device</span>(feats.<span class="built_in">device</span>()))</span><br></pre></td></tr></table></figure>

<p><code>torch.empty</code> only allocate the memory, but not initialize the entries to 0. Because sometimes, we’ll fill into the result tensors in the kernel functions, so it is not necessary to initialize as 0. </p>
<h2 id="3-Set-launch-configuration"><a href="#3-Set-launch-configuration" class="headerlink" title="3. Set launch configuration"></a>3. Set launch configuration</h2><p>You should know some basic CUDA knowledges before understand this part. Basically here, we are setting the launch configuration based on the input matrix size. We are using the macro functions defined before.</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">const</span> dim3 blockSize = <span class="built_in">dim3</span>(BLOCK_SIZE, BLOCK_SIZE);</span><br><span class="line"><span class="type">const</span> dim3 gridSize = <span class="built_in">dim3</span>(<span class="built_in">DIV_CEIL</span>(m, BLOCK_SIZE), <span class="built_in">DIV_CEIL</span>(p, BLOCK_SIZE));</span><br></pre></td></tr></table></figure>

<p>We set each thread block size to <code>16 x 16</code>. Then, we set the number of blocks according to the input size. </p>
<h2 id="4-Call-the-cuda-kernel-launcher"><a href="#4-Call-the-cuda-kernel-launcher" class="headerlink" title="4. Call the cuda kernel launcher"></a>4. Call the cuda kernel launcher</h2><p>Unlike normal cuda programs, we use <code>ATen</code>‘s function to start the kernel. This is a standard operation, and you can copy-paste it to anywhere. </p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AT_DISPATCH_FLOATING_TYPES</span>(A.<span class="built_in">type</span>(), <span class="string">&quot;matmul_cuda&quot;</span>, </span><br><span class="line">                           ([&amp;] &#123;</span><br><span class="line">                               matmul_fw_kernel&lt;<span class="type">scalar_t</span>&gt;&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(</span><br><span class="line">                                   A.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">                                   B.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">                                   result.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">                                   m, p</span><br><span class="line">                               );</span><br><span class="line">                           &#125;));</span><br></pre></td></tr></table></figure>

<ul>
<li><p>This function is named <code>AT_DISPATCH_FLOATING_TYPES</code>, meaning the inside kernel will support floating types, i.e., <code>float (32bit)</code> and <code>double (64bit)</code>.   For <code>float16</code>, you can use <code>AT_DISPATCH_ALL_TYPES_AND_HALF</code>. For int (<code>int (32bit)</code> and <code>long long (64 bit)</code> and more, use <code>AT_DISPATCH_INTEGRAL_TYPES</code>. </p>
</li>
<li><p>The first argument <code>A.type()</code>, indicates the actual chosen type in the runtime. </p>
</li>
<li><p>The second argument <code>matmul_cuda</code> can be used for error reporting.</p>
</li>
<li><p>The last argument, which is a lambda function, is the actual function to be called. Basically in this function, we start the kernel by the following statement:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">matmul_fw_kernel&lt;<span class="type">scalar_t</span>&gt;&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(</span><br><span class="line">A.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">B.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">result.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">m, p</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<ul>
<li><code>matmul_fw_kernel</code> is the kernel function name.</li>
<li><code>&lt;scalar_t&gt;</code> is the template parameter, will be replaced to all possible types in the outside <code>AT_DISPATCH_FLOATING_TYPES</code>.</li>
<li><code>&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;</code> passed in the launch configuration</li>
<li>In the parameter list, if that is a <code>Tensor</code>, we should pass in the packed accessor, which convenient indexing operation in the kernel.<ul>
<li><code>&lt;scalar_t&gt;</code> is the template parameter.</li>
<li><code>2</code> means the <code>Tensor.ndimension=2</code>.</li>
<li><code>torch::RestrictPtrTraits</code> means the pointer (tensor memory) would not not overlap. It enables some optimization. Usually not change.</li>
<li><code>size_t</code> indicates the index type. Usually not change.</li>
</ul>
</li>
<li>if the parameter is integer <code>m, p</code>, just pass it in as normal.</li>
</ul>
</li>
</ul>
<h2 id="5-Return-the-value"><a href="#5-Return-the-value" class="headerlink" title="5. Return the value"></a>5. Return the value</h2><p>If we have more then one return value, we can set the return type to <code>std::vector&lt;torch::Tensor&gt;</code>. Then we return with <code>&#123;xxx, yyy&#125;</code>. </p>
<h1 id="CUDA-kernel"><a href="#CUDA-kernel" class="headerlink" title="CUDA kernel"></a>CUDA kernel</h1><p>This is the <code>src/addmul_kernel.cu</code> file in our repo. </p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> <span class="type">scalar_t</span>&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">matmul_fw_kernel</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::PackedTensorAccessor&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt; A,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::PackedTensorAccessor&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt; B,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::PackedTensorAccessor&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt; result,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> m, <span class="type">const</span> <span class="type">int</span> p</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> row = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> col = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (row &gt;= m || col &gt;= p) <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">scalar_t</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; A.<span class="built_in">size</span>(<span class="number">1</span>); i++) &#123;</span><br><span class="line">        sum += A[row][i] * B[i][col];</span><br><span class="line">    &#125;</span><br><span class="line">    result[row][col] = sum;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>We define it as a template function <code>template &lt;typename scalar_t&gt;</code>, so that our kernel function can support different type of input tensor. </li>
<li>Usually we’ll set the input <code>PackedTensorAccessor</code> with <code>const</code>, to avoid some unexpected modification on them. </li>
<li>The main code is just a simple CUDA matrix multiplication example. This is very common, you can search online for explanation.</li>
</ul>
<h2 id="Ending"><a href="#Ending" class="headerlink" title="Ending"></a>Ending</h2><p>That’s too much things in this section. In the next section, we’ll talk about how to write the <code>setup.py</code> to <strong>compile</strong> the code, letting it be a module for python. </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/01/Pytorch-Practical-Basics-7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Future">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Future's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/01/Pytorch-Practical-Basics-7/" class="post-title-link" itemprop="url">PyTorch Practical Hand-Written Modules Basics 7--Mathematics Derivation of Linear Layer</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-08-01 20:15:44" itemprop="dateCreated datePublished" datetime="2023-08-01T20:15:44+08:00">2023-08-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-08-05 17:53:31" itemprop="dateModified" datetime="2023-08-05T17:53:31+08:00">2023-08-05</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>In the section 6 to 9, we’ll investigate how to use <code>torch.autograd.Function</code> to implement the hand-written operators. The tentative outline is:</p>
<ul>
<li><input checked disabled type="checkbox"> In the last section (6), we talk about the basics of <code>torch.autograd.Function</code>. <strong>The operators defined by <code>torch.autograd.Function</code> can be automatically back-propagated.</strong></li>
<li><input checked disabled type="checkbox"> In this section (7), we’ll talk about mathematic derivation for the “linear layer” operator. </li>
<li><input disabled type="checkbox"> In the section 8, we talk about writing C++ CUDA extension for the “linear layer” operator. </li>
<li><input disabled type="checkbox"> In the section 9, we talk details about building the extension to a module, as well as testing. Then we’ll conclude the things we’ve done so far.</li>
</ul>
<p>The linear layer is defined by <code>Y = XW + b</code>. There is a matrix multiplication operation, and a bias addition. <strong>We’ll talk about their forward&#x2F;backward derivation separately.</strong> </p>
<p>(I feel sorry that currently there is some problem with displaying mathematics formula here. I’ll use screenshot first.)</p>
<h1 id="Matrix-multiplication-forward"><a href="#Matrix-multiplication-forward" class="headerlink" title="Matrix multiplication: forward"></a>Matrix multiplication: forward</h1><p>The matrix multiplication operation is a common operator. Each entry in the result matrix is a vector dot product of two input matrixes. The <code>(i, j)</code> entry of the result is from multiplying first matrix’s row <code>i</code> vector and the second matrix’s column <code>j</code> vector. From this property, we know that number of columns in the first matrix should equal to number of rows in the second matrix. The shape should be: <code>[m, n] x [n, r] -&gt; [m, r]</code>. For more details, see the figure illustration below. </p>
<img src="/2023/08/01/Pytorch-Practical-Basics-7/matmul-forward.png" alt="matmul-forward" style="zoom: 67%;">



<h1 id="Matrix-multiplication-backward"><a href="#Matrix-multiplication-backward" class="headerlink" title="Matrix multiplication: backward"></a>Matrix multiplication: backward</h1><p>First, we should know what’s the <strong>goal</strong> of the backward propagation. In the upstream side, we would get the gradient of the answer matrix, C. (The gradient matrix has the same size as its corresponding matrix. i.e., if C is in shape <code>[m, r]</code>, then gradient of C is shape <code>[m, r]</code> as well.) <strong>In this step, we should get the gradient of matrix A and B.</strong> <u>Gradient of matrix A and B are functions in terms of matrix A and B and gradient of C.</u> Specially, by chain rule, we can formulate it as</p>
<img src="/2023/08/01/Pytorch-Practical-Basics-7/matmul-backward-math1.png" alt="matmul-backward-math1" style="zoom:50%;">

<!-- $$
\frac{\partial L}{\partial A} = \frac{\partial L}{\partial C} \cdot \frac{\partial C}{\partial A} \\
(\frac{\partial L}{\partial C} \text{ is the gradient of C, and we need to figure out } \frac{\partial C}{\partial A}.)
$$-->

<p>To figure out the gradient of A, we should first investigate how an entry <code>A[i, j]</code> contribute to the entries in the result matrix C. See the figure below:</p>
<img src="/2023/08/01/Pytorch-Practical-Basics-7/matmul-backward.png" alt="matmul-backward" style="zoom:67%;">

<p>As shown above, entry <code>A[i, j]</code> multiplies with entries in row <code>j</code> of matrix B, contributing to the entries in row <code>i</code> of matrix C. We can write the gradient down in mathematics formula below:</p>
<img src="/2023/08/01/Pytorch-Practical-Basics-7/matmul-backward-math2.png" alt="matmul-backward-math2" style="zoom:50%;">

<!--$$
\begin{align}
\frac{\partial L}{\partial A_{i, j}} &= \sum_{k=1}^r \frac{\partial L}{\partial C_{i, k}} \cdot \frac{\partial C_{i, k}}{\partial A_{i, j}} \\
&= \sum_{k=1}^r \frac{\partial L}{\partial C_{i, k}} \cdot B_{j, k}\\
&= \sum_{k=1}^r  \frac{\partial L}{\partial C}_{i, k}  \cdot  B^T_{k,j} \\
& = (\frac{\partial L}{\partial C})_{\text{row i}}  \cdot  (B^T)_{\text{col j}}\\
\end{align}
$$-->

<p>The result above is the gradient for one entry <code>A[i, j]</code>, and it’s a vector dot product between a matrix’s row <code>i</code> and another matrix’s column <code>j</code>. Observing this formula, we can naturally extend it to the gradient of the whole matrix A, and that will be a matrix product. </p>
<img src="/2023/08/01/Pytorch-Practical-Basics-7/matmul-backward-math3.png" alt="matmul-backward-math3" style="zoom:50%;">

<!--$$
\frac{\partial L}{\partial A} = \frac{\partial L}{\partial C} \cdot B^T \\
\text{At last, by the same principle, we know } \frac{\partial L}{\partial B} = A^T \cdot \frac{\partial L}{\partial C}.
$$-->

<p>Recall “Gradient of matrix A and B are functions in terms of matrix A and B and gradient of C” we said before. Our derivation indeed show that, uh?</p>
<h1 id="Add-bias-forward"><a href="#Add-bias-forward" class="headerlink" title="Add bias: forward"></a>Add bias: forward</h1><p>First, we should note that when doing the addition, we’re actually adding the <code>XW</code> matrix (shape <code>[n, r]</code>) with the bias vector (shape <code>[r]</code>). Indeed we have a <strong>broadcasting</strong> here. <strong>We add bias to each row of the <code>XW</code> matrix.</strong></p>
<img src="/2023/08/01/Pytorch-Practical-Basics-7/addbias-forward.png" alt="addbias-forward.drawio" style="zoom:67%;">

<h1 id="Add-bias-backward"><a href="#Add-bias-backward" class="headerlink" title="Add bias: backward"></a>Add bias: backward</h1><p>With the similar principle, we can get the gradient for the bias as well. </p>
<img src="/2023/08/01/Pytorch-Practical-Basics-7/addbias-backward.jpg" alt="addbias-backward" style="zoom:67%;">

<p>For each entry in vector <code>b</code>, the gradient is:</p>
<img src="/2023/08/01/Pytorch-Practical-Basics-7/addbias-backward-math1.png" alt="addbias-backward-math1" style="zoom:50%;">

<!--$$
\begin{align}
\frac{\partial L}{\partial b_{i}} &= \sum_{k=1}^m \frac{\partial L}{\partial C_{k, i}} \cdot \frac{\partial C_{k, i}}{\partial b_i} \\
&= \sum_{k=1}^m \frac{\partial L}{\partial C_{k, i}} \cdot 1 \\
&= \sum_{k=1}^m \frac{\partial L}{\partial C_{k, i}} \\
\end{align}
$$-->
<p>That is, the gradient of entry <code>b_i</code> is the summation of the i-th column. In total, the gradient will be the summation along each column (i.e., axis&#x3D;0). In programming, we write:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grad_b = torch.<span class="built_in">sum</span>(grad_C, axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>



<h1 id="PyTorch-Verification"><a href="#PyTorch-Verification" class="headerlink" title="PyTorch Verification"></a>PyTorch Verification</h1><p>Finally, we can write a PyTorch program to verify if our derivation is correct: we will compare our calculated gradients with the gradients calculated by the PyTorch. If they are same, our derivation would be correct. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">A = torch.randn(<span class="number">10</span>, <span class="number">20</span>).requires_grad_()</span><br><span class="line">B = torch.randn(<span class="number">20</span>, <span class="number">30</span>).requires_grad_()</span><br><span class="line"></span><br><span class="line">res = torch.mm(A, B)</span><br><span class="line">res.retain_grad()</span><br><span class="line">res.<span class="built_in">sum</span>().backward()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.allclose(A.grad, torch.mm(res.grad, B.t()))) <span class="comment"># grad_A = grad_res * B^T</span></span><br><span class="line"><span class="built_in">print</span>(torch.allclose(B.grad, torch.mm(A.t(), res.grad))) <span class="comment"># grad_B = A^T * grad_res</span></span><br></pre></td></tr></table></figure>

<p>Finally, the output is:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>

<p>Which means that our derivation is correct. </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/30/Pytorch-Practical-Basics-6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Future">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Future's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/30/Pytorch-Practical-Basics-6/" class="post-title-link" itemprop="url">PyTorch Practical Hand-Written Modules Basics 6--torch.autograd.Function</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-07-30 20:31:20" itemprop="dateCreated datePublished" datetime="2023-07-30T20:31:20+08:00">2023-07-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-08-01 20:19:16" itemprop="dateModified" datetime="2023-08-01T20:19:16+08:00">2023-08-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>In this section (and also three sections in the future), we investigate how to use <code>torch.autograd.Function</code> to implement the hand-written operators. The tentative outline is:</p>
<ul>
<li><input checked disabled type="checkbox"> This section (6), we talk about the basics of <code>torch.autograd.Function</code>.</li>
<li><input disabled type="checkbox"> In the next section (7), we’ll talk about mathematic derivation for the “linear layer” operator. </li>
<li><input disabled type="checkbox"> In the section 8, we talk about writing C++ CUDA extension for the “linear layer” operator. </li>
<li><input disabled type="checkbox"> In the section 9, we talk details about building the extension to a module, as well as testing. Then we’ll conclude the things we’ve done so far.</li>
</ul>
<h1 id="Backgrounds"><a href="#Backgrounds" class="headerlink" title="Backgrounds"></a>Backgrounds</h1><p>This article mainly takes reference of the <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd">Official tutorial</a> and summarizes, explains the important points. </p>
<p>By defining an operator with <code>torch.autograd.Function</code> and implement its forward &#x2F; backward function, we can use this operator with other PyTorch built-in operators together. <strong>The operators defined by <code>torch.autograd.Function</code> can be automatically back-propagated.</strong></p>
<p>As mentioned in the tutorial, we should use the <code>torch.autograd.Function</code> in the following scenes:</p>
<ul>
<li>The computation is from other libraries, so they don’t support differential natively. We should explicitly define its backward functions. </li>
<li>The PyTorch’s implementation of an operator cannot take benefits from the parallelization. We utilize the PyTorch C++&#x2F;CUDA extension for the better performance.</li>
</ul>
<h1 id="Basic-Structure"><a href="#Basic-Structure" class="headerlink" title="Basic Structure"></a>Basic Structure</h1><p>The following is the basic structure of the Function:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Function</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearFunction</span>(<span class="title class_ inherited__">Function</span>):</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, input0, input1, ... , inputN</span>):</span><br><span class="line">        <span class="comment"># Save the input for the backward use. </span></span><br><span class="line">        ctx.save_for_backward(input1, input1, ... , inputN)</span><br><span class="line">        <span class="comment"># Calculate the output0, ... outputM given the inputs.</span></span><br><span class="line">        ......</span><br><span class="line">        <span class="keyword">return</span> output0, ... , outputM</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, grad_output0, ... , grad_outputM</span>):</span><br><span class="line">        <span class="comment"># Get and unpack the input for the backward use. </span></span><br><span class="line">        input0, input1, ... , inputN = ctx.saved_tensors</span><br><span class="line">        </span><br><span class="line">        grad_input0 = grad_input1 = grad_inputN = <span class="literal">None</span></span><br><span class="line">        <span class="comment"># These needs_input_grad records whether each input need to calculate the gradient. This can improve the efficiency.</span></span><br><span class="line">        <span class="keyword">if</span> ctx.needs_input_grad[<span class="number">0</span>]:</span><br><span class="line">            grad_input0 = ...  <span class="comment"># backward calculation</span></span><br><span class="line">        <span class="keyword">if</span> ctx.needs_input_grad[<span class="number">1</span>]:</span><br><span class="line">            grad_input1 = ...  <span class="comment"># backward calculation</span></span><br><span class="line">        ......</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> grad_input0, grad_input1, grad_inputN</span><br></pre></td></tr></table></figure>

<ol>
<li><p>The <code>forward</code> and <code>backward</code> functions are <code>staticmethod</code>. The forward function is <code>o0, ..., oM = forward(i0, ..., iN)</code>, calculate the output0 ~ outputM by the input0 ~ inputN. Then the backward function is <code>g_i0, ..., g_iN = backward(g_o0, ..., g_M)</code>, calculate the gradient of input0 ~ gradient of inputM by the gradient of output0 ~ outputN.</p>
</li>
<li><p>Since forward and backward are merely functions. We need store the input tensors to the <code>ctx</code> in the forward pass, so that we can get them in the backward functions. See <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/notes/extending.html#combining-forward-context">here</a> to use the alternative way to define <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function"><code>Function</code></a>.</p>
</li>
<li><p><code>ctx.needs_input_grad</code> is a tuple of Booleans. It records whether one input needs to calculate the gradient or not. Therefore, we can save computation resources if one tensor doesn’t need gradients. In that case, the return value of backward function for that tensor is <code>None</code>.</p>
</li>
</ol>
<h1 id="Use-it"><a href="#Use-it" class="headerlink" title="Use it"></a>Use it</h1><h2 id="Pure-functions"><a href="#Pure-functions" class="headerlink" title="Pure functions"></a>Pure functions</h2><p>After defining the class, we can use the <code>.apply</code> method to use it. Simply</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Option 1: alias</span></span><br><span class="line">linear = LinearFunction.apply</span><br></pre></td></tr></table></figure>
<p>or, </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Option 2: wrap in a function, to support default args and keyword args.</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linear</span>(<span class="params"><span class="built_in">input</span>, weight, bias=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">return</span> LinearFunction.apply(<span class="built_in">input</span>, weight, bias)</span><br></pre></td></tr></table></figure>

<p>Then call as</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output = linear(<span class="built_in">input</span>, weight, bias) <span class="comment"># input, weight, bias are all tensors!</span></span><br></pre></td></tr></table></figure>

<h2 id="nn-Module"><a href="#nn-Module" class="headerlink" title="nn.Module"></a>nn.Module</h2><p>In most cases, the weight and bias are parameters that are trainable during the process. We can further wrap this linear function to a Linear module: </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Linear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_features, output_features, bias=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.input_features = input_features</span><br><span class="line">        self.output_features = output_features</span><br><span class="line"></span><br><span class="line">        <span class="comment"># nn.Parameters require gradients by default.</span></span><br><span class="line">        self.weight = nn.Parameter(torch.empty(output_features, input_features))</span><br><span class="line">        <span class="keyword">if</span> bias:</span><br><span class="line">            self.bias = nn.Parameter(torch.empty(output_features))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># You should always register all possible parameters, but the</span></span><br><span class="line">            <span class="comment"># optional ones can be None if you want.</span></span><br><span class="line">            self.register_parameter(<span class="string">&#x27;bias&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Not a very smart way to initialize weights</span></span><br><span class="line">        nn.init.uniform_(self.weight, -<span class="number">0.1</span>, <span class="number">0.1</span>)</span><br><span class="line">        <span class="keyword">if</span> self.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            nn.init.uniform_(self.bias, -<span class="number">0.1</span>, <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="comment"># See the autograd section for explanation of what happens here.</span></span><br><span class="line">        <span class="keyword">return</span> LinearFunction.apply(<span class="built_in">input</span>, self.weight, self.bias)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">extra_repr</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># (Optional)Set the extra information about this module. You can test</span></span><br><span class="line">        <span class="comment"># it by printing an object of this class.</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;input_features=&#123;&#125;, output_features=&#123;&#125;, bias=&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">            self.input_features, self.output_features, self.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">        )</span><br></pre></td></tr></table></figure>

<p>As mentioned in section 3, 4 of this series, the weight and bias should be <code>nn.Parameter</code> so that they can be recognized correctly. Then we initialize the weights with random variables. </p>
<p>In the <code>forward</code> functions, we use the defined <code>LinearFunction.apply</code> functions. The backward process will be automatically done just as other PyTorch modules. </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/26/An-Obscure-RuntimeError-for-CUDA-error-out-of-memory/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Future">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Future's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/26/An-Obscure-RuntimeError-for-CUDA-error-out-of-memory/" class="post-title-link" itemprop="url">An Obscure RuntimeError for CUDA error: out of memory</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-07-26 21:02:18" itemprop="dateCreated datePublished" datetime="2023-07-26T21:02:18+08:00">2023-07-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-07-28 13:12:30" itemprop="dateModified" datetime="2023-07-28T13:12:30+08:00">2023-07-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Problem-Solving/" itemprop="url" rel="index"><span itemprop="name">Problem Solving</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h2><p>Today when I was running PyTorch scripts, I met a strange problem: </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.rand(<span class="number">2</span>, <span class="number">2</span>).to(<span class="string">&#x27;cuda:1&#x27;</span>)</span><br><span class="line">......</span><br><span class="line">torch.cuda.synchronize()</span><br></pre></td></tr></table></figure>

<p>but result in the following error:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">  File <span class="string">&quot;....../test.py&quot;</span>, line 67, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    torch.cuda.synchronize()</span><br><span class="line">  File <span class="string">&quot;....../miniconda3/envs/py39/lib/python3.9/site-packages/torch/cuda/__init__.py&quot;</span>, line 495, <span class="keyword">in</span> synchronize</span><br><span class="line">    <span class="built_in">return</span> torch._C._cuda_synchronize()</span><br><span class="line">RuntimeError: CUDA error: out of memory</span><br></pre></td></tr></table></figure>

<p>but It’s clear that GPU1 has enough memory (we only need to allocate 16 bytes!):</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">|===============================+======================+======================|</span><br><span class="line">|   0  NVIDIA GeForce ...  Off  | 00000000:1A:00.0 Off |                  N/A |</span><br><span class="line">| 75%   73C    P2   303W / 350W |  24222MiB / 24268MiB |     64%      Default |</span><br><span class="line">|                               |                      |                  N/A |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line">|   1  NVIDIA GeForce ...  Off  | 00000000:1B:00.0 Off |                  N/A |</span><br><span class="line">| 90%   80C    P2   328W / 350W |  15838MiB / 24268MiB |     92%      Default |</span><br><span class="line">|                               |                      |                  N/A |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br></pre></td></tr></table></figure>

<p>And normally, when we fail to allocate the memory for tensors, the error is:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 6.00 GiB total capacity; 4.54 GiB already allocated; 14.94 MiB free; 4.64 GiB reserved <span class="keyword">in</span> total by PyTorch)</span><br></pre></td></tr></table></figure>

<p>But our error message is much “simpler”. So what happened?</p>
<h2 id="Possible-Answer"><a href="#Possible-Answer" class="headerlink" title="Possible Answer"></a>Possible Answer</h2><p>This confused me for some time. According to this <a target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/pytorch-cuda-synchronize-out-of-memory/9502">website</a>:</p>
<blockquote>
<p>When you initially do a CUDA call, it’ll create a cuda context and a THC context on the primary GPU (GPU0), and for that i think it needs 200 MB or so. That’s right at the edge of how much memory you have left. </p>
</blockquote>
<p>Surprisingly, in my case, GPU0 has occupied <code>24222MiB / 24268MiB</code> memory. So there is no more memory for the context. In addition, this makes sense that out error message is <code>RuntimeError: CUDA error: out of memory</code>, not the message that tensallocation failed. </p>
<h2 id="Possible-Solution"><a href="#Possible-Solution" class="headerlink" title="Possible Solution"></a>Possible Solution</h2><p>Set the <code>CUDA_VISIBLE_DEVICES</code> environment variable. We need to change primary GPU (GPU0) to other one.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do this before `import torch`</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;CUDA_VISIBLE_DEVICES&#x27;</span>] = <span class="string">&#x27;1&#x27;</span> <span class="comment"># set to what you like, e.g., &#x27;1,2,3,4,5,6,7&#x27;</span></span><br></pre></td></tr></table></figure>

<p>And then, our program is ready to go. </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/09/Pytorch-Practical-Basics-5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Future">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Future's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/09/Pytorch-Practical-Basics-5/" class="post-title-link" itemprop="url">PyTorch Practical Hand-Written Modules Basics 5--Implement a ResNet</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-07-09 09:47:01" itemprop="dateCreated datePublished" datetime="2023-07-09T09:47:01+08:00">2023-07-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-07-30 23:59:13" itemprop="dateModified" datetime="2023-07-30T23:59:13+08:00">2023-07-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>In this section, we’ll utilize knowledge we learnt from the last section (<a href>see here</a>), to implement a ResNet Network (<a href>paper</a>). </p>
<p>Note that we follow the original paper’s work. Our implementation is a simper version of the official <code>torchvision</code> implementation. (That is, we only implement the key structure, and the random weight init. We don’t consider dilation or other things). </p>
<h2 id="Preliminaries-Calculate-the-feature-map-size"><a href="#Preliminaries-Calculate-the-feature-map-size" class="headerlink" title="Preliminaries: Calculate the feature map size"></a>Preliminaries: Calculate the feature map size</h2><ul>
<li>Basic formula</li>
</ul>
<p>Given a convolution kernel with size <em>K</em>, and the padding <em>P</em>, the stride <em>S</em>, feature map size <em>I</em>, we can calculate the output size as <em>O</em> &#x3D; ( <em>I</em> - <em>K</em> + 2<em>P</em> ) &#x2F; <em>S</em> + 1. </p>
<ul>
<li>Corollary</li>
</ul>
<p>Based on the formula above, we know that when <em>S</em>&#x3D;1: </p>
<ol>
<li><em>K</em>&#x3D;3, <em>P</em>&#x3D;1 makes the input size and output size same.</li>
<li><em>K</em>&#x3D;1, <em>P</em>&#x3D;0 makes the input size and output size same.</li>
</ol>
<h2 id="Overall-Structure"><a href="#Overall-Structure" class="headerlink" title="Overall Structure"></a>Overall Structure</h2><p>The Table 1 in the original paper illustrates the overall structure of the ResNet:</p>
<img src="/2023/07/09/Pytorch-Practical-Basics-5/resnet_table1.png" alt="resnet_table1" style="zoom: 67%;">

<p>We know that from <code>conv2</code>, each layer consists of many blocks. And the blocks in <code>18, 34 layers</code> is different from blocks in <code>50, 101, 152 layers</code>. </p>
<p>We have several deductions: </p>
<ol>
<li>When the feature map enters the next layer, the first block need to do a down sampling operation. This is done by setting the one of the convolution kernel’s <code>stride=2</code>. </li>
<li>At other convolution kernels, the feature map’s size is same. So the convolution settings is same as the one referred in Preliminaries.</li>
</ol>
<h2 id="Basic-Block-Implementation"><a href="#Basic-Block-Implementation" class="headerlink" title="Basic Block Implementation"></a>Basic Block Implementation</h2><p>The basic block’s structure looks like this: </p>
<img src="/2023/07/09/Pytorch-Practical-Basics-5/basic.png" alt="basic">

<p>Please see the code below. Here, apart from <code>channels</code> defining the channels in the block, we have three additional parameters, <code>in_channels</code>, <code>stride</code>, and <code>downsample</code> to make this block versatile in the FIRST block in each layer. </p>
<p>According to the ResNet structure, for example, the first block in layer3 has the input <code>64*56*56</code>. The first block in layer3 has two tasks: </p>
<ol>
<li>Make the feature map size to <code>28*28</code>. Thus we need to set its stride to <code>2</code>. </li>
<li>Make the number of channels from <code>64</code> to <code>128</code>. Thus the <code>in_channel</code> should be <code>64</code>. </li>
<li>In addition, since the input is <code>64*56*56</code>, while the output is <code>128*28*28</code>, we need a down sample convolution to match the shortcut input to the output size.</li>
</ol>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualBasicBlock</span>(nn.Module):</span><br><span class="line">    expansion: <span class="built_in">int</span> = <span class="number">1</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels: <span class="built_in">int</span>, channels: <span class="built_in">int</span>, stride: <span class="built_in">int</span> = <span class="number">1</span>, downsample: nn.Module = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels, channels, <span class="number">3</span>, stride, <span class="number">1</span>)</span><br><span class="line">        self.batchnorm1 = nn.BatchNorm2d(channels)</span><br><span class="line">        self.relu1 = nn.ReLU()</span><br><span class="line">        self.conv2 = nn.Conv2d(channels, channels, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.batchnorm2 = nn.BatchNorm2d(channels)</span><br><span class="line">        self.relu2 = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        residual = x</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.batchnorm1(x)</span><br><span class="line">        x = self.relu1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.batchnorm2(x)</span><br><span class="line">        <span class="keyword">if</span> self.downsample:</span><br><span class="line">            residual = self.downsample(residual)</span><br><span class="line">        x += residual</span><br><span class="line">        x = self.relu2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>



<h2 id="Bottleneck-Block-Implementation"><a href="#Bottleneck-Block-Implementation" class="headerlink" title="Bottleneck Block Implementation"></a>Bottleneck Block Implementation</h2><p>The bottleneck block’s structure looks like this: </p>
<img src="/2023/07/09/Pytorch-Practical-Basics-5/bottleneck.png" alt="bottleneck">

<p>To reduce the computation cost, the Bottleneck block use <code>1x1</code> kernel to map the high number of channels (e.g., 256) to a low one (e.g., 64), and do the <code>3x3</code> convolution. Then, it maps the <code>64</code> channels to <code>256</code> again. </p>
<p>Please see the code below. Same as the basic block, We have three additional parameters, <code>in_channels</code>, <code>stride</code>, and <code>downsample</code> to make this block versatile in the FIRST block in each layer. The reasons are same as above. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualBottleNeck</span>(nn.Module):</span><br><span class="line">    expansion: <span class="built_in">int</span> = <span class="number">4</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels: <span class="built_in">int</span>, channels: <span class="built_in">int</span>, stride: <span class="built_in">int</span> = <span class="number">1</span>, downsample: nn.Module = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels, channels, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.batchnorm1 = nn.BatchNorm2d(channels)</span><br><span class="line">        self.relu1 = nn.ReLU()</span><br><span class="line">        self.conv2 = nn.Conv2d(channels, channels, <span class="number">3</span>, stride, <span class="number">1</span>)</span><br><span class="line">        self.batchnorm2 = nn.BatchNorm2d(channels)</span><br><span class="line">        self.relu2 = nn.ReLU()</span><br><span class="line">        self.conv3 = nn.Conv2d(channels, channels*<span class="number">4</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.batchnorm3 = nn.BatchNorm2d(channels*<span class="number">4</span>)</span><br><span class="line">        self.relu3 = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        residual = x</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.batchnorm1(x)</span><br><span class="line">        x = self.relu1(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.batchnorm2(x)</span><br><span class="line">        x = self.relu2(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        x = self.batchnorm3(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.downsample:</span><br><span class="line">            residual = self.downsample(residual)</span><br><span class="line"></span><br><span class="line">        x += residual</span><br><span class="line">        x = self.relu3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>



<h2 id="ResNet-Base-Implementation"><a href="#ResNet-Base-Implementation" class="headerlink" title="ResNet Base Implementation"></a>ResNet Base Implementation</h2><p>Then we can put thing together to form the ResNet model! The whole structure is straight-forward. We define the submodules one by one, and implement the <code>forward()</code> function. </p>
<p>There is only two tricky point: </p>
<ol>
<li>To support the <code>ResNetBase</code> for two different base blocks, the base block can be passed to this initializer. Since two base blocks have slightly differences in setting the channels, <code>ResidualBasicBlock</code> and <code>ResidualBottleNeck</code> have an attribute called <code>expansion</code>, which convenient the procedure in setting the correct number of channels and outputs. </li>
<li>See the <code>_make_layer</code> function below. It need to determine whether we need to do the down sample. And the condition and explanation is described below.</li>
</ol>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResNetBase</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, block, layer_blocks: <span class="built_in">list</span>, input_channels=<span class="number">3</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.block = block</span><br><span class="line">        <span class="comment"># conv1: 7x7</span></span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(input_channels, <span class="number">64</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>), </span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># max pool</span></span><br><span class="line">        self.maxpool = nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># conv2 ~ conv5_x</span></span><br><span class="line">        self.in_channels = <span class="number">64</span></span><br><span class="line">        self.conv2 = self._make_layer(<span class="number">64</span>, layer_blocks[<span class="number">0</span>])</span><br><span class="line">        self.conv3 = self._make_layer(<span class="number">128</span>, layer_blocks[<span class="number">1</span>], <span class="number">2</span>)</span><br><span class="line">        self.conv4 = self._make_layer(<span class="number">256</span>, layer_blocks[<span class="number">2</span>], <span class="number">2</span>)</span><br><span class="line">        self.conv5 = self._make_layer(<span class="number">512</span>, layer_blocks[<span class="number">3</span>], <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.downsample = nn.AvgPool2d(<span class="number">7</span>)</span><br><span class="line">        output_numel = <span class="number">512</span> * self.block.expansion</span><br><span class="line">        self.fc = nn.Linear(output_numel, <span class="number">1000</span>)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># init the weights</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(m.weight, mode=<span class="string">&quot;fan_out&quot;</span>, nonlinearity=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">                nn.init.normal_(m.weight, <span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">                nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, (nn.BatchNorm2d, nn.GroupNorm)):</span><br><span class="line">                nn.init.constant_(m.weight, <span class="number">1</span>)</span><br><span class="line">                nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_make_layer</span>(<span class="params">self, channel, replicates, stride=<span class="number">1</span></span>):</span><br><span class="line">        modules = []</span><br><span class="line"></span><br><span class="line">        downsample = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> self.in_channels != channel*self.block.expansion:</span><br><span class="line">            <span class="comment"># Use downsample to match the dimension in two cases: </span></span><br><span class="line">            <span class="comment"># 1. stride != 1, meaning we should downsample H, W in this layer. </span></span><br><span class="line">            <span class="comment">#   Then we need to match the residual&#x27;s H, W and the output&#x27;s H, W of this layer.</span></span><br><span class="line">            <span class="comment"># 2. self.in_channels != channel*block.expansion, meaning we should increase C in this layer.</span></span><br><span class="line">            <span class="comment">#   Then we need to match the residual&#x27;s C and the output&#x27;s C of this layer.</span></span><br><span class="line"></span><br><span class="line">            downsample = nn.Sequential(</span><br><span class="line">                nn.Conv2d(self.in_channels, channel*self.block.expansion, <span class="number">1</span>, stride),</span><br><span class="line">                nn.BatchNorm2d(channel*self.block.expansion)</span><br><span class="line">            )</span><br><span class="line">        </span><br><span class="line">        modules.append(self.block(self.in_channels, channel, stride, downsample))</span><br><span class="line"></span><br><span class="line">        self.in_channels = channel * self.block.expansion</span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, replicates):</span><br><span class="line">            modules.append(self.block(self.in_channels, channel))</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*modules)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x: shape Bx3x224x224</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.maxpool(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        x = self.conv4(x)</span><br><span class="line">        x = self.conv5(x)</span><br><span class="line"></span><br><span class="line">        x = self.downsample(x)</span><br><span class="line">        x = torch.flatten(x, start_dim=<span class="number">1</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>



<h2 id="Encapsulate-the-Constructors"><a href="#Encapsulate-the-Constructors" class="headerlink" title="Encapsulate the Constructors"></a>Encapsulate the Constructors</h2><p>Finally, we can encapsulate the constructors by functions:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">my_resnet18</span>(<span class="params">in_channels=<span class="number">3</span></span>):</span><br><span class="line">    <span class="keyword">return</span> ResNetBase(ResidualBasicBlock, [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>], in_channels)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">my_resnet34</span>(<span class="params">in_channels=<span class="number">3</span></span>):</span><br><span class="line">    <span class="keyword">return</span> ResNetBase(ResidualBasicBlock, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>], in_channels)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">my_resnet50</span>(<span class="params">in_channels=<span class="number">3</span></span>):</span><br><span class="line">    <span class="keyword">return</span> ResNetBase(ResidualBottleNeck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>], in_channels)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">my_resnet101</span>(<span class="params">in_channels=<span class="number">3</span></span>):</span><br><span class="line">    <span class="keyword">return</span> ResNetBase(ResidualBottleNeck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">3</span>], in_channels)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">my_resnet152</span>(<span class="params">in_channels=<span class="number">3</span></span>):</span><br><span class="line">    <span class="keyword">return</span> ResNetBase(ResidualBottleNeck, [<span class="number">3</span>, <span class="number">8</span>, <span class="number">36</span>, <span class="number">3</span>], in_channels)</span><br></pre></td></tr></table></figure>

<p>Then, we can use it as normal models:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">img = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line">model_my = my_resnet50()</span><br><span class="line">res_my = model_my(img)</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/18/Pytorch-Practical-Basics-4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Future">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Future's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/18/Pytorch-Practical-Basics-4/" class="post-title-link" itemprop="url">PyTorch Practical Hand-Written Modules Basics 4--Hand-written modules basics</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-06-18 10:01:32" itemprop="dateCreated datePublished" datetime="2023-06-18T10:01:32+08:00">2023-06-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-07-09 09:49:04" itemprop="dateModified" datetime="2023-07-09T09:49:04+08:00">2023-07-09</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>After three articles talking about tensors, in this article, we will talk about something to the PyTorch Hand Written Modules Basics. You can see the outline on the left sidebar.</p>
<h2 id="Basic-structure"><a href="#Basic-structure" class="headerlink" title="Basic structure"></a>Basic structure</h2><p>The model must inherit the <code>nn.Module</code> class. Basically, according to the <a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/nn_tutorial.html#closing-thoughts">official tutorial</a>, <code>nn.Module</code> “creates a callable which behaves like a function, but can also contain state(such as neural net layer weights).”</p>
<p>The following is an example from the <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">docs</a>:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">20</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        <span class="keyword">return</span> F.relu(self.conv2(x))</span><br></pre></td></tr></table></figure>

<p><strong>Some details</strong></p>
<ul>
<li>First, our model has Name <code>Model</code>, and inherits the <code>nn.Module</code> class.</li>
<li><code>super().__init__()</code> must be called at the first line of the <code>__init__</code> function.</li>
<li>The <code>Model</code> contains two submodules as attributes, <code>conv1</code> and <code>conv2</code>. They’re <code>nn.Conv2d</code> (The PyTorch implementation for 2-D convolution)</li>
<li>The <code>forward()</code> function do the forward-propagation of the model. It receives a tensor <code>x</code> and do two convolution-with-relu operation. And then return the result. </li>
<li>As for the backward-propagation, that step is calculated automatically by the powerful PyTorch’s auto-gradient technique. You don’t need to care about that.</li>
</ul>
<h2 id="load-store-the-model-state-dict"><a href="#load-store-the-model-state-dict" class="headerlink" title="load &#x2F; store the model.state_dict()"></a>load &#x2F; store the model.state_dict()</h2><p>Only model’s attributes that are subclass of <code>nn.Module</code> can be regarded as a valid registered parameters. These parameters are in the <code>model.state_dict()</code>, and can be load and store from&#x2F;to the disk.</p>
<ul>
<li><code>model.state_dict()</code>:</li>
</ul>
<p>The <code>state_dict()</code> is an <code>OrderedDict</code>. It contains the key value pair like “Parameter Name: Tensor”</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model.state_dict()</span><br><span class="line"></span><br><span class="line">model.state_dict().keys()</span><br><span class="line"><span class="comment"># OUTPUT:</span></span><br><span class="line"><span class="comment"># odict_keys([&#x27;conv1.weight&#x27;, &#x27;conv1.bias&#x27;, &#x27;conv2.weight&#x27;, &#x27;conv2.bias&#x27;])</span></span><br><span class="line"></span><br><span class="line">model.state_dict().values()</span><br><span class="line"><span class="comment"># OUTPUT:</span></span><br><span class="line"><span class="comment"># odict_values([tensor([[[[ 1.0481e-01, -2.3481e-02,  9.1083e-02,  1.9955e-01,  1.0437e-01], ... ...</span></span><br></pre></td></tr></table></figure>

<ul>
<li>Use the following code to <strong>store</strong> the parameters of the model <code>Model</code> above to the disk:</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(), <span class="string">&#x27;model.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>Use the following code to <strong>load</strong> the parameters from the disk:</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;model.pth&#x27;</span>))</span><br></pre></td></tr></table></figure>



<h2 id="Common-Submodules"><a href="#Common-Submodules" class="headerlink" title="Common Submodules"></a>Common Submodules</h2><p>This subsection introduces some common submodules used. As mentioned above, to make them as valid registered parameters, they are subclass of <code>nn.Module</code> or are type <code>nn.Parameter</code>.</p>
<h3 id="clone-the-module"><a href="#clone-the-module" class="headerlink" title="clone the module"></a>clone the module</h3><p>The module should be copied (cloned) by the <code>copy.deepcopy</code> method. </p>
<ul>
<li>Shallow copy (wrong!)</li>
</ul>
<p>The model is only shallow copied. We can see that the two models’ <code>conv1</code> Tensor are the same one!!!</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">model = Model()</span><br><span class="line">model2 = copy.copy(model) <span class="comment"># shallow copy</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(model.conv1), <span class="built_in">id</span>(model2.conv1))</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line"><span class="number">2755774917472</span> <span class="number">2755774917472</span></span><br></pre></td></tr></table></figure>

<ul>
<li>Deep copy (right!)</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">model = Model()</span><br><span class="line">model2 = copy.deepcopy(model) <span class="comment"># deep copy</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(model.conv1), <span class="built_in">id</span>(model2.conv1))</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line"><span class="number">2755774915552</span> <span class="number">2755774916272</span></span><br></pre></td></tr></table></figure>

<ul>
<li>Example:</li>
</ul>
<p>This is the code from <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/detr/blob/main/models/transformer.py#L272">DETR</a>. This copies <code>module</code> for N times, resulting in an <code>nn.ModuleList</code>. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_get_clones</span>(<span class="params">module, N</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N)])</span><br></pre></td></tr></table></figure>



<h3 id="nn-ModuleList"><a href="#nn-ModuleList" class="headerlink" title="nn.ModuleList"></a>nn.ModuleList</h3><p><code>nn.ModuleList</code> is a list, but inherited the <code>nn.Module</code>. It can be recognized by the model correctly. </p>
<ul>
<li>Wrong example: from the output, we can see the submodule is not registered correctly.</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model2</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args, **kwargs</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__(*args, **kwargs)</span><br><span class="line">        self.mlp = [nn.Linear(<span class="number">10</span>, <span class="number">10</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]</span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(Model2().state_dict().keys())</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">odict_keys([])</span><br></pre></td></tr></table></figure>

<ul>
<li>Correct example: from the output, we can see the submodule is registered correctly.</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model3</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args, **kwargs</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__(*args, **kwargs)</span><br><span class="line">        self.mlp = nn.ModuleList([nn.Linear(<span class="number">10</span>, <span class="number">10</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]) </span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(Model3().state_dict().keys())</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">odict_keys([<span class="string">&#x27;mlp.0.weight&#x27;</span>, <span class="string">&#x27;mlp.0.bias&#x27;</span>, ..., <span class="string">&#x27;mlp.9.weight&#x27;</span>, <span class="string">&#x27;mlp.9.bias&#x27;</span>])</span><br></pre></td></tr></table></figure>

<p><code>nn.ModuleDict</code> is similar to <code>nn.ModuleList</code>, but a dictionary. </p>
<h3 id="nn-Parameter"><a href="#nn-Parameter" class="headerlink" title="nn.Parameter"></a>nn.Parameter</h3><p>A plain tensor attributes can not be registered to the model. We need to wrap it with <code>nn.Parameter</code>, to make the model save the tensor’s state correctly. </p>
<p>The following is modified from the <a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/nn_tutorial.html#refactor-using-nn-module">official tutorial</a>. In this example, <code>self.weights</code> is merely a <code>torch.Tensor</code>, which cannot be regarded as a model’s <code>state_dict</code>. The <code>self.bias</code> would works normally, because it’s a <code>nn.Parameter</code>.  </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Mnist_Logistic</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weights = torch.randn(<span class="number">784</span>, <span class="number">10</span>) / math.sqrt(<span class="number">784</span>) <span class="comment"># WRONG</span></span><br><span class="line">        self.bias = nn.Parameter(torch.zeros(<span class="number">10</span>)) <span class="comment"># CORRECT</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, xb</span>):</span><br><span class="line">        <span class="keyword">return</span> xb @ self.weights + self.bias</span><br></pre></td></tr></table></figure>

<p>Check if submodules is correctly regiestered:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(Mnist_Logistic().state_dict().keys())</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">odict_keys([<span class="string">&#x27;bias&#x27;</span>]) <span class="comment"># only `bias` regiestered! no `weights` here</span></span><br></pre></td></tr></table></figure>



<h3 id="nn-Sequential"><a href="#nn-Sequential" class="headerlink" title="nn.Sequential"></a>nn.Sequential</h3><p>This is a sequential container. Data will flow by the submodules contained one by one. An example is shown below.  </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line">model =nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">784</span>, <span class="number">128</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">128</span>, <span class="number">64</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>



<h2 id="model-apply-weight-init"><a href="#model-apply-weight-init" class="headerlink" title="model.apply() &amp; weight init"></a>model.apply() &amp; weight init</h2><p>Applies <code>fn</code> recursively to every submodule (as returned by <code>model.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.init.html#nn-init-doc">torch.nn.init</a>).</p>
<p>A typical example can be:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">20</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        <span class="keyword">return</span> F.relu(self.conv2(x))</span><br><span class="line">    </span><br><span class="line">model = Model()   </span><br><span class="line"><span class="comment"># do init params with model.apply():</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        torch.nn.init.xavier_uniform_(m.weight)</span><br><span class="line">        m.bias.data.fill_(<span class="number">0.01</span>)</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">        torch.nn.init.xavier_uniform_(m.weight)</span><br><span class="line">        m.bias.data.fill_(<span class="number">0.01</span>)</span><br><span class="line">model.apply(init_weights)</span><br></pre></td></tr></table></figure>



<h2 id="model-eval-model-train-training"><a href="#model-eval-model-train-training" class="headerlink" title="model.eval() &#x2F; model.train() &#x2F; .training"></a>model.eval() &#x2F; model.train() &#x2F; .training</h2><p>The modules such as <code>BatchNorm</code> and <code>DropOut</code> performs differently on the training and evaluating stage. </p>
<p>We can use <code>model.train()</code> to set the model to the training stage. Use <code>model.eval()</code> to set the model to the training stage. </p>
<p>But, what if our <u>own written modules</u> need to perform differently in two stages? The answer is that, <code>nn.Module</code> has an attribute called <code>training</code>. It’s <code>True</code> when training, <code>False</code> otherwise. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># skipped in this example</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            ... <span class="comment"># write the code in training stage here</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            ... <span class="comment"># write the code in evaluating/inferencing stage here</span></span><br></pre></td></tr></table></figure>

<p>As we can see, when we called <code>model.train()</code>, actually, all submodules from <code>model</code> would set the <code>training</code> attribute to <code>True</code>, and <code>False</code> otherwise. </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/17/Pytorch-Practical-Basics-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Future">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Future's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/17/Pytorch-Practical-Basics-3/" class="post-title-link" itemprop="url">PyTorch Practical Hand-Written Modules Basics 3--Tensor-wise operations</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-06-17 18:07:05" itemprop="dateCreated datePublished" datetime="2023-06-17T18:07:05+08:00">2023-06-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-07-09 09:49:17" itemprop="dateModified" datetime="2023-07-09T09:49:17+08:00">2023-07-09</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>In this section we will talk about some PyTorch functions that operates the tensors. </p>
<h2 id="torch-Tensor-expand"><a href="#torch-Tensor-expand" class="headerlink" title="torch.Tensor.expand"></a>torch.Tensor.expand</h2><p>Signature: <code>Tensor.expand(*sizes) -&gt; Tensor</code></p>
<p>The <code>expand</code> function returns a new view of the <code>self</code> tensor, with singleton dimensions expanded to a larger size. The passing parameter indicates the destination size. (“singleton dimensions” means the dimension with shape <code>1</code>)</p>
<p><strong>Basic Usage</strong></p>
<p>Passing <code>-1</code> as the size for a dimension means not changing the size of that dimension.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]]) <span class="comment"># torch.Size([3, 1])</span></span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(x.expand(<span class="number">3</span>, <span class="number">4</span>))   <span class="comment"># torch.Size([3, 4])</span></span><br><span class="line"><span class="built_in">print</span>(x.expand(-<span class="number">1</span>, <span class="number">4</span>))  <span class="comment"># torch.Size([3, 4])</span></span><br><span class="line"><span class="built_in">print</span>(x.expand(<span class="number">3</span>, -<span class="number">1</span>))  <span class="comment"># torch.Size([3, 1])</span></span><br><span class="line"><span class="built_in">print</span>(x.expand(-<span class="number">1</span>, -<span class="number">1</span>)) <span class="comment"># torch.Size([3, 1])</span></span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">tensor([[<span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>]])</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>]])</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>]])</span><br><span class="line">tensor([[<span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>]])</span><br><span class="line">tensor([[<span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>]])</span><br></pre></td></tr></table></figure>

<p><strong>Wrong Usage</strong></p>
<p>Only the dimension with shape 1 can be expanded:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]]) <span class="comment"># torch.Size([3, 1])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.expand(<span class="number">2</span>, <span class="number">2</span>))   <span class="comment"># ERROR! can&#x27;t expand axis 0 shape from 3 (not 1)</span></span><br></pre></td></tr></table></figure>

<p><strong>Why use it?</strong></p>
<p>The return is only a view, not a new tensor. <strong>Therefore, if you only want to only read (not write) to an expanded tensor, use expand() will save much GPU memory.</strong> Note that modifying on the expanded tensor would make modification on the original as well. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]]) <span class="comment"># torch.Size([3, 1])</span></span><br><span class="line">x.expand(<span class="number">3</span>, <span class="number">4</span>)[<span class="number">0</span>, <span class="number">1</span>] = <span class="number">100</span></span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">tensor([[<span class="number">100</span>],</span><br><span class="line">        [  <span class="number">2</span>],</span><br><span class="line">        [  <span class="number">3</span>]])</span><br></pre></td></tr></table></figure>



<h2 id="torch-Tensor-repeat"><a href="#torch-Tensor-repeat" class="headerlink" title="torch.Tensor.repeat"></a>torch.Tensor.repeat</h2><p>Signature: <code>Tensor.repeat(*sizes) -&gt; Tensor)</code></p>
<p>Repeats this tensor along the specified dimensions. It is somewhat similar to <code>torch.Tensor.expand()</code>, but the passing in parameter indicates the repeat times. Also, this is a deep copy.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) <span class="comment"># torch.Size([3])</span></span><br><span class="line"><span class="built_in">print</span>(x.repeat(<span class="number">4</span>, <span class="number">2</span>)) <span class="comment"># torch.Size([4, 6])</span></span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]])</span><br></pre></td></tr></table></figure>

<p><strong>More than the given ndimension</strong></p>
<p>If the <code>size</code> has <strong>more</strong> dimension than the self tensor, like the example below, the <code>x</code> only have shape 3x1, while we have more than two input parameters, then additional dimensions will be added at the front. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]]) <span class="comment"># torch.Size([3, 1])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.repeat(<span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>).shape)    </span><br><span class="line"><span class="comment"># torch.Size([4, 6, 1])  first 1: same. last 2 dim: [3,1]*[2,1]=[6,1]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.repeat(<span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>).shape)    </span><br><span class="line"><span class="comment"># torch.Size([4, 2, 3, 1])  first 2: same. last 2 dim: [3,1]*[1,1]=[3,1]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.repeat(<span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>).shape)    </span><br><span class="line"><span class="comment"># torch.Size([1, 4, 6, 1])  first 2: same. last 2 dim: [3,1]*[2,1]=[6,1]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.repeat(<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>).shape)    </span><br><span class="line"><span class="comment"># torch.Size([1, 1, 12, 2])  first 2: same. last 2 dim: [3,1]*[4,2]=[12,2]</span></span><br></pre></td></tr></table></figure>



<h2 id="torch-Tensor-transpose"><a href="#torch-Tensor-transpose" class="headerlink" title="torch.Tensor.transpose"></a>torch.Tensor.transpose</h2><p>Signature: <code>torch.transpose(input, dim0, dim1) -&gt; Tensor</code></p>
<p>Signature: <code>torch.Tensor.transpose(dim0, dim1) -&gt; Tensor</code></p>
<p>Returns a tensor that is a transposed version of <code>input</code>. The given dimensions <code>dim0</code> and <code>dim1</code> are swapped. </p>
<p>Therefore, like the examples below, <code>x.transpose(0, 1)</code> and <code>x.transpose(1, 0)</code> are same. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x) <span class="comment"># shape: torch.Size([2, 3])</span></span><br><span class="line"><span class="built_in">print</span>(x.transpose(<span class="number">0</span>, <span class="number">1</span>)) <span class="comment"># shape: torch.Size([3, 2])</span></span><br><span class="line"><span class="built_in">print</span>(x.transpose(<span class="number">1</span>, <span class="number">0</span>)) <span class="comment"># shape: torch.Size([3, 2])</span></span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">y = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(y) <span class="comment"># shape: torch.Size([2, 3, 4])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(y.transpose(<span class="number">0</span>, <span class="number">1</span>)) <span class="comment"># shape: torch.Size([3, 2, 4])</span></span><br><span class="line"><span class="built_in">print</span>(y.transpose(<span class="number">1</span>, <span class="number">0</span>)) <span class="comment"># shape: torch.Size([3, 2, 4])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(y.transpose(<span class="number">0</span>, <span class="number">2</span>)) <span class="comment"># shape: torch.Size([4, 3, 2])</span></span><br><span class="line"><span class="built_in">print</span>(y.transpose(<span class="number">2</span>, <span class="number">0</span>)) <span class="comment"># shape: torch.Size([4, 3, 2])</span></span><br></pre></td></tr></table></figure>



<h2 id="torch-Tensor-permute"><a href="#torch-Tensor-permute" class="headerlink" title="torch.Tensor.permute"></a>torch.Tensor.permute</h2><p>Signature: <code>torch.Tensor.permute(dims) -&gt; Tensor</code></p>
<p>Signature: <code>torch.permute(input, dims) -&gt; Tensor</code></p>
<p>This function reorder the dimensions. See the example below. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>) <span class="comment"># Shape: torch.Size([2, 3, 4])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(y.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>)) <span class="comment"># Shape: torch.Size([2, 3, 4])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(y.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)) <span class="comment"># Shape: torch.Size([2, 4, 3])</span></span><br></pre></td></tr></table></figure>

<p>Let’s have a close look to the third line as an example. </p>
<ul>
<li><p>The first argument <code>0</code> means that the new tensor’s first dimension is the original dimension at <code>0</code>, so the shape is 2.</p>
</li>
<li><p>The second argument <code>2</code> means that the new tensor’s second dimension is the original dimension at <code>2</code>, so the shape is 4. </p>
</li>
<li><p>The third argument <code>1</code> means that the new tensor’s third dimension is the original dimension at <code>1</code>, so the shape is 3.</p>
</li>
</ul>
<p>Finally, the result shape is <code>torch.Size([2, 4, 3])</code>. </p>
<h2 id="torch-Tensor-view-torch-Tensor-reshape"><a href="#torch-Tensor-view-torch-Tensor-reshape" class="headerlink" title="torch.Tensor.view &#x2F; torch.Tensor.reshape"></a>torch.Tensor.view &#x2F; torch.Tensor.reshape</h2><p>Signature: <code>Tensor.view(*shape) -&gt; Tensor</code></p>
<p>Signature: <code>Tensor.reshape(*shape) -&gt; Tensor</code></p>
<p>Reshape the Tensor to <code>shape</code>. </p>
<p>The function <code>shape()</code> always return a new copy of the tensor. </p>
<p>For function <code>view()</code>, if the <code>shape</code> satisfies some conditions (see <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.Tensor.view.html">here</a>), deep copy can be avoided to save the GPU memory. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x) <span class="comment"># Shape: torch.Size([4, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.reshape(<span class="number">3</span>, <span class="number">4</span>)) <span class="comment"># Shape: torch.Size([3, 4])</span></span><br><span class="line"><span class="built_in">print</span>(x.reshape(-<span class="number">1</span>, <span class="number">4</span>)) <span class="comment"># Shape: torch.Size([3, 4])</span></span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x) <span class="comment"># Shape: torch.Size([4, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.view(<span class="number">3</span>, <span class="number">4</span>)) <span class="comment"># Shape: torch.Size([3, 4])</span></span><br><span class="line"><span class="built_in">print</span>(x.view(-<span class="number">1</span>, <span class="number">4</span>)) <span class="comment"># Shape: torch.Size([3, 4])</span></span><br></pre></td></tr></table></figure>

<h2 id="torch-cat"><a href="#torch-cat" class="headerlink" title="torch.cat"></a>torch.cat</h2><p>Signature: <code>torch.cat(tensors, dim=0, out=None) -&gt; Tensor</code></p>
<p>Concatenates the given sequence of <code>tensors</code> in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty. For how to determine the <code>dim</code>, please refer to my previous <a target="_blank" rel="noopener" href="https://i-am-future.github.io/2023/05/27/Pytorch-Practical-Basics-2/#Key-What-is-the-%E2%80%9Cdim%E2%80%9D-parameter">article</a>. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x) <span class="comment"># Shape: torch.Size([2, 3])</span></span><br><span class="line"></span><br><span class="line">y = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(y) <span class="comment"># Shape: torch.Size([2, 3])</span></span><br><span class="line"></span><br><span class="line">z = torch.cat((x, y), dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(z) <span class="comment"># Shape: torch.Size([4, 3]) [2+2, 3]</span></span><br><span class="line"></span><br><span class="line">z = torch.cat((x, y), dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(z) <span class="comment"># Shape: torch.Size([2, 6]) [2, 3+3]</span></span><br></pre></td></tr></table></figure>



<h2 id="torch-stack"><a href="#torch-stack" class="headerlink" title="torch.stack"></a>torch.stack</h2><p>Signature: <code>torch.stack(tensors, dim=0, out=None) -&gt; Tensor</code></p>
<p>Concatenates a sequence of tensors along a new dimension. See example below. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>) <span class="comment"># Shape: torch.Size([2, 3])</span></span><br><span class="line">y = torch.randn(<span class="number">2</span>, <span class="number">3</span>) <span class="comment"># Shape: torch.Size([2, 3])</span></span><br><span class="line"></span><br><span class="line">z = torch.stack((x, y), dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(z) <span class="comment"># Shape: torch.Size([*2, 2, 3]) The first 2 is the new dimension</span></span><br><span class="line"></span><br><span class="line">z = torch.stack((x, y), dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(z) <span class="comment"># Shape: torch.Size([2, *2, 3]) The second 2 is the new dimension</span></span><br><span class="line"></span><br><span class="line">z = torch.stack((x, y), dim=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(z) <span class="comment"># Shape: torch.Size([2, 3, *2]) The last 2 is the new dimension</span></span><br></pre></td></tr></table></figure>



<h2 id="torch-vstack-hstack"><a href="#torch-vstack-hstack" class="headerlink" title="torch.vstack&#x2F;hstack"></a>torch.vstack&#x2F;hstack</h2><p><code>torch.vsplit(...)</code> is spliting the tensors vertically, which is equivalent to <code>torch.split(..., dim=0)</code>. </p>
<p> <code>torch.hsplit(...)</code> is spliting the tensors horizontally, which is equivalent to <code>torch.split(..., dim=1)</code>. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>) <span class="comment"># Shape: torch.Size([2, 3])</span></span><br><span class="line">y = torch.randn(<span class="number">2</span>, <span class="number">3</span>) <span class="comment"># Shape: torch.Size([2, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> torch.vstack((x, y)).shape == torch.cat((x, y), dim=<span class="number">0</span>).shape</span><br><span class="line"><span class="keyword">assert</span> torch.hstack((x, y)).shape == torch.cat((x, y), dim=<span class="number">1</span>).shape</span><br></pre></td></tr></table></figure>



<h2 id="torch-split"><a href="#torch-split" class="headerlink" title="torch.split"></a>torch.split</h2><p>Signature: <code>torch.split(tensor, split_size_or_sections, dim=0)</code></p>
<ul>
<li>If <code>split_size_or_sections</code> is an integer, then tensor will be split into equally sized chunks (if possible, ptherwise, last would be smaller).</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">3</span>) <span class="comment"># Shape: torch.Size([4, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.split(x, <span class="number">2</span>, dim=<span class="number">0</span>)) <span class="comment"># 2-item tuple, each Shape: (2, 3)</span></span><br><span class="line"><span class="built_in">print</span>(torch.split(x, <span class="number">1</span>, dim=<span class="number">1</span>)) <span class="comment"># 3-item tuple, each Shape: (4, 1)</span></span><br></pre></td></tr></table></figure>

<ul>
<li>If <code>split_size_or_sections</code> is a list, then tensor will be split into <code>len(split_size_or_sections)</code> chunks with sizes in <code>dim</code> according to <code>split_size_or_sections</code>.</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">3</span>) <span class="comment"># Shape: torch.Size([4, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.split(x, (<span class="number">1</span>, <span class="number">3</span>), dim=<span class="number">0</span>)) </span><br><span class="line"><span class="comment"># 2-item tuple, each Shape: (1, 3) and (3, 3)</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.split(x, (<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>), dim=<span class="number">1</span>)) </span><br><span class="line"><span class="comment"># 3-item tuple, each Shape: (4, 1) and (4, 1) and (4, 1)</span></span><br></pre></td></tr></table></figure>



<h2 id="torch-vsplit-hsplit"><a href="#torch-vsplit-hsplit" class="headerlink" title="torch.vsplit&#x2F;hsplit"></a>torch.vsplit&#x2F;hsplit</h2><p>This is actually similar to <code>torch.vstack</code> and <code>torch.hstack</code>. v means vertically, along dim&#x3D;0, and h means horizontally, along dim&#x3D;1.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The followings are equivalent:</span></span><br><span class="line"><span class="comment"># pair 1</span></span><br><span class="line"><span class="built_in">print</span>(torch.vsplit(x, <span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(torch.split(x, <span class="number">1</span>, dim=<span class="number">0</span>))</span><br><span class="line"><span class="comment"># pair 2</span></span><br><span class="line"><span class="built_in">print</span>(torch.hsplit(x, <span class="number">4</span>))</span><br><span class="line"><span class="built_in">print</span>(torch.split(x, <span class="number">1</span>, dim=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>



<h2 id="torch-flatten"><a href="#torch-flatten" class="headerlink" title="torch.flatten"></a>torch.flatten</h2><p>Signature: <code>torch.flatten(input, start_dim=0, end_dim=-1) -&gt; Tensor</code></p>
<p>flatten the given dimension from <code>start_dim</code> to <code>end_dim</code>. This is especially useful when converting a 3D (image) tensor to a linear vector.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(x) <span class="comment"># Shape: torch.Size([2, 4, 4])</span></span><br><span class="line"></span><br><span class="line">flattened = torch.flatten(x, start_dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(flattened) <span class="comment"># Shape: torch.Size([2, 16])</span></span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/05/30/CMake-QRH/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Future">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Future's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/05/30/CMake-QRH/" class="post-title-link" itemprop="url">CMake Quick Reference</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-05-30 10:28:18" itemprop="dateCreated datePublished" datetime="2023-05-30T10:28:18+08:00">2023-05-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-07-24 12:00:05" itemprop="dateModified" datetime="2023-07-24T12:00:05+08:00">2023-07-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/QRH/" itemprop="url" rel="index"><span itemprop="name">QRH</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Learned and made up from <a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1fa411r7zp">video</a> and <a target="_blank" rel="noopener" href="https://github.com/parallel101/course">code</a>.</p>
<p>Prerequisite: basic knowledge in C&#x2F;C++.</p>
<h2 id="Compile-a-Basic-Program"><a href="#Compile-a-Basic-Program" class="headerlink" title="Compile a Basic Program"></a>Compile a Basic Program</h2><p><a target="_blank" rel="noopener" href="https://github.com/parallel101/course/tree/master/01/05">See code here</a></p>
<p>A project root directory must contain a file called <code>CMakeLists.txt</code>, describing the build procedure of the project. </p>
<p>A typical simple <code>CMakeLists.txt</code> contains the following (assuming we have two source files in the current directory, <code>main.cpp</code> and <code>hello.cpp</code>):</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.12</span>) <span class="comment"># describe the minimum cmake version</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">project</span>(hellocmake LANGUAGES CXX)    <span class="comment"># describe the project name, and lan</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">add_executable</span>(a.out main.cpp hello.cpp)</span><br></pre></td></tr></table></figure>

<p>The <code>add_executable</code> function’s signature is <code>add_executable(target, [src files...])</code>, meaning to use all <code>src files</code> to compile the <code>target</code>.</p>
<p>To build the program, run in the shell:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cmake -B build</span><br><span class="line">cmake --build build</span><br><span class="line"><span class="comment"># run with</span></span><br><span class="line">./build/&lt;program_name&gt;</span><br></pre></td></tr></table></figure>

<p>To clean and rebuild from scratch, just</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">rm</span> -rf build</span><br></pre></td></tr></table></figure>



<h2 id="Compile-a-Library-Link-it"><a href="#Compile-a-Library-Link-it" class="headerlink" title="Compile a Library &amp; Link it"></a>Compile a Library &amp; Link it</h2><p><a target="_blank" rel="noopener" href="https://github.com/parallel101/course/tree/master/01/06">See code here</a></p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># compile static OR dynamic library</span></span><br><span class="line"><span class="keyword">add_library</span>(hellolib STATIC hello.cpp)</span><br><span class="line"><span class="keyword">add_library</span>(hellolib SHARED hello.cpp)</span><br><span class="line"></span><br><span class="line"><span class="keyword">add_executable</span>(a.out main.cpp)</span><br><span class="line"></span><br><span class="line"><span class="keyword">target_link_libraries</span>(a.out PUBLIC hellolib)</span><br></pre></td></tr></table></figure>

<ul>
<li>The <code>add_library</code> function’s signature is <code>add_library(target, STATIC/SHARED [src files...])</code>, meaning to use all <code>src files</code> to compile the static&#x2F;dynamic <code>target</code> library.</li>
<li>Then, <code>target_link_libraries(a.out PUBLIC hellolib)</code> links the <code>hellolib</code>‘s source to the <code>a.out</code>.</li>
</ul>
<h2 id="Compile-a-subdirectory"><a href="#Compile-a-subdirectory" class="headerlink" title="Compile a subdirectory"></a>Compile a subdirectory</h2><p><a target="_blank" rel="noopener" href="https://github.com/parallel101/course/tree/master/01/10">See code here</a></p>
<p>The sub-directory could contain a set of source codes to compile a library&#x2F;executable.</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># main CMakeLists.txt</span></span><br><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.12</span>)</span><br><span class="line"><span class="keyword">project</span>(hellocmake LANGUAGES CXX)</span><br><span class="line"></span><br><span class="line"><span class="keyword">add_subdirectory</span>(hellolib)  <span class="comment"># the name of subdirectory</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">add_executable</span>(a.out main.cpp)</span><br><span class="line"><span class="keyword">target_link_libraries</span>(a.out PUBLIC hellolib)</span><br></pre></td></tr></table></figure>

<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sub-directory CMakeLists.txt</span></span><br><span class="line"><span class="keyword">add_library</span>(hellolib STATIC hello.cpp)</span><br></pre></td></tr></table></figure>



<p>If the <code>main.cpp</code> uses the headers in the subdirectory <code>hellolib</code>, then <code>main.cpp</code> should write <code>#include &quot;hellolib/hello.h&quot;</code>. To simplify the <code>#include</code> statement, we could add the following to main’s <code>CMakeLists.txt</code>:</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">add_executable</span>(a.out main.cpp)</span><br><span class="line"><span class="keyword">target_include_directories</span>(a.out PUBLIC hellolib)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>This is still some complex. If we want to build two executable, we need write the following, with repeated code:</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">add_executable</span>(a.out main.cpp)</span><br><span class="line"><span class="keyword">target_include_directories</span>(a.out PUBLIC hellolib)</span><br><span class="line"><span class="keyword">add_executable</span>(b.out main.cpp)</span><br><span class="line"><span class="keyword">target_include_directories</span>(b.out PUBLIC hellolib)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>A solution is to move the <code>target_include_directories()</code> to the subdirectory. Then all the further library&#x2F;executable relied on the <code>hellolib</code> will include this subdirectory. </p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sub-directory</span></span><br><span class="line"><span class="keyword">target_include_directories</span>(hellolib PUBLIC .)</span><br></pre></td></tr></table></figure>

<p>If we change the <code>PUBLIC</code> to <code>PRIVATE</code>, then the further dependent would not have the effects.</p>
<h2 id="Link-existing-library"><a href="#Link-existing-library" class="headerlink" title="Link existing library"></a>Link existing library</h2><p>For example, use the following code to link the <code>OpenMP</code> library. </p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">find_package</span>(OpenMP REQUIRD)</span><br><span class="line"><span class="keyword">target_link_libraries</span>(main PUBLIC OpenMP::OpenMP_CXX)</span><br></pre></td></tr></table></figure>

<p>Use the following code to link the <code>OpenMP</code> library. </p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">find_package</span>(OpenCV REQUIRED)</span><br><span class="line"><span class="keyword">target_link_libraries</span>(main <span class="variable">$&#123;OpenCV_LIBS&#125;</span>)</span><br></pre></td></tr></table></figure>



<h2 id="Further-options"><a href="#Further-options" class="headerlink" title="Further options"></a>Further options</h2><ul>
<li>Set release type (Default type is DEBUG):</li>
</ul>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span>(CMAKE_BUILD_TYPE Release)</span><br><span class="line"><span class="comment"># Or set it when building</span></span><br><span class="line">cmake --build build --config Release</span><br></pre></td></tr></table></figure>

<ul>
<li>Set C++ standard:</li>
</ul>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SET</span>(CMAKE_CXX_STANDARD <span class="number">17</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>Set global &#x2F; special macros:</li>
</ul>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># global</span></span><br><span class="line"><span class="keyword">add_definitions</span>(-DDEBUG) <span class="comment"># -D is not necessary</span></span><br><span class="line"><span class="keyword">add_definitions</span>(DEBUG)</span><br><span class="line"><span class="comment"># special target</span></span><br><span class="line"><span class="keyword">target_compile_definitions</span>(a.out PUBLIC -DDEBUG)</span><br><span class="line"><span class="keyword">target_compile_definitions</span>(a.out PUBLIC DEBUG)</span><br><span class="line"></span><br><span class="line"><span class="comment"># They have the same effect as</span></span><br><span class="line">g++ xx.cpp -DDEBUG <span class="comment"># (define a `DEBUG` macro to the file)</span></span><br></pre></td></tr></table></figure>

<ul>
<li>Set global &#x2F; special compiling options:</li>
</ul>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># global</span></span><br><span class="line"><span class="keyword">add_compile_options</span>(-O2)</span><br><span class="line"><span class="comment"># special target</span></span><br><span class="line"><span class="keyword">target_compile_options</span>(a.out PUBLIC -O0)</span><br><span class="line"></span><br><span class="line"><span class="comment"># They have the same effect as</span></span><br><span class="line">g++ xx.cpp -O0 <span class="comment"># (add a `-O0` option in the compilation)</span></span><br></pre></td></tr></table></figure>

<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set SIMD and fast-math</span></span><br><span class="line"><span class="keyword">target_compile_options</span>(a.out PUBLIC -ffast-<span class="keyword">math</span> -march=native)</span><br></pre></td></tr></table></figure>

<ul>
<li>Set global &#x2F; special include directories:</li>
</ul>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># global</span></span><br><span class="line"><span class="keyword">include_directories</span>(hellolib)</span><br><span class="line"><span class="comment"># special target</span></span><br><span class="line"><span class="keyword">target_include_directories</span>(a.out PUBLIC hellolib)</span><br></pre></td></tr></table></figure>




<h2 id="CUDA-with-CMake"><a href="#CUDA-with-CMake" class="headerlink" title="CUDA with CMake"></a>CUDA with CMake</h2><p>A common template can be:</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.10</span>)</span><br><span class="line"><span class="keyword">project</span>(main LANGUAGES CUDA CXX)</span><br><span class="line"></span><br><span class="line"><span class="keyword">SET</span>(CMAKE_CXX_STANDARD <span class="number">17</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">add_executable</span>(main main.cu)</span><br><span class="line"><span class="keyword">set_target_properties</span>(main PROPERTIES CUDA_ARCHITECTURES <span class="string">&quot;75&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/05/27/Pytorch-Practical-Basics-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Future">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Future's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/05/27/Pytorch-Practical-Basics-2/" class="post-title-link" itemprop="url">PyTorch Practical Hand-Written Modules Basics 2--Tensor basics (Tensor functions, "axis" and indexing)</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-05-27 13:32:58" itemprop="dateCreated datePublished" datetime="2023-05-27T13:32:58+08:00">2023-05-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-06-24 18:55:04" itemprop="dateModified" datetime="2023-06-24T18:55:04+08:00">2023-06-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>In this section, we will briefly talk about the arithmetic functions in the PyTorch. Then, we will introduce the <code>axis</code> parameter in most of these functions in detail. </p>
<p>Finally, we talk about indexing the tensor, which is very tricky in manipulating the tensors as well. </p>
<h2 id="Tensor-functions"><a href="#Tensor-functions" class="headerlink" title="Tensor functions"></a>Tensor functions</h2><p>PyTorch supports many arithmetic functions for tensor. They are vectorized and acts very similar to <code>numpy</code>. (So if you are not familiar with <code>numpy</code>, learn it first). In the following, I’ll introduce some functions with the official docs.</p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#pointwise-ops">binary arithmetic functions</a>, such as <code>+, -, *, /, @</code> etc. Entry-wise operations, supports <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/notes/broadcasting.html">broadcasting</a>.</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#pointwise-ops">binary logical functions</a>, such as <code>torch.bitwise_and()</code>, <code>torch.bitwise_or</code>…</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#pointwise-ops">math functions</a>, such as <code>exp, log, sigmoid</code> etc.</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#comparison-ops">comparison functions</a>, such as <code>torch.eq</code>, <code>torch.ge</code>. The <code>==</code> and <code>&gt;=</code> operators are overloaded, so they have the same effect.</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#reduction-ops">reduction functions</a>. They are usually very useful. e.g., <code>mean</code>, <code>median</code>, <code>argmax</code>, <code>sum</code>… They do the corresponding operations on a specific dimension, requiring the “dim” parameter (See below). </p>
</li>
<li><p>…… For more functions, please visit the <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#module-torch">docs</a>.</p>
</li>
</ul>
<h3 id="Key-What-is-the-“dim”-parameter"><a href="#Key-What-is-the-“dim”-parameter" class="headerlink" title="Key: What is the “dim” parameter?"></a>Key: What is the “dim” parameter?</h3><p>For the reduction functions such as <code>argmax</code>, we need to pass a parameter called <code>dim</code>. What does it mean?</p>
<ul>
<li><p>The default value or <code>dim</code> is <code>None</code>, indicates that do the <code>argmax</code> for all the entries. </p>
</li>
<li><p>On the other hand, if we specifies the <code>dim</code> parameter, that means, we apply the function <code>argmax</code> on each vector along a specific “axis”. For all of the example below, we use a <code>4x3x4</code> 3D tensor.</p>
</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create a 4x3x4 tensor</span></span><br><span class="line">a = torch.randn(<span class="number">4</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>



<ol>
<li>Then, in the first case, we do:</li>
</ol>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a1 = torch.argmax(a, dim=<span class="number">0</span>)</span><br><span class="line">a1.shape</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure>

<p>See the gif below. If we set <code>dim=0</code>, that means, we apply the <code>argmax</code> function on each yellow vector (they are in the direction of dim0). The original tensor’s shape is 4x3x4, we reduce on the dim0, so now it’s 3x4, containing all results from <code>argmax</code> on the yellow vectors. </p>
<img src="/2023/05/27/Pytorch-Practical-Basics-2/dim0.gif" alt="dim0" style="zoom: 67%;">



<ol start="2">
<li>Then, in the second case, we do:</li>
</ol>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a2 = torch.argmax(a, dim=<span class="number">1</span>)</span><br><span class="line">a2.shape</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure>

<p>See the gif below. If we set <code>dim=1</code>, that means, we apply the <code>argmax</code> function on each yellow vector (they are in the direction of dim1). The original tensor’s shape is 4x3x4, we reduce on the dim1, so now we will have a result with 4x4 shape. </p>
<img src="/2023/05/27/Pytorch-Practical-Basics-2/dim1.gif" alt="dim1" style="zoom: 67%;">



<ol start="3">
<li>Then, in the third case, we do:</li>
</ol>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a3 = torch.argmax(a, dim=<span class="number">2</span>)</span><br><span class="line">a3.shape</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>

<p>See the gif below. If we set <code>dim=2</code>, that means, we apply the <code>argmax</code> function on each yellow vector (they are in the direction of dim2). The original tensor’s shape is 4x3x4, we reduce on the dim2, so now we will have a result with 4x3 shape. </p>
<img src="/2023/05/27/Pytorch-Practical-Basics-2/dim2.gif" alt="dim2" style="zoom: 67%;">

<h3 id="As-member-function"><a href="#As-member-function" class="headerlink" title="As member function"></a>As member function</h3><p>Many functions mentioned above has member function style. For example, the following pairs are equivalent.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment"># pair1</span></span><br><span class="line">_ = torch.<span class="built_in">sum</span>(a)</span><br><span class="line">_ = a.<span class="built_in">sum</span>()</span><br><span class="line"><span class="comment"># pair2</span></span><br><span class="line">_ = torch.argmax(a, dim=<span class="number">0</span>)</span><br><span class="line">_ = a.argmax(dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<h3 id="As-in-place-function"><a href="#As-in-place-function" class="headerlink" title="As in-place function"></a>As in-place function</h3><p>The functions mentioned above returns a new result tensor, keeping the original one same. In some cases, we can do in-place operation on the tensor. The in-place functions are terminated with a <code>_</code>. </p>
<p>For example, the following pairs are equivalent.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment"># pair 1</span></span><br><span class="line">a = torch.cos(a)</span><br><span class="line">a = a.cos()</span><br><span class="line">a.cos_()</span><br><span class="line"><span class="comment"># pair 2</span></span><br><span class="line">a = torch.clamp(a, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">a = a.clamp(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">a.clamp_(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">a.clamp(<span class="number">1</span>, <span class="number">2</span>) <span class="comment"># Wrong: this line has no effect. The a remains same; the return value was assigned to nothing.</span></span><br></pre></td></tr></table></figure>





<h2 id="Tensor-indexing"><a href="#Tensor-indexing" class="headerlink" title="Tensor indexing"></a>Tensor indexing</h2><p>Indexing is very powerful in torch. They are very similar to the one in <code>numpy</code>.  Learn <code>numpy</code> first if you are not familiar with it.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># a is</span></span><br><span class="line">tensor([[ <span class="number">1.1351</span>,  <span class="number">0.7592</span>, -<span class="number">3.5945</span>],</span><br><span class="line">        [ <span class="number">0.0192</span>,  <span class="number">0.1052</span>,  <span class="number">0.9603</span>],</span><br><span class="line">        [-<span class="number">0.5672</span>, -<span class="number">0.5706</span>,  <span class="number">1.5980</span>],</span><br><span class="line">        [ <span class="number">0.1115</span>, -<span class="number">0.0392</span>,  <span class="number">1.4112</span>]])</span><br></pre></td></tr></table></figure>

<p>The indexing supports many types, you can pass:</p>
<ul>
<li><p>An integer. <code>a[1, 2]</code> returns just one value 0-D tensor <code>tensor(0.9603)</code>, one element at (row 1, col 2).</p>
</li>
<li><p>A Slice. <code>a[1::2, 2]</code> returns 1-D tensor <code>tensor([0.9603, 1.4112])</code>, two elements at (row 1, col 2) and (row 3, col 2).</p>
</li>
<li><p>A colon. colon means everything on this dim.<code>a[:, 2]</code> returns 1-D tensor <code>tensor([-3.5945,  0.9603,  1.5980,  1.4112])</code>, a column of 4 elements at col 2.</p>
</li>
<li><p>A None. None is used to create a new dim on the given axis. E.g., <code>a[:, None, :]</code> has the shape of <code>torch.Size([4, 1, 3])</code>. A further example:</p>
</li>
</ul>
<p>​		<code>a[:, 2]</code> returns 1-D vector <code>tensor([-3.5945,  0.9603,  1.5980,  1.4112])</code> of col 2.</p>
<p>​		<code>a[:, 2, None]</code> returns 2-D vector <code>tensor([[-3.5945],  [0.9603],  [1.5980],  [1.4112]])</code> of col 2, which the original shape is kept.</p>
<ul>
<li><p>A <code>...</code> (Ellipsis). Ellipsis can be used as multiple <code>:</code>. E.g., </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">16</span>).reshape(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"><span class="comment"># The following returns the same value</span></span><br><span class="line">a[..., <span class="number">1</span>]</span><br><span class="line">a[:, :, :, <span class="number">1</span>]</span><br></pre></td></tr></table></figure></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Future</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">25</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Future</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
