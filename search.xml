<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>How to use this blog 本站食用指南</title>
    <url>/2023/03/21/%E6%9C%AC%E7%AB%99%E9%A3%9F%E7%94%A8%E6%8C%87%E5%8D%97-How-to-use-this-blog/</url>
    <content><![CDATA[<p>Hello! 你好！</p>
<p>This blog is established on 2023&#x2F;3&#x2F;21. The aim to build such a blog website which is used to share my programming &#x2F; other knowledges.</p>
<p>本站建立于2023.3.21，建立之初是为了拥有一个自己的知识内容分享平台。</p>
<p>This is the home page, and this article is pinned. If you want to search for some past articles, use “Categories”, “Archives” or “Search” on the top of the page.</p>
<p>这是主页。这篇文章置顶于此。如果你想找指定文章内容，可以在页面上方“Categories”或者“Archives”寻找。</p>
<p><strong>Future and Plans 计划与憧憬</strong></p>
<p>On the first day of our blog, let’s plan it: 建博客第一天，就先来立一些这个博客的计划：</p>
<ul>
<li>Language 语言：We will mainly use English in this blog. 网站主要使用英文。（单词也比较简单，对于中文用户其实阅读难度不大啦）</li>
<li>Contents 内容：The content should mainly be my original content, or a comprehensive content that combines other existing materials with my annotations or supplements. 内容主要应该是我的原创内容，或者是结合了其他现有资料，配合我的注释或是增补的综合内容。</li>
<li>Categories 类别：<ul>
<li>Deep Learning: Things related to PyTorch, deep learning。和torch，深度学习相关的会在这。</li>
<li>QRH: Quick Reference Handbook of tools&#x2F;programming languages&#x2F;packages, 一些工具、软件、包、语言的快速参考手册。</li>
<li>QuickIntro: Quick Introduction to a tools&#x2F;programming languages&#x2F;packages, 快速入门某一工具、软件、包、语言。</li>
<li>Techniques: Techniques and tricks in something. 技艺，一些环境配置、瞎折腾的技术、搭建过程会写在这。</li>
<li>Problem Solving: Record the problems I met and how did I solve it out. 记录我遇到的问题以及我的解决方案。</li>
<li>Others: 其它。</li>
</ul>
</li>
</ul>
<p><strong>TODO list：</strong></p>
<p>在下面列一些近期可能想做的topic（立flag中）</p>
<ul>
<li>QRH</li>
</ul>
<ul>
<li><input checked disabled type="checkbox"> Matplotlib common functions</li>
<li><input disabled type="checkbox"> conda common commands</li>
<li><input disabled type="checkbox"> Linux shell common commands</li>
<li><input disabled type="checkbox"> vim common keys</li>
<li><input checked disabled type="checkbox"> latex common code blocks</li>
<li><input disabled type="checkbox"> </li>
</ul>
<ul>
<li>QuickIntro</li>
</ul>
<ul>
<li><input checked disabled type="checkbox"> NumPy quick intro</li>
<li><input disabled type="checkbox"> 15 min regex</li>
<li><input disabled type="checkbox"> 15 min  Makefile</li>
<li><input disabled type="checkbox"> 15 min  MySQL</li>
<li><input disabled type="checkbox"> </li>
</ul>
<ul>
<li>Techniques</li>
</ul>
<ul>
<li><input checked disabled type="checkbox"> C++ complex type declaration rules (In Chinese, 2&#x2F;2)</li>
<li><input disabled type="checkbox"> </li>
</ul>
<ul>
<li>Others</li>
</ul>
<ul>
<li><input disabled type="checkbox"> Emm…</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>Pandas &amp; MySQL Leetcode Practice Notes 2--String operations</title>
    <url>/2023/09/02/Pandas-MySQL-Leetcode-Practice-Notes-2/</url>
    <content><![CDATA[<h1 id="2-String-Operations"><a href="#2-String-Operations" class="headerlink" title="2. String Operations"></a>2. String Operations</h1><h2 id="2-1-MySQL"><a href="#2-1-MySQL" class="headerlink" title="2.1 MySQL"></a>2.1 MySQL</h2><h2 id="2-2-Pandas"><a href="#2-2-Pandas" class="headerlink" title="2.2 Pandas"></a>2.2 Pandas</h2><p>For numeric data types, we usually directly use grammar like <code>df[&#39;col&#39;] &gt; 5</code> to get the indexing series. For string operations, we have some similar ways to get the indexing series. </p>
<p><strong>String Accessor.</strong> For each column, we can use <code>.str</code> to get its <code>pandas.core.strings.accessor.StringMethods</code>. This object has various of string operation functions, such as <code>.replace()</code>, <code>.find()</code>, <code>.len()</code>, <code>.startswith()</code>…… </p>
<ul>
<li>Some function, such as <code>.len()</code>, returns a new integer series, containing length of string for each entry:</li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># df is:</span></span><br><span class="line">   <span class="built_in">id</span> content</span><br><span class="line"><span class="number">0</span>   <span class="number">1</span>   apple</span><br><span class="line"><span class="number">1</span>   <span class="number">2</span>  banana</span><br><span class="line"><span class="number">2</span>   <span class="number">3</span>  orange</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get len of each entry in &quot;content&quot; column</span></span><br><span class="line">df[<span class="string">&#x27;content&#x27;</span>].<span class="built_in">str</span>.<span class="built_in">len</span>()</span><br><span class="line"><span class="number">0</span>    <span class="number">5</span></span><br><span class="line"><span class="number">1</span>    <span class="number">6</span></span><br><span class="line"><span class="number">2</span>    <span class="number">6</span></span><br><span class="line">Name: content, dtype: int64</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assign the len series to df, named &quot;content_len&quot;:</span></span><br><span class="line">df[<span class="string">&#x27;content_len&#x27;</span>] = df[<span class="string">&#x27;content&#x27;</span>].<span class="built_in">str</span>.<span class="built_in">len</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># df now is:</span></span><br><span class="line">   <span class="built_in">id</span> content  content_len</span><br><span class="line"><span class="number">0</span>   <span class="number">1</span>   apple            <span class="number">5</span></span><br><span class="line"><span class="number">1</span>   <span class="number">2</span>  banana            <span class="number">6</span></span><br><span class="line"><span class="number">2</span>   <span class="number">3</span>  orange            <span class="number">6</span></span><br></pre></td></tr></table></figure>

<ul>
<li>Some function, like <code>.startwith()</code>, returns a new boolean series, so you can do indexing:</li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># df is:</span></span><br><span class="line">   <span class="built_in">id</span> content</span><br><span class="line"><span class="number">0</span>   <span class="number">1</span>   apple</span><br><span class="line"><span class="number">1</span>   <span class="number">2</span>  banana</span><br><span class="line"><span class="number">2</span>   <span class="number">3</span>  orange</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get indexing series where &quot;content&quot; entry starts with &quot;a&quot;</span></span><br><span class="line">cond = df[<span class="string">&#x27;content&#x27;</span>].<span class="built_in">str</span>.startswith(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># cond is:</span></span><br><span class="line"><span class="number">0</span>     <span class="literal">True</span></span><br><span class="line"><span class="number">1</span>    <span class="literal">False</span></span><br><span class="line"><span class="number">2</span>    <span class="literal">False</span></span><br><span class="line">Name: content, dtype: <span class="built_in">bool</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Do filtering</span></span><br><span class="line">df = df.loc[cond, :]</span><br><span class="line"></span><br><span class="line"><span class="comment"># df is:</span></span><br><span class="line">   <span class="built_in">id</span> content</span><br><span class="line"><span class="number">0</span>   <span class="number">1</span>   apple</span><br></pre></td></tr></table></figure>

<ul>
<li>Regular Expression</li>
</ul>
<p>Sometimes, we want to utilize Regex to help us match entries. We have <code>.match()</code> function. Note that <code>match</code> will exactly match from the beginning. If you just want to match any substring, use <code>.contains</code> instead. </p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># df is:</span></span><br><span class="line">   <span class="built_in">id</span> content</span><br><span class="line"><span class="number">0</span>   <span class="number">1</span>   apple</span><br><span class="line"><span class="number">1</span>   <span class="number">2</span>  banana</span><br><span class="line"><span class="number">2</span>   <span class="number">3</span>  orange</span><br><span class="line"></span><br><span class="line"><span class="comment"># Match all entries, begining with &quot;a&quot;</span></span><br><span class="line">cond = df[<span class="string">&#x27;content&#x27;</span>].<span class="built_in">str</span>.<span class="keyword">match</span>(<span class="string">r&#x27;a[a-zA-Z]*&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Result will be:</span></span><br><span class="line">df.loc[cond, :]</span><br><span class="line">   <span class="built_in">id</span> content</span><br><span class="line"><span class="number">0</span>   <span class="number">1</span>   apple</span><br><span class="line"></span><br><span class="line"><span class="comment"># Match all entries, containing &quot;an&quot;</span></span><br><span class="line">cond = df[<span class="string">&#x27;content&#x27;</span>].<span class="built_in">str</span>.contains(<span class="string">r&#x27;an&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Result will be:</span></span><br><span class="line">df.loc[cond, :]</span><br><span class="line">   <span class="built_in">id</span> content</span><br><span class="line"><span class="number">1</span>   <span class="number">2</span>  banana</span><br><span class="line"><span class="number">2</span>   <span class="number">3</span>  orange</span><br></pre></td></tr></table></figure>



]]></content>
      <categories>
        <category>Pandas_SQL</category>
      </categories>
  </entry>
  <entry>
    <title>Pandas &amp; MySQL Leetcode Practice Notes 1--Condition Filter &amp; Misc</title>
    <url>/2023/08/20/Pandas-MySQL-Leetcode-Practice-Notes-1/</url>
    <content><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>There are 30 questions to practice Python Pandas and MySQL on <a href="https://leetcode.cn/studyplan/30-days-of-pandas/">https://leetcode.cn/studyplan/30-days-of-pandas/</a> . These questions basically practice the fundamental skills. Pandas and MySQL are similar in some aspects, and one can write the same functional code in two languages. I’ll practice my skill with these questions, and write notes about the equivalent operations in two languages. </p>
<p>Accordingly, there are six sections in this section (so will use 6 articles in total):</p>
<ol>
<li>Condition Filter &amp; Misc</li>
<li>String Manipulation</li>
<li>Data Manipulation</li>
<li>Data Statistics</li>
<li>Grouping</li>
<li>Merging</li>
</ol>
<h1 id="1-Condition-Filter-Join-Misc"><a href="#1-Condition-Filter-Join-Misc" class="headerlink" title="1. Condition Filter &amp; Join &amp; Misc"></a>1. Condition Filter &amp; Join &amp; Misc</h1><p>The four related question in this area are:</p>
<p>Basic <code>where</code> clause filtering, with <code>OR</code> operator: <a href="https://leetcode.cn/problems/big-countries/">https://leetcode.cn/problems/big-countries/</a></p>
<p>Basic <code>where</code> clause filtering, with <code>AND</code> operator: <a href="https://leetcode.cn/problems/recyclable-and-low-fat-products/">https://leetcode.cn/problems/recyclable-and-low-fat-products/</a></p>
<p>Joining two tables, with <code>where</code> clause filtering null: <a href="https://leetcode.cn/problems/customers-who-never-order/">https://leetcode.cn/problems/customers-who-never-order/</a></p>
<p>Basic <code>where</code> clause filtering, with <code>DISTINCT</code> and <code>sort</code>: <a href="https://leetcode.cn/problems/article-views-i">https://leetcode.cn/problems/article-views-i</a></p>
<h2 id="1-1-Condition"><a href="#1-1-Condition" class="headerlink" title="1.1 Condition"></a>1.1 Condition</h2><p>Typical condition filter work is asking us to do like this:</p>
<blockquote>
<p>Filter the <strong>big</strong> country in World scheme. A country is <strong>big</strong> if:</p>
<ul>
<li>it has an area of at least three million (i.e., <code>3000000 km2</code>), or</li>
<li>it has a population of at least twenty-five million (i.e., <code>25000000</code>).</li>
</ul>
</blockquote>
<p>We’ll use this example to illustrate the point. </p>
<h3 id="1-1-a-MySQL"><a href="#1-1-a-MySQL" class="headerlink" title="1.1.a MySQL"></a>1.1.a MySQL</h3><p>A typical condition filter in SQL looks like this:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">&lt;</span>col_names<span class="operator">&gt;</span></span><br><span class="line"><span class="keyword">from</span> <span class="operator">&lt;</span>table_name<span class="operator">&gt;</span></span><br><span class="line"><span class="keyword">where</span> (</span><br><span class="line">	<span class="operator">&lt;</span><span class="keyword">condition</span> predicate<span class="operator">&gt;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>SQL allows the use of the logical connectives <strong>and, or, and not</strong>. </p>
<p>The operands of the logical connectives can be expressions involving the comparison operators <strong>&lt;, &lt;&#x3D;, &gt;, &gt;&#x3D;, &#x3D;, and &lt;&gt;</strong> (Note that, equal symbol is one <code>=</code>, but not <code>==</code>!)</p>
<p>For example, in the codes above, the answer is</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> name, population, area</span><br><span class="line"><span class="keyword">FROM</span> World</span><br><span class="line"><span class="keyword">WHERE</span> area <span class="operator">&gt;=</span> <span class="number">3000000</span> <span class="keyword">OR</span> population <span class="operator">&gt;=</span> <span class="number">25000000</span></span><br></pre></td></tr></table></figure>



<h3 id="1-1-b-Pandas"><a href="#1-1-b-Pandas" class="headerlink" title="1.1.b Pandas"></a>1.1.b Pandas</h3><p>Filter operations in pandas is a little bit difference. We should know two basics things first:</p>
<ul>
<li>Condition result in Index Series</li>
</ul>
<p>If we want to get all data that are greater than 5 in <code>df</code>‘s <code>col1</code> column, the code and result is as below:</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(df)</span><br><span class="line"><span class="comment"># OUTPUT:</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  name  col1  col2</span></span><br><span class="line"><span class="string">0    a     1    10</span></span><br><span class="line"><span class="string">1    b     2     9</span></span><br><span class="line"><span class="string">2    c     3     8</span></span><br><span class="line"><span class="string">3    d     4     7</span></span><br><span class="line"><span class="string">4    e     5     6</span></span><br><span class="line"><span class="string">5    f     6     5</span></span><br><span class="line"><span class="string">6    g     7     4</span></span><br><span class="line"><span class="string">7    h     8     3</span></span><br><span class="line"><span class="string">8    i     9     2</span></span><br><span class="line"><span class="string">9    j    10     1</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(df[<span class="string">&#x27;col1&#x27;</span>] &gt; <span class="number">5</span>)</span><br><span class="line"><span class="comment"># OUTPUT: </span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">0    False</span></span><br><span class="line"><span class="string">1    False</span></span><br><span class="line"><span class="string">2    False</span></span><br><span class="line"><span class="string">3    False</span></span><br><span class="line"><span class="string">4    False</span></span><br><span class="line"><span class="string">5     True</span></span><br><span class="line"><span class="string">6     True</span></span><br><span class="line"><span class="string">7     True</span></span><br><span class="line"><span class="string">8     True</span></span><br><span class="line"><span class="string">9     True</span></span><br><span class="line"><span class="string">Name: col1, dtype: bool</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<p>As you can see, the result of <code>df[&#39;col1&#39;] &gt; 5</code> is a bool Series with <code>len(df)</code> size. The entry is True only when the corresponding entry at <code>col1</code> satisfies the condition. </p>
<ul>
<li>Indexing by the Index Series</li>
</ul>
<p>We can pass this bool index series in to the <code>df.loc[]</code>. Then, only the rows with the condition satisfied are displayed:</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">a = df.loc[df[<span class="string">&#x27;col1&#x27;</span>] &gt; <span class="number">5</span>]</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  name  col1  col2</span></span><br><span class="line"><span class="string">5    f     6     5</span></span><br><span class="line"><span class="string">6    g     7     4</span></span><br><span class="line"><span class="string">7    h     8     3</span></span><br><span class="line"><span class="string">8    i     9     2</span></span><br><span class="line"><span class="string">9    j    10     1</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<p>By such method, we can do the filtering in the pandas. Note that:</p>
<p>For the <code>and</code>, <code>or</code> and <code>not</code> logic operators, the corresponding operator in pandas is <code>&amp;</code>, <code>|</code> and <code>~</code>. For example:</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">cond1 = df[<span class="string">&#x27;col1&#x27;</span>] &gt; <span class="number">5</span></span><br><span class="line">cond2 = df[<span class="string">&#x27;col2&#x27;</span>] &gt; <span class="number">2</span></span><br><span class="line"></span><br><span class="line">b = df.loc[cond1 &amp; cond2]</span><br><span class="line"></span><br><span class="line">c = df.loc[ (df[<span class="string">&#x27;col1&#x27;</span>] &gt; <span class="number">5</span>) &amp; (df[<span class="string">&#x27;col2&#x27;</span>] &gt; <span class="number">2</span>) ]</span><br></pre></td></tr></table></figure>

<p><strong>Note</strong> that the <code>()</code> symbols are necessary because of the operator precedence. For example, in the codes above, the answer is:</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">cond1 = world[<span class="string">&#x27;area&#x27;</span>] &gt;= <span class="number">300_0000</span></span><br><span class="line">cond2 = world[<span class="string">&#x27;population&#x27;</span>] &gt;= <span class="number">2500_0000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> world.loc[cond1 | cond2, [<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;population&#x27;</span>, <span class="string">&#x27;area&#x27;</span>]]</span><br></pre></td></tr></table></figure>



<h2 id="1-2-Misc"><a href="#1-2-Misc" class="headerlink" title="1.2 Misc"></a>1.2 Misc</h2><h3 id="1-2-1-Rename-output-columns"><a href="#1-2-1-Rename-output-columns" class="headerlink" title="1.2.1 Rename output columns"></a>1.2.1 Rename output columns</h3><h4 id="1-2-1-a-MySQL"><a href="#1-2-1-a-MySQL" class="headerlink" title="1.2.1.a MySQL"></a>1.2.1.a MySQL</h4><p>In MySQL, rename a column is very easy. By default, if the command is</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> name, population, area</span><br><span class="line"><span class="keyword">FROM</span> World</span><br><span class="line"><span class="keyword">WHERE</span> area <span class="operator">&gt;=</span> <span class="number">3000000</span> <span class="keyword">OR</span> population <span class="operator">&gt;=</span> <span class="number">25000000</span></span><br></pre></td></tr></table></figure>

<p>Then the output columns are named <code>name</code>, <code>population</code>, and <code>area</code>. To rename, just use the <code>as</code> symbol to give a new name as output:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- change name =&gt; Name, population =&gt; Population, area =&gt; Area</span></span><br><span class="line"><span class="keyword">SELECT</span> name <span class="keyword">as</span> Name, population <span class="keyword">as</span> Population, area <span class="keyword">as</span> Area</span><br><span class="line"><span class="keyword">FROM</span> World</span><br><span class="line"><span class="keyword">WHERE</span> area <span class="operator">&gt;=</span> <span class="number">3000000</span> <span class="keyword">OR</span> population <span class="operator">&gt;=</span> <span class="number">25000000</span></span><br></pre></td></tr></table></figure>

<h4 id="1-2-1-b-Pandas"><a href="#1-2-1-b-Pandas" class="headerlink" title="1.2.1.b Pandas"></a>1.2.1.b Pandas</h4><p>In pandas, we can use the <code>pd.DataFrame.rename</code> function to rename the columns. The input parameter <code>columns</code> is a <code>dict[str, str]</code>, where <code>key</code> is the old name, and the <code>value</code> is the new name. For example in the example below, we make all names capitalized:</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">df = df.rename(columns=&#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Name&#x27;</span>, <span class="string">&#x27;population&#x27;</span>: <span class="string">&#x27;Population&#x27;</span>, <span class="string">&#x27;area&#x27;</span>: <span class="string">&#x27;Area&#x27;</span>&#125;)</span><br></pre></td></tr></table></figure>

<h3 id="1-2-2-Swap-output-columns-order"><a href="#1-2-2-Swap-output-columns-order" class="headerlink" title="1.2.2 Swap output columns order"></a>1.2.2 Swap output columns order</h3><h4 id="1-2-2-a-MySQL"><a href="#1-2-2-a-MySQL" class="headerlink" title="1.2.2.a MySQL"></a>1.2.2.a MySQL</h4><p>In SQL, this work is relatively easy. You only need to swap the column names in the <code>select</code> command: </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- show with A, B, C columns </span></span><br><span class="line"><span class="keyword">select</span> A, B, C <span class="keyword">from</span> <span class="keyword">table</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- show with C, B, A columns </span></span><br><span class="line"><span class="keyword">select</span> C, B, A <span class="keyword">from</span> <span class="keyword">table</span>;</span><br></pre></td></tr></table></figure>

<h4 id="1-2-2-b-Pandas"><a href="#1-2-2-b-Pandas" class="headerlink" title="1.2.2.b Pandas"></a>1.2.2.b Pandas</h4><p>The method is relatively similar as the code in MySQL, by:</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># show with A, B, C columns </span></span><br><span class="line">df.loc[:, [<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;C&#x27;</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># show with C, B, A columns </span></span><br><span class="line">df.loc[:, [<span class="string">&#x27;C&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;A&#x27;</span>]]</span><br></pre></td></tr></table></figure>

<h3 id="1-2-3-Remove-duplicate-rows"><a href="#1-2-3-Remove-duplicate-rows" class="headerlink" title="1.2.3 Remove duplicate rows"></a>1.2.3 Remove duplicate rows</h3><h4 id="1-2-3-a-MySQL"><a href="#1-2-3-a-MySQL" class="headerlink" title="1.2.3.a MySQL"></a>1.2.3.a MySQL</h4><p>It is quite easy to do that: you only need to add <code>DISTINCT</code> keyword. For example: </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">DISTINCT</span> student_id</span><br><span class="line"><span class="keyword">from</span> courses</span><br></pre></td></tr></table></figure>

<h4 id="1-2-3-b-Pandas"><a href="#1-2-3-b-Pandas" class="headerlink" title="1.2.3.b Pandas"></a>1.2.3.b Pandas</h4><p>The DataFrame has a method called <code>drop_duplicates()</code>. For example: </p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">selected_views = selected_views.drop_duplicates()</span><br></pre></td></tr></table></figure>

<p>More advanced usage is use parameter <code>subset</code> to  tell which columns for identifying duplicates, by default use all of the columns. For example, the following code drop the duplicate rows whenever they have same value in column <code>teacher_id</code>, <code>subject_id</code>. </p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">teacher = teacher.drop_duplicates(subset=[<span class="string">&#x27;teacher_id&#x27;</span>, <span class="string">&#x27;subject_id&#x27;</span>])</span><br></pre></td></tr></table></figure>

<h3 id="1-2-4-Sort"><a href="#1-2-4-Sort" class="headerlink" title="1.2.4 Sort"></a>1.2.4 Sort</h3><h4 id="1-2-4-a-MySQL"><a href="#1-2-4-a-MySQL" class="headerlink" title="1.2.4.a MySQL"></a>1.2.4.a MySQL</h4><p>In the <code>select</code> statement, add with <code>order by &lt;col&gt; asc/desc</code> statement. <code>&lt;col&gt;</code> indicates sort according to which column, <code>asc</code> means ascending sorting, <code>desc</code> means descending sorting. </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> v.author_id </span><br><span class="line"><span class="keyword">from</span> Views <span class="keyword">as</span> v</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> id <span class="keyword">asc</span></span><br></pre></td></tr></table></figure>

<p>To sort according multiple columns (if first column is the same, sort by second column), do it by:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> v.author_id </span><br><span class="line"><span class="keyword">from</span> Views <span class="keyword">as</span> v</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> id <span class="keyword">asc</span>, col2 <span class="keyword">desc</span></span><br></pre></td></tr></table></figure>

<h4 id="1-2-4-b-Pandas"><a href="#1-2-4-b-Pandas" class="headerlink" title="1.2.4.b Pandas"></a>1.2.4.b Pandas</h4><p>Use the <code>pd.DataFrame.sort_values()</code> to sort according to which column(s). If we want to sort from biggest to smallest, use <code>ascending</code> parameter. </p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># sort by one column, id</span></span><br><span class="line">selected_views = selected_views.sort_values(by=[<span class="string">&#x27;id&#x27;</span>])</span><br><span class="line">selected_views = selected_views.sort_values(by=[<span class="string">&#x27;id&#x27;</span>], ascending=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sort by two columns</span></span><br><span class="line"><span class="comment"># if first column is the same, sort by second column</span></span><br><span class="line">selected_views = selected_views.sort_values(by=[<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;view_date&#x27;</span>])</span><br><span class="line">selected_views = selected_views.sort_values(by=[<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;view_date&#x27;</span>], ascending=[<span class="literal">True</span>, <span class="literal">False</span>])</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Pandas_SQL</category>
      </categories>
  </entry>
  <entry>
    <title>PyTorch Practical Hand-Written Modules Basics 10--Summary and Conclusion</title>
    <url>/2023/08/15/Pytorch-Practical-Basics-10/</url>
    <content><![CDATA[<p>Finally, we have finished all contents we want to talk about. In this section, we’ll do a quick summary about what we have talked about and plan for the future of this series.</p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>In our ten sections of tutorial, we are learning from low-level (tensors) to high-level (modules). In detail, the structure looks like this:</p>
<ul>
<li>Tensor operations (Sec <a href="https://i-am-future.github.io/2023/05/27/Pytorch-Practical-Basics-1/">1</a>, <a href="https://i-am-future.github.io/2023/05/27/Pytorch-Practical-Basics-2/">2</a>)</li>
<li>Tensor-wise operations (Sec <a href="https://i-am-future.github.io/2023/06/17/Pytorch-Practical-Basics-3/">3</a>)</li>
<li>Module basics (Sec <a href="https://i-am-future.github.io/2023/06/18/Pytorch-Practical-Basics-4/">4</a>)</li>
<li>Implement by pure-python (Sec <a href="https://i-am-future.github.io/2023/07/09/Pytorch-Practical-Basics-5/">5</a> ResNet)</li>
<li>Implement by CUDA (Sec <a href="https://i-am-future.github.io/2023/07/30/Pytorch-Practical-Basics-6/">6</a>, <a href="https://i-am-future.github.io/2023/08/01/Pytorch-Practical-Basics-7/">7</a>, <a href="https://i-am-future.github.io/2023/08/02/Pytorch-Practical-Basics-8/">8</a>, <a href="https://i-am-future.github.io/2023/08/07/Pytorch-Practical-Basics-9/">9</a>)</li>
</ul>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>From our tutorial, we know that the model consists of <code>nn.Module</code>s. We implement the <code>forward()</code> function with many tensor-wise operations to do the forward pass. </p>
<p>The PyTorch is highly optimized. The Python side <strong>is enough for most cases</strong>. So, it is unnecessary to implement the algorithm in C++/CUDA. (Ref to sec 9. Our CUDA matrix multiplication operation is slower than the PyTorch’s). In addition, when we are writing in native Python, we <strong>don’t need to worry about the correctness of the gradient calculation</strong>. </p>
<p>But just in some rare cases, the <code>forward()</code> implementation is complicated, and they may contain <code>for</code> loop. The performance is low. Under such circumstances, you may consider to write the operator by yourself. But keep in mind that:</p>
<ul>
<li>You need to check if the forward &amp; backward propagations are correct;</li>
<li>You need to do benchmarks - does my operator really get faster?</li>
</ul>
<p>Therefore, manually write a optimized CUDA operator is time consuming and complicated. In addition, one should be equipped with proficient CUDA knowledge. But once you write the good CUDA operators, your program will boost for many times. They are all about trade-off.</p>
<h1 id="Announce-in-Advance"><a href="#Announce-in-Advance" class="headerlink" title="Announce in Advance"></a>Announce in Advance</h1><p>Finally, let’s talk about some things I will do in the future: </p>
<ul>
<li>This series will not end. For this series article 11 and later: we’ll talk about some famous model implementations.</li>
<li>As I said above, writing CUDA operator needs proficient CUDA knowledge. So I’ll setup a new series to tell you how to write good CUDA programs: <em>CUDA Medium Tutorials</em></li>
</ul>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title>PyTorch Practical Hand-Written Modules Basics 9--Compiling and Testing the Module</title>
    <url>/2023/08/07/Pytorch-Practical-Basics-9/</url>
    <content><![CDATA[<p>In the section 6 to 9, we’ll investigate how to use <code>torch.autograd.Function</code> to implement the hand-written operators. The tentative outline is:</p>
<ul>
<li><input checked disabled type="checkbox"> In the section 6, we talk about the basics of <code>torch.autograd.Function</code>. <strong>The operators defined by <code>torch.autograd.Function</code> can be automatically back-propagated.</strong></li>
<li><input checked disabled type="checkbox"> In the last section (7), we’ll talk about <strong>mathematic derivation</strong> for the “linear layer” operator. </li>
<li><input checked disabled type="checkbox"> In the section (8), we talk about <strong>writing C++ CUDA extension</strong> for the “linear layer” operator. </li>
<li><input checked disabled type="checkbox"> In this section (9), we talk details about <strong>building the extension</strong> to a python module, as well as <strong>testing</strong> the module. Then we’ll <strong>conclude</strong> the things we’ve done so far.</li>
</ul>
<p><strong>Note:</strong></p>
<ul>
<li>This blog is written with following reference: <ul>
<li>PyTorch official tutorial about CUDA extension: <a href="https://pytorch.org/tutorials/advanced/cpp_extension.html">website</a>.</li>
<li>YouTube video about writing CUDA extension: <a href="https://www.youtube.com/watch?v=l_Rpk6CRJYI">video</a>, <a href="https://github.com/kwea123/pytorch-cppcuda-tutorial">code</a>.</li>
</ul>
</li>
<li>For how to write CUDA code, you can follow <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">official documentation</a>, <a href="https://zhuanlan.zhihu.com/p/34587739">blogs</a> (In Chinese). You can search by yourself for English tutorials and video tutorials.</li>
<li>This blog only talk some important points in the matrix multiplication example. Code are picked by pieces for illustration. Whole code is at: <a href="https://github.com/I-am-Future/PyTorch-Linear-Operator-CUDA">code</a>.</li>
</ul>
<h1 id="Python-side-Wrapper"><a href="#Python-side-Wrapper" class="headerlink" title="Python-side Wrapper"></a>Python-side Wrapper</h1><p>Purely using C++ extension functions is not enough in our case. As mentioned in the Section 6, we need to build our operators with <code>torch.autograd.Function</code>. It is not convenient to let the user define the operator wrapper every time, so it’s better if we can write the wrapper in a python module. Then, users can easily import our python module, and using the wrapper class and functions in it.</p>
<img src="/2023/08/07/Pytorch-Practical-Basics-9/cudaops-struct-improved.drawio.png" alt="cudaops-struct-improved.drawio" style="zoom:50%;">

<p>The python module is at <code>mylinearops/</code>. Follow the section 6, we define some <code>autograd.Function</code> operators and <code>nn.Module</code> modules in the <code>mylinearops/mylinearops.py</code>. Then, we export the operators and modules by the code in the <code>mylinearops/__init__.py</code>:</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> .mylinearops <span class="keyword">import</span> matmul</span><br><span class="line"><span class="keyword">from</span> .mylinearops <span class="keyword">import</span> linearop</span><br><span class="line"><span class="keyword">from</span> .mylinearops <span class="keyword">import</span> LinearLayer</span><br></pre></td></tr></table></figure>

<p>As a result, when user imports the <code>mylinearops</code>, only the <code>matmul</code> (Y &#x3D; XW) function, <code>linearop</code> (Y &#x3D; XW+b) function and <code>LinearLayer</code> module are public to the users. </p>
<h1 id="Writing-setup-py-and-Building"><a href="#Writing-setup-py-and-Building" class="headerlink" title="Writing setup.py and Building"></a>Writing setup.py and Building</h1><h2 id="setup-py-script"><a href="#setup-py-script" class="headerlink" title="setup.py script"></a>setup.py script</h2><p>The <code>setup.py</code> script is general same for all packages. Next time, you can just copy-paste the code above and modify some key components. </p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> os.path <span class="keyword">as</span> osp</span><br><span class="line"><span class="keyword">from</span> setuptools <span class="keyword">import</span> setup</span><br><span class="line"><span class="keyword">from</span> torch.utils.cpp_extension <span class="keyword">import</span> CUDAExtension, BuildExtension</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ROOT_DIR = osp.dirname(osp.abspath(__file__))</span><br><span class="line">include_dirs = [osp.join(ROOT_DIR, <span class="string">&quot;include&quot;</span>)]</span><br><span class="line"></span><br><span class="line">SRC_DIR = osp.join(ROOT_DIR, <span class="string">&quot;src&quot;</span>)</span><br><span class="line">sources = glob.glob(osp.join(SRC_DIR, <span class="string">&#x27;*.cpp&#x27;</span>))+glob.glob(osp.join(SRC_DIR, <span class="string">&#x27;*.cu&#x27;</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">setup(</span><br><span class="line">    name=<span class="string">&#x27;mylinearops&#x27;</span>,</span><br><span class="line">    version=<span class="string">&#x27;1.0&#x27;</span>,</span><br><span class="line">    author=...,</span><br><span class="line">    author_email=...,</span><br><span class="line">    description=<span class="string">&#x27;Hand-written Linear ops for PyTorch&#x27;</span>,</span><br><span class="line">    long_description=<span class="string">&#x27;Simple demo for writing Linear ops in CUDA extensions with PyTorch&#x27;</span>,</span><br><span class="line">    ext_modules=[</span><br><span class="line">        CUDAExtension(</span><br><span class="line">            name=<span class="string">&#x27;mylinearops_cuda&#x27;</span>,</span><br><span class="line">            sources=sources,</span><br><span class="line">            include_dirs=include_dirs,</span><br><span class="line">            extra_compile_args=&#123;<span class="string">&#x27;cxx&#x27;</span>: [<span class="string">&#x27;-O2&#x27;</span>],</span><br><span class="line">                                <span class="string">&#x27;nvcc&#x27;</span>: [<span class="string">&#x27;-O2&#x27;</span>]&#125;</span><br><span class="line">        )</span><br><span class="line">    ],</span><br><span class="line">    py_modules=[<span class="string">&#x27;mylinearops.mylinearops&#x27;</span>],</span><br><span class="line">    cmdclass=&#123;</span><br><span class="line">        <span class="string">&#x27;build_ext&#x27;</span>: BuildExtension</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>At the beginning, we first get the path information. We get the <code>include_dirs</code> (Where we store our <code>.h</code> headers), <code>sources</code> (Where we store our C++&#x2F;CUDA source code) directory. </p>
<p>Then, we call the <code>setup</code> function. The parameter explanation are as following:</p>
<ul>
<li><code>name</code>: The package name, how do users call this program</li>
<li><code>version</code>: The version number, decided by the creator</li>
<li><code>author</code>: The creator’s name</li>
<li><code>author_email</code>: The creator’s email</li>
<li><code>description</code>: The package’s description, short version</li>
<li><code>long_description</code>: The package’s description, long version</li>
<li><code>ext_modules</code>: <strong>Key</strong> in our building process. When we are building the PyTorch CUDA extension, we should use <code>CUDAExtension</code>, so that the build helper can know how to compile correctly<ul>
<li><code>name</code>: the CUDA extension name. We import this name in our wrapper to access the cuda functions</li>
<li><code>sources</code>: the source files</li>
<li><code>include_dirs</code>: the header files</li>
<li><code>extra_compile_args</code>: The extra compiling flags. <code>&#123;&#39;cxx&#39;: [&#39;-O2&#39;], nvcc&#39;: [&#39;-O2&#39;]&#125;</code> is commonly used, which means using <code>-O2</code> optimization level when compiling</li>
</ul>
</li>
<li><code>py_modules</code>: The Python modules needed for the package, which is our wrapper, <code>mylinearops</code>. In most cases, the wrapper module has the same name as the overall package name. (<code>&#39;mylinearops.mylinearops&#39;</code> stands for <code>&#39;mylinearops/mylinearops.py&#39;</code>)</li>
<li><code>cmdclass</code>: When building the PyTorch CUDA extension, we always pass in this: <code>&#123;&#39;build_ext&#39;: BuildExtension&#125;</code></li>
</ul>
<h2 id="Building"><a href="#Building" class="headerlink" title="Building"></a>Building</h2><p>Then, we can build the package. We first activate the conda environment where we want to install in:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">conda activate &lt;target_env&gt;</span><br></pre></td></tr></table></figure>

<p>Then run:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> &lt;proj_root&gt;</span><br><span class="line">python setup.py install</span><br></pre></td></tr></table></figure>

<p><strong>Note:</strong> Don’t run <code>pip install .</code>, otherwise your python module will not be successfully installed, at least in my case.</p>
<p>It may take some time to compile it. If the building process ends up with some error message, go and fix them. If it finally displays something as “successfully installed mylinearops”, then you are ready to go.</p>
<p>To check if the installation is successful, we can try to import it:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ python</span><br><span class="line">Python 3.9.15 (main, Nov 24 2022, 14:31:59) </span><br><span class="line">[GCC 11.2.0] :: Anaconda, Inc. on linux</span><br><span class="line">Type <span class="string">&quot;help&quot;</span>, <span class="string">&quot;copyright&quot;</span>, <span class="string">&quot;credits&quot;</span> or <span class="string">&quot;license&quot;</span> <span class="keyword">for</span> more information.</span><br><span class="line">&gt;&gt;&gt; import mylinearops</span><br><span class="line">&gt;&gt;&gt; <span class="built_in">dir</span>(mylinearops)</span><br><span class="line">[<span class="string">&#x27;LinearLayer&#x27;</span>, <span class="string">&#x27;__builtins__&#x27;</span>, <span class="string">&#x27;__cached__&#x27;</span>, <span class="string">&#x27;__doc__&#x27;</span>, <span class="string">&#x27;__file__&#x27;</span>, <span class="string">&#x27;__loader__&#x27;</span>, <span class="string">&#x27;__name__&#x27;</span>, <span class="string">&#x27;__package__&#x27;</span>, <span class="string">&#x27;__path__&#x27;</span>, <span class="string">&#x27;__spec__&#x27;</span>, <span class="string">&#x27;linearop&#x27;</span>, <span class="string">&#x27;matmul&#x27;</span>, <span class="string">&#x27;mylinearops&#x27;</span>]</span><br><span class="line">&gt;&gt;&gt; </span><br></pre></td></tr></table></figure>

<p>Further testing will be mentioned in the next subsection.</p>
<h1 id="Module-Testing"><a href="#Module-Testing" class="headerlink" title="Module Testing"></a>Module Testing</h1><p>We will test the forward and backward of <code>matmul</code> and <code>LinearLayer</code> calculations respectively. To verify the answer, we’ll compare our answer with the PyTorch’s implementation or with <code>torch.autograd.gradcheck</code>. To increase the accuracy, we recommend to use <code>double</code> (<code>torch.float64</code>) type instead of <code>float</code> (<code>torch.float32</code>).</p>
<p>For tensors: create with argument <code>dtype=torch.float64</code>.</p>
<p>For modules: a good way is to use <code>model.double()</code> to convert all the parameters and buffers to <code>double</code>. </p>
<h2 id="forward"><a href="#forward" class="headerlink" title="forward"></a>forward</h2><p>A typical method is to use <code>torch.allclose</code> to verify if two tensors are close to each other. We can create the reference answer by PyTorch’s implementation.</p>
<ul>
<li><code>matmul</code>:</li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> mylinearops</span><br><span class="line"></span><br><span class="line">A = torch.randn(<span class="number">20</span>, <span class="number">30</span>, dtype=torch.float64).cuda().requires_grad_()</span><br><span class="line">B = torch.randn(<span class="number">30</span>, <span class="number">40</span>, dtype=torch.float64).cuda().requires_grad_()</span><br><span class="line"></span><br><span class="line">res_my = mylinearops.matmul(A, B)</span><br><span class="line">res_torch = torch.matmul(A, B)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.allclose(res_my, res_torch))</span><br></pre></td></tr></table></figure>

<ul>
<li><code>LinearLayer</code>:</li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> mylinearops</span><br><span class="line"></span><br><span class="line">A = torch.randn(<span class="number">40</span>, <span class="number">30</span>, dtype=torch.float64).cuda().requires_grad_() * <span class="number">100</span></span><br><span class="line">linear = mylinearops.LinearLayer(<span class="number">30</span>, <span class="number">50</span>).cuda().double()</span><br><span class="line"></span><br><span class="line">res_my = linear(A)</span><br><span class="line">res_torch = torch.matmul(A, linear.weight) + linear.bias</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.allclose(res_my, res_torch))</span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">max</span>(torch.<span class="built_in">abs</span>(res_my - res_torch)))</span><br></pre></td></tr></table></figure>

<p>It is worthwhile that sometimes, because of the floating number error, the answer from PyTorch is not consistent with the answer from our implementations. We have three methods:</p>
<ol>
<li>Pass <code>atol=1e-5, rtol=1e-5</code> into the <code>torch.allclose</code> to increase the tolerance level.</li>
<li><strong>[Not very recommended]</strong> We can observe the absolute error by <code>torch.max(torch.abs(res_my - res_torch))</code> for reference. If the result is merely 0.01 ~ 0.1, That would be OK in most cases.</li>
</ol>
<h2 id="backward"><a href="#backward" class="headerlink" title="backward"></a>backward</h2><p>For backward calculation, we can use <code>torch.autograd.gradcheck</code> to verify the result. If some tensors are only <code>float</code>, an warning will occur:</p>
<blockquote>
<p>……&#x2F;torch&#x2F;autograd&#x2F;gradcheck.py:647: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. </p>
</blockquote>
<p>So it is recommended to use the <code>double</code> type. Otherwise the check will likely fail.</p>
<ul>
<li><code>matmul</code>:</li>
</ul>
<p>As mentioned above, for pure calculation functions, we can assign all tensor as <code>double</code> (<code>torch.float64</code>) type. We are ready to go: </p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> mylinearops</span><br><span class="line"></span><br><span class="line">A = torch.randn(<span class="number">20</span>, <span class="number">30</span>, dtype=torch.float64).cuda().requires_grad_()</span><br><span class="line">B = torch.randn(<span class="number">30</span>, <span class="number">40</span>, dtype=torch.float64).cuda().requires_grad_()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.autograd.gradcheck(mylinearops.matmul, (A, B)))    <span class="comment"># pass</span></span><br></pre></td></tr></table></figure>

<ul>
<li><code>LinearLayer</code>:</li>
</ul>
<p>As mentioned above, we can use <code>model.double()</code>. We are ready to go: </p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> mylinearops</span><br><span class="line"></span><br><span class="line"><span class="comment">## CHECK for Linear Layer with bias ##</span></span><br><span class="line">A = torch.randn(<span class="number">40</span>, <span class="number">30</span>, dtype=torch.float64).cuda().requires_grad_()</span><br><span class="line">linear = mylinearops.LinearLayer(<span class="number">30</span>, <span class="number">40</span>).cuda().double()</span><br><span class="line"><span class="built_in">print</span>(torch.autograd.gradcheck(linear, (A,)))    <span class="comment"># pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## CHECK for Linear Layer without bias ##</span></span><br><span class="line">A = torch.randn(<span class="number">40</span>, <span class="number">30</span>, dtype=torch.float64).cuda().requires_grad_()</span><br><span class="line">linear_nobias = mylinearops.LinearLayer(<span class="number">30</span>, <span class="number">40</span>, bias=<span class="literal">False</span>).cuda().double()</span><br><span class="line"><span class="built_in">print</span>(torch.autograd.gradcheck(linear_nobias, (A,)))    <span class="comment"># pass</span></span><br></pre></td></tr></table></figure>



<h1 id="Full-Example"><a href="#Full-Example" class="headerlink" title="Full Example"></a>Full Example</h1><p>Now, we use our linear module to build a three layer classic linear model <code>[784, 256, 10]</code>to classify the MNIST digits. See the <code>examples/main.py</code> file. </p>
<p>Just as the <code>nn.Linear</code>, we create the model by:</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MLP, self).__init__()</span><br><span class="line">        self.linear1 = mylinearops.LinearLayer(<span class="number">784</span>, <span class="number">256</span>, bias=<span class="literal">True</span>)<span class="comment">#.cuda()</span></span><br><span class="line">        self.linear2 = mylinearops.LinearLayer(<span class="number">256</span>, <span class="number">256</span>, bias=<span class="literal">True</span>)<span class="comment">#.cuda()</span></span><br><span class="line">        self.linear3 = mylinearops.LinearLayer(<span class="number">256</span>, <span class="number">10</span>, bias=<span class="literal">True</span>)<span class="comment">#.cuda()</span></span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        <span class="comment"># self.softmax = nn.Softmax(dim=1)</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">784</span>)</span><br><span class="line">        x = self.relu(self.linear1(x))</span><br><span class="line">        x = self.relu(self.linear2(x))</span><br><span class="line">        <span class="comment"># x = self.softmax(self.linear3(x))</span></span><br><span class="line">        x = self.linear3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<p>After writing some basic things, we can run our model: <code>python examples/tests.py</code>.</p>
<p>We also build the model by PyTorch’s <code>nn.Linear</code>. The result logging is:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># mylinearops</span></span><br><span class="line">...</span><br><span class="line">Epoch: [10/10], Step: [100/468], Loss: 0.0417, Acc: 0.9844</span><br><span class="line">Epoch: [10/10], Step: [200/468], Loss: 0.0971, Acc: 0.9609</span><br><span class="line">Epoch: [10/10], Step: [300/468], Loss: 0.0759, Acc: 0.9766</span><br><span class="line">Epoch: [10/10], Step: [400/468], Loss: 0.0777, Acc: 0.9766</span><br><span class="line">Time: 23.4661s</span><br><span class="line"></span><br><span class="line"><span class="comment"># torch</span></span><br><span class="line">...</span><br><span class="line">Epoch: [10/10], Step: [100/468], Loss: 0.1048, Acc: 0.9688</span><br><span class="line">Epoch: [10/10], Step: [200/468], Loss: 0.0412, Acc: 0.9844</span><br><span class="line">Epoch: [10/10], Step: [300/468], Loss: 0.0566, Acc: 0.9688</span><br><span class="line">Epoch: [10/10], Step: [400/468], Loss: 0.0217, Acc: 0.9922</span><br><span class="line">Time: 26.5896s</span><br></pre></td></tr></table></figure>

<p>It is surprising that our implementation is even faster than the torch’s one. (But relax, after trying for some repetitions, we find ours is just as fast as the torch’s one). This is because the data scale is relatively small, the computation proportion is small. When the data scale is larger, ours may be slower than torch’s. </p>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title>PyTorch Practical Hand-Written Modules Basics 8--Linear Layer Ops on C++/CUDA</title>
    <url>/2023/08/02/Pytorch-Practical-Basics-8/</url>
    <content><![CDATA[<p>In the section 6 to 9, we’ll investigate how to use <code>torch.autograd.Function</code> to implement the hand-written operators. The tentative outline is:</p>
<ul>
<li><input checked disabled type="checkbox"> In the section 6, we talk about the basics of <code>torch.autograd.Function</code>. <strong>The operators defined by <code>torch.autograd.Function</code> can be automatically back-propagated.</strong></li>
<li><input checked disabled type="checkbox"> In the last section (7), we’ll talk about <strong>mathematic derivation</strong> for the “linear layer” operator. </li>
<li><input checked disabled type="checkbox"> In this section (8), we talk about <strong>writing C++ CUDA extension</strong> for the “linear layer” operator. </li>
<li><input disabled type="checkbox"> In the section 9, we talk details about building the extension to a module, as well as testing. Then we’ll conclude the things we’ve done so far.</li>
</ul>
<p><strong>Note:</strong></p>
<ul>
<li>This blog is written with following reference: <ul>
<li>PyTorch official tutorial about CUDA extension: <a href="https://pytorch.org/tutorials/advanced/cpp_extension.html">website</a>.</li>
<li>YouTube video about writing CUDA extension: <a href="https://www.youtube.com/watch?v=l_Rpk6CRJYI">video</a>, <a href="https://github.com/kwea123/pytorch-cppcuda-tutorial">code</a>.</li>
</ul>
</li>
<li>For how to write CUDA code, you can follow <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">official documentation</a>, <a href="https://zhuanlan.zhihu.com/p/34587739">blogs</a> (In Chinese). You can search by yourself for English tutorials and video tutorials.</li>
<li>This blog only talk some important points in the matrix multiplication example. Code are picked by pieces for illustration. Whole code is at: <a href="https://github.com/I-am-Future/PyTorch-Linear-Operator-CUDA">code</a>.</li>
</ul>
<h1 id="Overall-Structure"><a href="#Overall-Structure" class="headerlink" title="Overall Structure"></a>Overall Structure</h1><p>The general structure for our PyTorch C++ &#x2F; CUDA extension looks like following:</p>
<img src="/2023/08/02/Pytorch-Practical-Basics-8/cudaops-struct.png" alt="cudaops-struct" style="zoom:50%;">

<p>We mainly have three kinds of file: Library interface, Core code on CPU, and Core code on GPU. Let’s explain them in detail:</p>
<ul>
<li><p>Library interface (.cpp)</p>
<ul>
<li>Contains Functions Interface for Python to call. These functions usually have Tensor input and Tensor return value.</li>
<li>Contains a standard pybind declaration, since our extension uses pybind to bind the C++ functions for Python. It indicates which functions are needed to be bound.</li>
</ul>
</li>
<li><p>Core code on CPU (.cpp)</p>
<ul>
<li>Contains core function to do the calculation. </li>
<li>Contains wrapper for the core function, serves to creating the result tensor, checking the input shape, etc.</li>
</ul>
</li>
<li><p>Core code on GPU (.cu)</p>
<ul>
<li>Contains CUDA kernel function <code>__global__</code> to do the parallel calculation. </li>
<li>Contains wrapper for the core function, serves to creating the result tensor, checking the input shape, setting the launch configs, launching the kernel, etc.</li>
</ul>
</li>
</ul>
<p>Then, after we finishing the code, we can use Python build tools to compile the code into a static object library (.so file). Then, we can import them normally in the Python side. We can call the functions we declared in library interface by pybind11.</p>
<p>In our example <a href="https://github.com/I-am-Future/PyTorch-Linear-Operator-CUDA">code</a>, we don’t provide code for CPU calculation. We only support GPU. So we only have two files (<code>src/linearops.cpp</code> and <code>src/addmul_kernel.cu</code>)</p>
<h1 id="Pybind-Interface"><a href="#Pybind-Interface" class="headerlink" title="Pybind Interface"></a>Pybind Interface</h1><p>This is the <code>src/linearops.cpp</code> file in our repo. </p>
<h2 id="1-Utils-function"><a href="#1-Utils-function" class="headerlink" title="1. Utils function"></a>1. Utils function</h2><p>We usually defines some utility macro functions in our code. They are in the <code>include/utils.h</code> header file. </p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// PyTorch CUDA Utils</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x <span class="string">&quot; must be a CUDA tensor&quot;</span>)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x <span class="string">&quot; must be contiguous&quot;</span>)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Kernel Config Utils</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> DIV_CEIL(a, b) (((a) + (b) - 1) / (b))</span></span><br></pre></td></tr></table></figure>

<p>The third macro will call first two macros, which are used to make sure the tenser is on the CUDA devices and is contiguous. </p>
<p>The last macro performs ceil division, which are often used in setting the CUDA kernel launch configurations. </p>
<h2 id="2-Interface-functions"><a href="#2-Interface-functions" class="headerlink" title="2. Interface functions"></a>2. Interface functions</h2><p>Benefited by pybind, we can simply define functions in C++ and use them in Python. A function looks like</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">torch::Tensor <span class="title">func</span><span class="params">(torch::Tensor a, torch::Tensor b, <span class="type">int</span> c)</span></span>&#123;</span><br><span class="line">    torch::Tensor res;</span><br><span class="line">    ......</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>is relatively same as the Python function below. </p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">func</span>(<span class="params">a: torch.Tensor, b: torch.Tensor, c: <span class="built_in">int</span></span>) -&gt; torch.Tensor</span><br><span class="line">	res = ... <span class="comment"># torch.Tensor</span></span><br><span class="line">    ......</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>

<p>Then, we can define our matrix multiplication interface as below. Note that we need to implement both the forward and backward functions!</p>
<ul>
<li>forward</li>
</ul>
<p>Check the input, input size, and then call the CUDA function wrapper.</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">torch::Tensor <span class="title">matmul_forward</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor &amp;A, </span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor &amp;B)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(A);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(B);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(A.<span class="built_in">size</span>(<span class="number">1</span>) == B.<span class="built_in">size</span>(<span class="number">0</span>), <span class="string">&quot;matmul_fast_forward: shape mismatch&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">matmul_cuda</span>(A, B);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>backward</li>
</ul>
<p>Also check the input, input size, and then call the CUDA function wrapper. Note that we calculate the backward of <code>A * B = C</code> for input matrix A, B in two different function. So that when someday we don’t need to calculate the gradient of A, we can just pass it. </p>
<p>The gradient function derivation is mentioned in last section <a href="https://i-am-future.github.io/2023/08/01/Pytorch-Practical-Basics-7/#Matrix-multiplication-backward">here</a>. </p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">/* Backward for A gradient */</span></span><br><span class="line"><span class="function">torch::Tensor <span class="title">matmul_dA_backward</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor &amp;grad_output, </span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor &amp;A, </span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor &amp;B)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(grad_output);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(B);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// dL/dB = dL/dY * B^T</span></span><br><span class="line">    <span class="keyword">auto</span> grad_A = <span class="built_in">matmul_cuda</span>(grad_output, <span class="built_in">transpose_cuda</span>(B));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grad_A;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Backward for B gradient */</span></span><br><span class="line"><span class="function">torch::Tensor <span class="title">matmul_dB_backward</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor &amp;grad_output, </span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor &amp;A, </span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::Tensor &amp;B)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(grad_output);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(A);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// dL/dB = A^T * dL/dY</span></span><br><span class="line">    <span class="keyword">auto</span> grad_B = <span class="built_in">matmul_cuda</span>(<span class="built_in">transpose_cuda</span>(A), grad_output);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grad_B;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="3-Binding"><a href="#3-Binding" class="headerlink" title="3. Binding"></a>3. Binding</h2><p>At the last of the <code>src/linearops.cpp</code>, we use the following code to bind the functions. The first string is the function name in Python side, the second is a function pointer to the function be called, and the last is the docstring for that function in Python side. </p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">PYBIND11_MODULE</span>(TORCH_EXTENSION_NAME, m) &#123;</span><br><span class="line">    ......</span><br><span class="line">    m.<span class="built_in">def</span>(<span class="string">&quot;matmul_forward&quot;</span>, &amp;matmul_forward, <span class="string">&quot;Matmul forward&quot;</span>);</span><br><span class="line">    m.<span class="built_in">def</span>(<span class="string">&quot;matmul_dA_backward&quot;</span>, &amp;matmul_dA_backward, <span class="string">&quot;Matmul dA backward&quot;</span>);</span><br><span class="line">    m.<span class="built_in">def</span>(<span class="string">&quot;matmul_dB_backward&quot;</span>, &amp;matmul_dB_backward, <span class="string">&quot;Matmul dB backward&quot;</span>);</span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h1 id="CUDA-wrapper"><a href="#CUDA-wrapper" class="headerlink" title="CUDA wrapper"></a>CUDA wrapper</h1><p>This is the <code>src/addmul_kernel.cu</code> file in our repo. </p>
<p>The wrapper for matrix multiplication looks like below: </p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">torch::Tensor <span class="title">matmul_cuda</span><span class="params">(torch::Tensor A, torch::Tensor B)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 1. Get metadata</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> m = A.<span class="built_in">size</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> n = A.<span class="built_in">size</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> p = B.<span class="built_in">size</span>(<span class="number">1</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 2. Create output tensor</span></span><br><span class="line">    <span class="keyword">auto</span> result = torch::<span class="built_in">empty</span>(&#123;m, p&#125;, A.<span class="built_in">options</span>());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. Set launch configuration</span></span><br><span class="line">    <span class="type">const</span> dim3 blockSize = <span class="built_in">dim3</span>(BLOCK_SIZE, BLOCK_SIZE);</span><br><span class="line">    <span class="type">const</span> dim3 gridSize = <span class="built_in">dim3</span>(<span class="built_in">DIV_CEIL</span>(m, BLOCK_SIZE), <span class="built_in">DIV_CEIL</span>(p, BLOCK_SIZE));</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// 4. Call the cuda kernel launcher</span></span><br><span class="line">    <span class="built_in">AT_DISPATCH_FLOATING_TYPES</span>(A.<span class="built_in">type</span>(), <span class="string">&quot;matmul_cuda&quot;</span>, </span><br><span class="line">    ([&amp;] &#123;</span><br><span class="line">        matmul_fw_kernel&lt;<span class="type">scalar_t</span>&gt;&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(</span><br><span class="line">            A.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">            B.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">            result.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">            m, p</span><br><span class="line">        );</span><br><span class="line">    &#125;));</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 5. Return the value</span></span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>And here, we’ll talk in details: </p>
<h2 id="1-Get-metadata"><a href="#1-Get-metadata" class="headerlink" title="1. Get metadata"></a>1. Get metadata</h2><p>Just as the tensor in PyTorch, we can use <code>Tensor.size(0)</code> to axis the shape size of dimension 0.</p>
<p>Note that we have checked the dimension match at the interface side, we don’t need to check it here.</p>
<h2 id="2-Create-output-tensor"><a href="#2-Create-output-tensor" class="headerlink" title="2. Create output tensor"></a>2. Create output tensor</h2><p>We can do operation in-place or create a new tensor for output. Use the following code to create a tensor shape <code>m x p</code>, with same dtype &#x2F; device as <code>A</code>. </p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> result = torch::<span class="built_in">empty</span>(&#123;m, p&#125;, A.<span class="built_in">options</span>());</span><br></pre></td></tr></table></figure>

<p>In other situations, when we want special dtype &#x2F; device, we can follow the declaration as below:</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">torch::<span class="built_in">empty</span>(&#123;m, p&#125;, torch::<span class="built_in">dtype</span>(torch::kInt32).<span class="built_in">device</span>(feats.<span class="built_in">device</span>()))</span><br></pre></td></tr></table></figure>

<p><code>torch.empty</code> only allocate the memory, but not initialize the entries to 0. Because sometimes, we’ll fill into the result tensors in the kernel functions, so it is not necessary to initialize as 0. </p>
<h2 id="3-Set-launch-configuration"><a href="#3-Set-launch-configuration" class="headerlink" title="3. Set launch configuration"></a>3. Set launch configuration</h2><p>You should know some basic CUDA knowledges before understand this part. Basically here, we are setting the launch configuration based on the input matrix size. We are using the macro functions defined before.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="type">const</span> dim3 blockSize = <span class="built_in">dim3</span>(BLOCK_SIZE, BLOCK_SIZE);</span><br><span class="line"><span class="type">const</span> dim3 gridSize = <span class="built_in">dim3</span>(<span class="built_in">DIV_CEIL</span>(m, BLOCK_SIZE), <span class="built_in">DIV_CEIL</span>(p, BLOCK_SIZE));</span><br></pre></td></tr></table></figure>

<p>We set each thread block size to <code>16 x 16</code>. Then, we set the number of blocks according to the input size. </p>
<h2 id="4-Call-the-cuda-kernel-launcher"><a href="#4-Call-the-cuda-kernel-launcher" class="headerlink" title="4. Call the cuda kernel launcher"></a>4. Call the cuda kernel launcher</h2><p>Unlike normal cuda programs, we use <code>ATen</code>‘s function to start the kernel. This is a standard operation, and you can copy-paste it to anywhere. </p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">AT_DISPATCH_FLOATING_TYPES</span>(A.<span class="built_in">type</span>(), <span class="string">&quot;matmul_cuda&quot;</span>, </span><br><span class="line">                           ([&amp;] &#123;</span><br><span class="line">                               matmul_fw_kernel&lt;<span class="type">scalar_t</span>&gt;&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(</span><br><span class="line">                                   A.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">                                   B.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">                                   result.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">                                   m, p</span><br><span class="line">                               );</span><br><span class="line">                           &#125;));</span><br></pre></td></tr></table></figure>

<ul>
<li><p>This function is named <code>AT_DISPATCH_FLOATING_TYPES</code>, meaning the inside kernel will support floating types, i.e., <code>float (32bit)</code> and <code>double (64bit)</code>.   For <code>float16</code>, you can use <code>AT_DISPATCH_ALL_TYPES_AND_HALF</code>. For int (<code>int (32bit)</code> and <code>long long (64 bit)</code> and more, use <code>AT_DISPATCH_INTEGRAL_TYPES</code>. </p>
</li>
<li><p>The first argument <code>A.type()</code>, indicates the actual chosen type in the runtime. </p>
</li>
<li><p>The second argument <code>matmul_cuda</code> can be used for error reporting.</p>
</li>
<li><p>The last argument, which is a lambda function, is the actual function to be called. Basically in this function, we start the kernel by the following statement:</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">matmul_fw_kernel&lt;<span class="type">scalar_t</span>&gt;&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(</span><br><span class="line">A.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">B.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">result.<span class="built_in">packed_accessor</span>&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt;(),</span><br><span class="line">m, p</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<ul>
<li><code>matmul_fw_kernel</code> is the kernel function name.</li>
<li><code>&lt;scalar_t&gt;</code> is the template parameter, will be replaced to all possible types in the outside <code>AT_DISPATCH_FLOATING_TYPES</code>.</li>
<li><code>&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;</code> passed in the launch configuration</li>
<li>In the parameter list, if that is a <code>Tensor</code>, we should pass in the packed accessor, which convenient indexing operation in the kernel.<ul>
<li><code>&lt;scalar_t&gt;</code> is the template parameter.</li>
<li><code>2</code> means the <code>Tensor.ndimension=2</code>.</li>
<li><code>torch::RestrictPtrTraits</code> means the pointer (tensor memory) would not not overlap. It enables some optimization. Usually not change.</li>
<li><code>size_t</code> indicates the index type. Usually not change.</li>
</ul>
</li>
<li>if the parameter is integer <code>m, p</code>, just pass it in as normal.</li>
</ul>
</li>
</ul>
<h2 id="5-Return-the-value"><a href="#5-Return-the-value" class="headerlink" title="5. Return the value"></a>5. Return the value</h2><p>If we have more then one return value, we can set the return type to <code>std::vector&lt;torch::Tensor&gt;</code>. Then we return with <code>&#123;xxx, yyy&#125;</code>. </p>
<h1 id="CUDA-kernel"><a href="#CUDA-kernel" class="headerlink" title="CUDA kernel"></a>CUDA kernel</h1><p>This is the <code>src/addmul_kernel.cu</code> file in our repo. </p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> <span class="type">scalar_t</span>&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">matmul_fw_kernel</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::PackedTensorAccessor&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt; A,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> torch::PackedTensorAccessor&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt; B,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::PackedTensorAccessor&lt;<span class="type">scalar_t</span>, <span class="number">2</span>, torch::RestrictPtrTraits, <span class="type">size_t</span>&gt; result,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> m, <span class="type">const</span> <span class="type">int</span> p</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> row = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> col = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (row &gt;= m || col &gt;= p) <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">scalar_t</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; A.<span class="built_in">size</span>(<span class="number">1</span>); i++) &#123;</span><br><span class="line">        sum += A[row][i] * B[i][col];</span><br><span class="line">    &#125;</span><br><span class="line">    result[row][col] = sum;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>We define it as a template function <code>template &lt;typename scalar_t&gt;</code>, so that our kernel function can support different type of input tensor. </li>
<li>Usually we’ll set the input <code>PackedTensorAccessor</code> with <code>const</code>, to avoid some unexpected modification on them. </li>
<li>The main code is just a simple CUDA matrix multiplication example. This is very common, you can search online for explanation.</li>
</ul>
<h2 id="Ending"><a href="#Ending" class="headerlink" title="Ending"></a>Ending</h2><p>That’s too much things in this section. In the next section, we’ll talk about how to write the <code>setup.py</code> to <strong>compile</strong> the code, letting it be a module for python. </p>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title>PyTorch Practical Hand-Written Modules Basics 7--Mathematics Derivation of Linear Layer</title>
    <url>/2023/08/01/Pytorch-Practical-Basics-7/</url>
    <content><![CDATA[<p>In the section 6 to 9, we’ll investigate how to use <code>torch.autograd.Function</code> to implement the hand-written operators. The tentative outline is:</p>
<ul>
<li><input checked disabled type="checkbox"> In the last section (6), we talk about the basics of <code>torch.autograd.Function</code>. <strong>The operators defined by <code>torch.autograd.Function</code> can be automatically back-propagated.</strong></li>
<li><input checked disabled type="checkbox"> In this section (7), we’ll talk about <strong>mathematic derivation</strong> for the “linear layer” operator. </li>
<li><input disabled type="checkbox"> In the section 8, we talk about writing C++ CUDA extension for the “linear layer” operator. </li>
<li><input disabled type="checkbox"> In the section 9, we talk details about building the extension to a module, as well as testing. Then we’ll conclude the things we’ve done so far.</li>
</ul>
<p>The linear layer is defined by <code>Y = XW + b</code>. There is a matrix multiplication operation, and a bias addition. <strong>We’ll talk about their forward&#x2F;backward derivation separately.</strong> </p>
<p>(I feel sorry that currently there is some problem with displaying mathematics formula here. I’ll use screenshot first.)</p>
<h1 id="Matrix-multiplication-forward"><a href="#Matrix-multiplication-forward" class="headerlink" title="Matrix multiplication: forward"></a>Matrix multiplication: forward</h1><p>The matrix multiplication operation is a common operator. Each entry in the result matrix is a vector dot product of two input matrixes. The <code>(i, j)</code> entry of the result is from multiplying first matrix’s row <code>i</code> vector and the second matrix’s column <code>j</code> vector. From this property, we know that number of columns in the first matrix should equal to number of rows in the second matrix. The shape should be: <code>[m, n] x [n, r] -&gt; [m, r]</code>. For more details, see the figure illustration below. </p>
<img src="/2023/08/01/Pytorch-Practical-Basics-7/matmul-forward.png" alt="matmul-forward" style="zoom: 67%;">



<h1 id="Matrix-multiplication-backward"><a href="#Matrix-multiplication-backward" class="headerlink" title="Matrix multiplication: backward"></a>Matrix multiplication: backward</h1><p>First, we should know what’s the <strong>goal</strong> of the backward propagation. In the upstream side, we would get the gradient of the answer matrix, C. (The gradient matrix has the same size as its corresponding matrix. i.e., if C is in shape <code>[m, r]</code>, then gradient of C is shape <code>[m, r]</code> as well.) <strong>In this step, we should get the gradient of matrix A and B.</strong> <u>Gradient of matrix A and B are functions in terms of matrix A and B and gradient of C.</u> Specially, by chain rule, we can formulate it as</p>
<img src="/2023/08/01/Pytorch-Practical-Basics-7/matmul-backward-math1.png" alt="matmul-backward-math1" style="zoom:50%;">

<!-- $$
\frac{\partial L}{\partial A} = \frac{\partial L}{\partial C} \cdot \frac{\partial C}{\partial A} \\
(\frac{\partial L}{\partial C} \text{ is the gradient of C, and we need to figure out } \frac{\partial C}{\partial A}.)
$$-->

<p>To figure out the gradient of A, we should first investigate how an entry <code>A[i, j]</code> contribute to the entries in the result matrix C. See the figure below:</p>
<img src="/2023/08/01/Pytorch-Practical-Basics-7/matmul-backward.png" alt="matmul-backward" style="zoom:67%;">

<p>As shown above, entry <code>A[i, j]</code> multiplies with entries in row <code>j</code> of matrix B, contributing to the entries in row <code>i</code> of matrix C. We can write the gradient down in mathematics formula below:</p>
<img src="/2023/08/01/Pytorch-Practical-Basics-7/matmul-backward-math2.png" alt="matmul-backward-math2" style="zoom:50%;">

<!--$$
\begin{align}
\frac{\partial L}{\partial A_{i, j}} &= \sum_{k=1}^r \frac{\partial L}{\partial C_{i, k}} \cdot \frac{\partial C_{i, k}}{\partial A_{i, j}} \\
&= \sum_{k=1}^r \frac{\partial L}{\partial C_{i, k}} \cdot B_{j, k}\\
&= \sum_{k=1}^r  \frac{\partial L}{\partial C}_{i, k}  \cdot  B^T_{k,j} \\
& = (\frac{\partial L}{\partial C})_{\text{row i}}  \cdot  (B^T)_{\text{col j}}\\
\end{align}
$$-->

<p>The result above is the gradient for one entry <code>A[i, j]</code>, and it’s a vector dot product between a matrix’s row <code>i</code> and another matrix’s column <code>j</code>. Observing this formula, we can naturally extend it to the gradient of the whole matrix A, and that will be a matrix product. </p>
<img src="/2023/08/01/Pytorch-Practical-Basics-7/matmul-backward-math3.png" alt="matmul-backward-math3" style="zoom:50%;">

<!--$$
\frac{\partial L}{\partial A} = \frac{\partial L}{\partial C} \cdot B^T \\
\text{At last, by the same principle, we know } \frac{\partial L}{\partial B} = A^T \cdot \frac{\partial L}{\partial C}.
$$-->

<p>Recall “Gradient of matrix A and B are functions in terms of matrix A and B and gradient of C” we said before. Our derivation indeed show that, uh?</p>
<h1 id="Add-bias-forward"><a href="#Add-bias-forward" class="headerlink" title="Add bias: forward"></a>Add bias: forward</h1><p>First, we should note that when doing the addition, we’re actually adding the <code>XW</code> matrix (shape <code>[n, r]</code>) with the bias vector (shape <code>[r]</code>). Indeed we have a <strong>broadcasting</strong> here. <strong>We add bias to each row of the <code>XW</code> matrix.</strong></p>
<img src="/2023/08/01/Pytorch-Practical-Basics-7/addbias-forward.png" alt="addbias-forward.drawio" style="zoom:67%;">

<h1 id="Add-bias-backward"><a href="#Add-bias-backward" class="headerlink" title="Add bias: backward"></a>Add bias: backward</h1><p>With the similar principle, we can get the gradient for the bias as well. </p>
<img src="/2023/08/01/Pytorch-Practical-Basics-7/addbias-backward.jpg" alt="addbias-backward" style="zoom:67%;">

<p>For each entry in vector <code>b</code>, the gradient is:</p>
<img src="/2023/08/01/Pytorch-Practical-Basics-7/addbias-backward-math1.png" alt="addbias-backward-math1" style="zoom:50%;">

<!--$$
\begin{align}
\frac{\partial L}{\partial b_{i}} &= \sum_{k=1}^m \frac{\partial L}{\partial C_{k, i}} \cdot \frac{\partial C_{k, i}}{\partial b_i} \\
&= \sum_{k=1}^m \frac{\partial L}{\partial C_{k, i}} \cdot 1 \\
&= \sum_{k=1}^m \frac{\partial L}{\partial C_{k, i}} \\
\end{align}
$$-->
<p>That is, the gradient of entry <code>b_i</code> is the summation of the i-th column. In total, the gradient will be the summation along each column (i.e., axis&#x3D;0). In programming, we write:</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">grad_b = torch.<span class="built_in">sum</span>(grad_C, axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>



<h1 id="PyTorch-Verification"><a href="#PyTorch-Verification" class="headerlink" title="PyTorch Verification"></a>PyTorch Verification</h1><p>Finally, we can write a PyTorch program to verify if our derivation is correct: we will compare our calculated gradients with the gradients calculated by the PyTorch. If they are same, our derivation would be correct. </p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">A = torch.randn(<span class="number">10</span>, <span class="number">20</span>).requires_grad_()</span><br><span class="line">B = torch.randn(<span class="number">20</span>, <span class="number">30</span>).requires_grad_()</span><br><span class="line"></span><br><span class="line">res = torch.mm(A, B)</span><br><span class="line">res.retain_grad()</span><br><span class="line">res.<span class="built_in">sum</span>().backward()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.allclose(A.grad, torch.mm(res.grad, B.t()))) <span class="comment"># grad_A = grad_res * B^T</span></span><br><span class="line"><span class="built_in">print</span>(torch.allclose(B.grad, torch.mm(A.t(), res.grad))) <span class="comment"># grad_B = A^T * grad_res</span></span><br></pre></td></tr></table></figure>

<p>Finally, the output is:</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>

<p>Which means that our derivation is correct. </p>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title>PyTorch Practical Hand-Written Modules Basics 6--torch.autograd.Function</title>
    <url>/2023/07/30/Pytorch-Practical-Basics-6/</url>
    <content><![CDATA[<p>In this section (and also three sections in the future), we investigate how to use <code>torch.autograd.Function</code> to implement the hand-written operators. The tentative outline is:</p>
<ul>
<li><input checked disabled type="checkbox"> This section (6), we talk about the basics of <code>torch.autograd.Function</code>.</li>
<li><input disabled type="checkbox"> In the next section (7), we’ll talk about mathematic derivation for the “linear layer” operator. </li>
<li><input disabled type="checkbox"> In the section 8, we talk about writing C++ CUDA extension for the “linear layer” operator. </li>
<li><input disabled type="checkbox"> In the section 9, we talk details about building the extension to a module, as well as testing. Then we’ll conclude the things we’ve done so far.</li>
</ul>
<h1 id="Backgrounds"><a href="#Backgrounds" class="headerlink" title="Backgrounds"></a>Backgrounds</h1><p>This article mainly takes reference of the <a href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd">Official tutorial</a> and summarizes, explains the important points. </p>
<p>By defining an operator with <code>torch.autograd.Function</code> and implement its forward &#x2F; backward function, we can use this operator with other PyTorch built-in operators together. <strong>The operators defined by <code>torch.autograd.Function</code> can be automatically back-propagated.</strong></p>
<p>As mentioned in the tutorial, we should use the <code>torch.autograd.Function</code> in the following scenes:</p>
<ul>
<li>The computation is from other libraries, so they don’t support differential natively. We should explicitly define its backward functions. </li>
<li>The PyTorch’s implementation of an operator cannot take benefits from the parallelization. We utilize the PyTorch C++&#x2F;CUDA extension for the better performance.</li>
</ul>
<h1 id="Basic-Structure"><a href="#Basic-Structure" class="headerlink" title="Basic Structure"></a>Basic Structure</h1><p>The following is the basic structure of the Function:</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Function</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearFunction</span>(<span class="title class_ inherited__">Function</span>):</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, input0, input1, ... , inputN</span>):</span><br><span class="line">        <span class="comment"># Save the input for the backward use. </span></span><br><span class="line">        ctx.save_for_backward(input1, input1, ... , inputN)</span><br><span class="line">        <span class="comment"># Calculate the output0, ... outputM given the inputs.</span></span><br><span class="line">        ......</span><br><span class="line">        <span class="keyword">return</span> output0, ... , outputM</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, grad_output0, ... , grad_outputM</span>):</span><br><span class="line">        <span class="comment"># Get and unpack the input for the backward use. </span></span><br><span class="line">        input0, input1, ... , inputN = ctx.saved_tensors</span><br><span class="line">        </span><br><span class="line">        grad_input0 = grad_input1 = grad_inputN = <span class="literal">None</span></span><br><span class="line">        <span class="comment"># These needs_input_grad records whether each input need to calculate the gradient. This can improve the efficiency.</span></span><br><span class="line">        <span class="keyword">if</span> ctx.needs_input_grad[<span class="number">0</span>]:</span><br><span class="line">            grad_input0 = ...  <span class="comment"># backward calculation</span></span><br><span class="line">        <span class="keyword">if</span> ctx.needs_input_grad[<span class="number">1</span>]:</span><br><span class="line">            grad_input1 = ...  <span class="comment"># backward calculation</span></span><br><span class="line">        ......</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> grad_input0, grad_input1, grad_inputN</span><br></pre></td></tr></table></figure>

<ol>
<li><p>The <code>forward</code> and <code>backward</code> functions are <code>staticmethod</code>. The forward function is <code>o0, ..., oM = forward(i0, ..., iN)</code>, calculate the output0 ~ outputM by the input0 ~ inputN. Then the backward function is <code>g_i0, ..., g_iN = backward(g_o0, ..., g_M)</code>, calculate the gradient of input0 ~ gradient of inputM by the gradient of output0 ~ outputN.</p>
</li>
<li><p>Since forward and backward are merely functions. We need store the input tensors to the <code>ctx</code> in the forward pass, so that we can get them in the backward functions. See <a href="https://pytorch.org/docs/stable/notes/extending.html#combining-forward-context">here</a> to use the alternative way to define <a href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function"><code>Function</code></a>.</p>
</li>
<li><p><code>ctx.needs_input_grad</code> is a tuple of Booleans. It records whether one input needs to calculate the gradient or not. Therefore, we can save computation resources if one tensor doesn’t need gradients. In that case, the return value of backward function for that tensor is <code>None</code>.</p>
</li>
</ol>
<h1 id="Use-it"><a href="#Use-it" class="headerlink" title="Use it"></a>Use it</h1><h2 id="Pure-functions"><a href="#Pure-functions" class="headerlink" title="Pure functions"></a>Pure functions</h2><p>After defining the class, we can use the <code>.apply</code> method to use it. Simply</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Option 1: alias</span></span><br><span class="line">linear = LinearFunction.apply</span><br></pre></td></tr></table></figure>
<p>or, </p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Option 2: wrap in a function, to support default args and keyword args.</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linear</span>(<span class="params"><span class="built_in">input</span>, weight, bias=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">return</span> LinearFunction.apply(<span class="built_in">input</span>, weight, bias)</span><br></pre></td></tr></table></figure>

<p>Then call as</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">output = linear(<span class="built_in">input</span>, weight, bias) <span class="comment"># input, weight, bias are all tensors!</span></span><br></pre></td></tr></table></figure>

<h2 id="nn-Module"><a href="#nn-Module" class="headerlink" title="nn.Module"></a>nn.Module</h2><p>In most cases, the weight and bias are parameters that are trainable during the process. We can further wrap this linear function to a Linear module: </p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Linear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_features, output_features, bias=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.input_features = input_features</span><br><span class="line">        self.output_features = output_features</span><br><span class="line"></span><br><span class="line">        <span class="comment"># nn.Parameters require gradients by default.</span></span><br><span class="line">        self.weight = nn.Parameter(torch.empty(output_features, input_features))</span><br><span class="line">        <span class="keyword">if</span> bias:</span><br><span class="line">            self.bias = nn.Parameter(torch.empty(output_features))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># You should always register all possible parameters, but the</span></span><br><span class="line">            <span class="comment"># optional ones can be None if you want.</span></span><br><span class="line">            self.register_parameter(<span class="string">&#x27;bias&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Not a very smart way to initialize weights</span></span><br><span class="line">        nn.init.uniform_(self.weight, -<span class="number">0.1</span>, <span class="number">0.1</span>)</span><br><span class="line">        <span class="keyword">if</span> self.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            nn.init.uniform_(self.bias, -<span class="number">0.1</span>, <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="comment"># See the autograd section for explanation of what happens here.</span></span><br><span class="line">        <span class="keyword">return</span> LinearFunction.apply(<span class="built_in">input</span>, self.weight, self.bias)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">extra_repr</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># (Optional)Set the extra information about this module. You can test</span></span><br><span class="line">        <span class="comment"># it by printing an object of this class.</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;input_features=&#123;&#125;, output_features=&#123;&#125;, bias=&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">            self.input_features, self.output_features, self.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">        )</span><br></pre></td></tr></table></figure>

<p>As mentioned in section 3, 4 of this series, the weight and bias should be <code>nn.Parameter</code> so that they can be recognized correctly. Then we initialize the weights with random variables. </p>
<p>In the <code>forward</code> functions, we use the defined <code>LinearFunction.apply</code> functions. The backward process will be automatically done just as other PyTorch modules. </p>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title>An Obscure RuntimeError for CUDA error: out of memory</title>
    <url>/2023/07/26/An-Obscure-RuntimeError-for-CUDA-error-out-of-memory/</url>
    <content><![CDATA[<h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h2><p>Today when I was running PyTorch scripts, I met a strange problem: </p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">a = torch.rand(<span class="number">2</span>, <span class="number">2</span>).to(<span class="string">&#x27;cuda:1&#x27;</span>)</span><br><span class="line">......</span><br><span class="line">torch.cuda.synchronize()</span><br></pre></td></tr></table></figure>

<p>but result in the following error:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">  File <span class="string">&quot;....../test.py&quot;</span>, line 67, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    torch.cuda.synchronize()</span><br><span class="line">  File <span class="string">&quot;....../miniconda3/envs/py39/lib/python3.9/site-packages/torch/cuda/__init__.py&quot;</span>, line 495, <span class="keyword">in</span> synchronize</span><br><span class="line">    <span class="built_in">return</span> torch._C._cuda_synchronize()</span><br><span class="line">RuntimeError: CUDA error: out of memory</span><br></pre></td></tr></table></figure>

<p>but It’s clear that GPU1 has enough memory (we only need to allocate 16 bytes!):</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">|===============================+======================+======================|</span><br><span class="line">|   0  NVIDIA GeForce ...  Off  | 00000000:1A:00.0 Off |                  N/A |</span><br><span class="line">| 75%   73C    P2   303W / 350W |  24222MiB / 24268MiB |     64%      Default |</span><br><span class="line">|                               |                      |                  N/A |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line">|   1  NVIDIA GeForce ...  Off  | 00000000:1B:00.0 Off |                  N/A |</span><br><span class="line">| 90%   80C    P2   328W / 350W |  15838MiB / 24268MiB |     92%      Default |</span><br><span class="line">|                               |                      |                  N/A |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br></pre></td></tr></table></figure>

<p>And normally, when we fail to allocate the memory for tensors, the error is:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 6.00 GiB total capacity; 4.54 GiB already allocated; 14.94 MiB free; 4.64 GiB reserved <span class="keyword">in</span> total by PyTorch)</span><br></pre></td></tr></table></figure>

<p>But our error message is much “simpler”. So what happened?</p>
<h2 id="Possible-Answer"><a href="#Possible-Answer" class="headerlink" title="Possible Answer"></a>Possible Answer</h2><p>This confused me for some time. According to this <a href="https://discuss.pytorch.org/t/pytorch-cuda-synchronize-out-of-memory/9502">website</a>:</p>
<blockquote>
<p>When you initially do a CUDA call, it’ll create a cuda context and a THC context on the primary GPU (GPU0), and for that i think it needs 200 MB or so. That’s right at the edge of how much memory you have left. </p>
</blockquote>
<p>Surprisingly, in my case, GPU0 has occupied <code>24222MiB / 24268MiB</code> memory. So there is no more memory for the context. In addition, this makes sense that out error message is <code>RuntimeError: CUDA error: out of memory</code>, not the message that tensallocation failed. </p>
<h2 id="Possible-Solution"><a href="#Possible-Solution" class="headerlink" title="Possible Solution"></a>Possible Solution</h2><p>Set the <code>CUDA_VISIBLE_DEVICES</code> environment variable. We need to change primary GPU (GPU0) to other one.</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Do this before `import torch`</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;CUDA_VISIBLE_DEVICES&#x27;</span>] = <span class="string">&#x27;1&#x27;</span> <span class="comment"># set to what you like, e.g., &#x27;1,2,3,4,5,6,7&#x27;</span></span><br></pre></td></tr></table></figure>

<p>And then, our program is ready to go. </p>
]]></content>
      <categories>
        <category>Problem Solving</category>
      </categories>
  </entry>
  <entry>
    <title>PyTorch Practical Hand-Written Modules Basics 5--Implement a ResNet</title>
    <url>/2023/07/09/Pytorch-Practical-Basics-5/</url>
    <content><![CDATA[<p>In this section, we’ll utilize knowledge we learnt from the last section (<a href>see here</a>), to implement a ResNet Network (<a href>paper</a>). </p>
<p>Note that we follow the original paper’s work. Our implementation is a simper version of the official <code>torchvision</code> implementation. (That is, we only implement the key structure, and the random weight init. We don’t consider dilation or other things). </p>
<h2 id="Preliminaries-Calculate-the-feature-map-size"><a href="#Preliminaries-Calculate-the-feature-map-size" class="headerlink" title="Preliminaries: Calculate the feature map size"></a>Preliminaries: Calculate the feature map size</h2><ul>
<li>Basic formula</li>
</ul>
<p>Given a convolution kernel with size <em>K</em>, and the padding <em>P</em>, the stride <em>S</em>, feature map size <em>I</em>, we can calculate the output size as <em>O</em> &#x3D; ( <em>I</em> - <em>K</em> + 2<em>P</em> ) &#x2F; <em>S</em> + 1. </p>
<ul>
<li>Corollary</li>
</ul>
<p>Based on the formula above, we know that when <em>S</em>&#x3D;1: </p>
<ol>
<li><em>K</em>&#x3D;3, <em>P</em>&#x3D;1 makes the input size and output size same.</li>
<li><em>K</em>&#x3D;1, <em>P</em>&#x3D;0 makes the input size and output size same.</li>
</ol>
<h2 id="Overall-Structure"><a href="#Overall-Structure" class="headerlink" title="Overall Structure"></a>Overall Structure</h2><p>The Table 1 in the original paper illustrates the overall structure of the ResNet:</p>
<img src="/2023/07/09/Pytorch-Practical-Basics-5/resnet_table1.png" alt="resnet_table1" style="zoom: 67%;">

<p>We know that from <code>conv2</code>, each layer consists of many blocks. And the blocks in <code>18, 34 layers</code> is different from blocks in <code>50, 101, 152 layers</code>. </p>
<p>We have several deductions: </p>
<ol>
<li>When the feature map enters the next layer, the first block need to do a down sampling operation. This is done by setting the one of the convolution kernel’s <code>stride=2</code>. </li>
<li>At other convolution kernels, the feature map’s size is same. So the convolution settings is same as the one referred in Preliminaries.</li>
</ol>
<h2 id="Basic-Block-Implementation"><a href="#Basic-Block-Implementation" class="headerlink" title="Basic Block Implementation"></a>Basic Block Implementation</h2><p>The basic block’s structure looks like this: </p>
<img src="/2023/07/09/Pytorch-Practical-Basics-5/basic.png" alt="basic">

<p>Please see the code below. Here, apart from <code>channels</code> defining the channels in the block, we have three additional parameters, <code>in_channels</code>, <code>stride</code>, and <code>downsample</code> to make this block versatile in the FIRST block in each layer. </p>
<p>According to the ResNet structure, for example, the first block in layer3 has the input <code>64*56*56</code>. The first block in layer3 has two tasks: </p>
<ol>
<li>Make the feature map size to <code>28*28</code>. Thus we need to set its stride to <code>2</code>. </li>
<li>Make the number of channels from <code>64</code> to <code>128</code>. Thus the <code>in_channel</code> should be <code>64</code>. </li>
<li>In addition, since the input is <code>64*56*56</code>, while the output is <code>128*28*28</code>, we need a down sample convolution to match the shortcut input to the output size.</li>
</ol>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualBasicBlock</span>(nn.Module):</span><br><span class="line">    expansion: <span class="built_in">int</span> = <span class="number">1</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels: <span class="built_in">int</span>, channels: <span class="built_in">int</span>, stride: <span class="built_in">int</span> = <span class="number">1</span>, downsample: nn.Module = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels, channels, <span class="number">3</span>, stride, <span class="number">1</span>)</span><br><span class="line">        self.batchnorm1 = nn.BatchNorm2d(channels)</span><br><span class="line">        self.relu1 = nn.ReLU()</span><br><span class="line">        self.conv2 = nn.Conv2d(channels, channels, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.batchnorm2 = nn.BatchNorm2d(channels)</span><br><span class="line">        self.relu2 = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        residual = x</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.batchnorm1(x)</span><br><span class="line">        x = self.relu1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.batchnorm2(x)</span><br><span class="line">        <span class="keyword">if</span> self.downsample:</span><br><span class="line">            residual = self.downsample(residual)</span><br><span class="line">        x += residual</span><br><span class="line">        x = self.relu2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>



<h2 id="Bottleneck-Block-Implementation"><a href="#Bottleneck-Block-Implementation" class="headerlink" title="Bottleneck Block Implementation"></a>Bottleneck Block Implementation</h2><p>The bottleneck block’s structure looks like this: </p>
<img src="/2023/07/09/Pytorch-Practical-Basics-5/bottleneck.png" alt="bottleneck">

<p>To reduce the computation cost, the Bottleneck block use <code>1x1</code> kernel to map the high number of channels (e.g., 256) to a low one (e.g., 64), and do the <code>3x3</code> convolution. Then, it maps the <code>64</code> channels to <code>256</code> again. </p>
<p>Please see the code below. Same as the basic block, We have three additional parameters, <code>in_channels</code>, <code>stride</code>, and <code>downsample</code> to make this block versatile in the FIRST block in each layer. The reasons are same as above. </p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualBottleNeck</span>(nn.Module):</span><br><span class="line">    expansion: <span class="built_in">int</span> = <span class="number">4</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels: <span class="built_in">int</span>, channels: <span class="built_in">int</span>, stride: <span class="built_in">int</span> = <span class="number">1</span>, downsample: nn.Module = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels, channels, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.batchnorm1 = nn.BatchNorm2d(channels)</span><br><span class="line">        self.relu1 = nn.ReLU()</span><br><span class="line">        self.conv2 = nn.Conv2d(channels, channels, <span class="number">3</span>, stride, <span class="number">1</span>)</span><br><span class="line">        self.batchnorm2 = nn.BatchNorm2d(channels)</span><br><span class="line">        self.relu2 = nn.ReLU()</span><br><span class="line">        self.conv3 = nn.Conv2d(channels, channels*<span class="number">4</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.batchnorm3 = nn.BatchNorm2d(channels*<span class="number">4</span>)</span><br><span class="line">        self.relu3 = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        residual = x</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.batchnorm1(x)</span><br><span class="line">        x = self.relu1(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.batchnorm2(x)</span><br><span class="line">        x = self.relu2(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        x = self.batchnorm3(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.downsample:</span><br><span class="line">            residual = self.downsample(residual)</span><br><span class="line"></span><br><span class="line">        x += residual</span><br><span class="line">        x = self.relu3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>



<h2 id="ResNet-Base-Implementation"><a href="#ResNet-Base-Implementation" class="headerlink" title="ResNet Base Implementation"></a>ResNet Base Implementation</h2><p>Then we can put thing together to form the ResNet model! The whole structure is straight-forward. We define the submodules one by one, and implement the <code>forward()</code> function. </p>
<p>There is only two tricky point: </p>
<ol>
<li>To support the <code>ResNetBase</code> for two different base blocks, the base block can be passed to this initializer. Since two base blocks have slightly differences in setting the channels, <code>ResidualBasicBlock</code> and <code>ResidualBottleNeck</code> have an attribute called <code>expansion</code>, which convenient the procedure in setting the correct number of channels and outputs. </li>
<li>See the <code>_make_layer</code> function below. It need to determine whether we need to do the down sample. And the condition and explanation is described below.</li>
</ol>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResNetBase</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, block, layer_blocks: <span class="built_in">list</span>, input_channels=<span class="number">3</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.block = block</span><br><span class="line">        <span class="comment"># conv1: 7x7</span></span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(input_channels, <span class="number">64</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>), </span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># max pool</span></span><br><span class="line">        self.maxpool = nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># conv2 ~ conv5_x</span></span><br><span class="line">        self.in_channels = <span class="number">64</span></span><br><span class="line">        self.conv2 = self._make_layer(<span class="number">64</span>, layer_blocks[<span class="number">0</span>])</span><br><span class="line">        self.conv3 = self._make_layer(<span class="number">128</span>, layer_blocks[<span class="number">1</span>], <span class="number">2</span>)</span><br><span class="line">        self.conv4 = self._make_layer(<span class="number">256</span>, layer_blocks[<span class="number">2</span>], <span class="number">2</span>)</span><br><span class="line">        self.conv5 = self._make_layer(<span class="number">512</span>, layer_blocks[<span class="number">3</span>], <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.downsample = nn.AvgPool2d(<span class="number">7</span>)</span><br><span class="line">        output_numel = <span class="number">512</span> * self.block.expansion</span><br><span class="line">        self.fc = nn.Linear(output_numel, <span class="number">1000</span>)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># init the weights</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(m.weight, mode=<span class="string">&quot;fan_out&quot;</span>, nonlinearity=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">                nn.init.normal_(m.weight, <span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">                nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, (nn.BatchNorm2d, nn.GroupNorm)):</span><br><span class="line">                nn.init.constant_(m.weight, <span class="number">1</span>)</span><br><span class="line">                nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_make_layer</span>(<span class="params">self, channel, replicates, stride=<span class="number">1</span></span>):</span><br><span class="line">        modules = []</span><br><span class="line"></span><br><span class="line">        downsample = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> self.in_channels != channel*self.block.expansion:</span><br><span class="line">            <span class="comment"># Use downsample to match the dimension in two cases: </span></span><br><span class="line">            <span class="comment"># 1. stride != 1, meaning we should downsample H, W in this layer. </span></span><br><span class="line">            <span class="comment">#   Then we need to match the residual&#x27;s H, W and the output&#x27;s H, W of this layer.</span></span><br><span class="line">            <span class="comment"># 2. self.in_channels != channel*block.expansion, meaning we should increase C in this layer.</span></span><br><span class="line">            <span class="comment">#   Then we need to match the residual&#x27;s C and the output&#x27;s C of this layer.</span></span><br><span class="line"></span><br><span class="line">            downsample = nn.Sequential(</span><br><span class="line">                nn.Conv2d(self.in_channels, channel*self.block.expansion, <span class="number">1</span>, stride),</span><br><span class="line">                nn.BatchNorm2d(channel*self.block.expansion)</span><br><span class="line">            )</span><br><span class="line">        </span><br><span class="line">        modules.append(self.block(self.in_channels, channel, stride, downsample))</span><br><span class="line"></span><br><span class="line">        self.in_channels = channel * self.block.expansion</span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, replicates):</span><br><span class="line">            modules.append(self.block(self.in_channels, channel))</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*modules)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x: shape Bx3x224x224</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.maxpool(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        x = self.conv4(x)</span><br><span class="line">        x = self.conv5(x)</span><br><span class="line"></span><br><span class="line">        x = self.downsample(x)</span><br><span class="line">        x = torch.flatten(x, start_dim=<span class="number">1</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>



<h2 id="Encapsulate-the-Constructors"><a href="#Encapsulate-the-Constructors" class="headerlink" title="Encapsulate the Constructors"></a>Encapsulate the Constructors</h2><p>Finally, we can encapsulate the constructors by functions:</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">my_resnet18</span>(<span class="params">in_channels=<span class="number">3</span></span>):</span><br><span class="line">    <span class="keyword">return</span> ResNetBase(ResidualBasicBlock, [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>], in_channels)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">my_resnet34</span>(<span class="params">in_channels=<span class="number">3</span></span>):</span><br><span class="line">    <span class="keyword">return</span> ResNetBase(ResidualBasicBlock, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>], in_channels)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">my_resnet50</span>(<span class="params">in_channels=<span class="number">3</span></span>):</span><br><span class="line">    <span class="keyword">return</span> ResNetBase(ResidualBottleNeck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>], in_channels)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">my_resnet101</span>(<span class="params">in_channels=<span class="number">3</span></span>):</span><br><span class="line">    <span class="keyword">return</span> ResNetBase(ResidualBottleNeck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">3</span>], in_channels)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">my_resnet152</span>(<span class="params">in_channels=<span class="number">3</span></span>):</span><br><span class="line">    <span class="keyword">return</span> ResNetBase(ResidualBottleNeck, [<span class="number">3</span>, <span class="number">8</span>, <span class="number">36</span>, <span class="number">3</span>], in_channels)</span><br></pre></td></tr></table></figure>

<p>Then, we can use it as normal models:</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">img = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line">model_my = my_resnet50()</span><br><span class="line">res_my = model_my(img)</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title>PyTorch Practical Hand-Written Modules Basics 4--Hand-written modules basics</title>
    <url>/2023/06/18/Pytorch-Practical-Basics-4/</url>
    <content><![CDATA[<p>After three articles talking about tensors, in this article, we will talk about something to the PyTorch Hand Written Modules Basics. You can see the outline on the left sidebar.</p>
<h2 id="Basic-structure"><a href="#Basic-structure" class="headerlink" title="Basic structure"></a>Basic structure</h2><p>The model must inherit the <code>nn.Module</code> class. Basically, according to the <a href="https://pytorch.org/tutorials/beginner/nn_tutorial.html#closing-thoughts">official tutorial</a>, <code>nn.Module</code> “creates a callable which behaves like a function, but can also contain state(such as neural net layer weights).”</p>
<p>The following is an example from the <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">docs</a>:</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">20</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        <span class="keyword">return</span> F.relu(self.conv2(x))</span><br></pre></td></tr></table></figure>

<p><strong>Some details</strong></p>
<ul>
<li>First, our model has Name <code>Model</code>, and inherits the <code>nn.Module</code> class.</li>
<li><code>super().__init__()</code> must be called at the first line of the <code>__init__</code> function.</li>
<li>The <code>Model</code> contains two submodules as attributes, <code>conv1</code> and <code>conv2</code>. They’re <code>nn.Conv2d</code> (The PyTorch implementation for 2-D convolution)</li>
<li>The <code>forward()</code> function do the forward-propagation of the model. It receives a tensor <code>x</code> and do two convolution-with-relu operation. And then return the result. </li>
<li>As for the backward-propagation, that step is calculated automatically by the powerful PyTorch’s auto-gradient technique. You don’t need to care about that.</li>
</ul>
<h2 id="load-store-the-model-state-dict"><a href="#load-store-the-model-state-dict" class="headerlink" title="load &#x2F; store the model.state_dict()"></a>load &#x2F; store the model.state_dict()</h2><p>Only model’s attributes that are subclass of <code>nn.Module</code> can be regarded as a valid registered parameters. These parameters are in the <code>model.state_dict()</code>, and can be load and store from&#x2F;to the disk.</p>
<ul>
<li><code>model.state_dict()</code>:</li>
</ul>
<p>The <code>state_dict()</code> is an <code>OrderedDict</code>. It contains the key value pair like “Parameter Name: Tensor”</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">model.state_dict()</span><br><span class="line"></span><br><span class="line">model.state_dict().keys()</span><br><span class="line"><span class="comment"># OUTPUT:</span></span><br><span class="line"><span class="comment"># odict_keys([&#x27;conv1.weight&#x27;, &#x27;conv1.bias&#x27;, &#x27;conv2.weight&#x27;, &#x27;conv2.bias&#x27;])</span></span><br><span class="line"></span><br><span class="line">model.state_dict().values()</span><br><span class="line"><span class="comment"># OUTPUT:</span></span><br><span class="line"><span class="comment"># odict_values([tensor([[[[ 1.0481e-01, -2.3481e-02,  9.1083e-02,  1.9955e-01,  1.0437e-01], ... ...</span></span><br></pre></td></tr></table></figure>

<ul>
<li>Use the following code to <strong>store</strong> the parameters of the model <code>Model</code> above to the disk:</li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">torch.save(model.state_dict(), <span class="string">&#x27;model.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>Use the following code to <strong>load</strong> the parameters from the disk:</li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;model.pth&#x27;</span>))</span><br></pre></td></tr></table></figure>



<h2 id="Common-Submodules"><a href="#Common-Submodules" class="headerlink" title="Common Submodules"></a>Common Submodules</h2><p>This subsection introduces some common submodules used. As mentioned above, to make them as valid registered parameters, they are subclass of <code>nn.Module</code> or are type <code>nn.Parameter</code>.</p>
<h3 id="clone-the-module"><a href="#clone-the-module" class="headerlink" title="clone the module"></a>clone the module</h3><p>The module should be copied (cloned) by the <code>copy.deepcopy</code> method. </p>
<ul>
<li>Shallow copy (wrong!)</li>
</ul>
<p>The model is only shallow copied. We can see that the two models’ <code>conv1</code> Tensor are the same one!!!</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">model = Model()</span><br><span class="line">model2 = copy.copy(model) <span class="comment"># shallow copy</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(model.conv1), <span class="built_in">id</span>(model2.conv1))</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line"><span class="number">2755774917472</span> <span class="number">2755774917472</span></span><br></pre></td></tr></table></figure>

<ul>
<li>Deep copy (right!)</li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">model = Model()</span><br><span class="line">model2 = copy.deepcopy(model) <span class="comment"># deep copy</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(model.conv1), <span class="built_in">id</span>(model2.conv1))</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line"><span class="number">2755774915552</span> <span class="number">2755774916272</span></span><br></pre></td></tr></table></figure>

<ul>
<li>Example:</li>
</ul>
<p>This is the code from <a href="https://github.com/facebookresearch/detr/blob/main/models/transformer.py#L272">DETR</a>. This copies <code>module</code> for N times, resulting in an <code>nn.ModuleList</code>. </p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_get_clones</span>(<span class="params">module, N</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N)])</span><br></pre></td></tr></table></figure>



<h3 id="nn-ModuleList"><a href="#nn-ModuleList" class="headerlink" title="nn.ModuleList"></a>nn.ModuleList</h3><p><code>nn.ModuleList</code> is a list, but inherited the <code>nn.Module</code>. It can be recognized by the model correctly. </p>
<ul>
<li>Wrong example: from the output, we can see the submodule is not registered correctly.</li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model2</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args, **kwargs</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__(*args, **kwargs)</span><br><span class="line">        self.mlp = [nn.Linear(<span class="number">10</span>, <span class="number">10</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]</span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(Model2().state_dict().keys())</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">odict_keys([])</span><br></pre></td></tr></table></figure>

<ul>
<li>Correct example: from the output, we can see the submodule is registered correctly.</li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model3</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args, **kwargs</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__(*args, **kwargs)</span><br><span class="line">        self.mlp = nn.ModuleList([nn.Linear(<span class="number">10</span>, <span class="number">10</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]) </span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(Model3().state_dict().keys())</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">odict_keys([<span class="string">&#x27;mlp.0.weight&#x27;</span>, <span class="string">&#x27;mlp.0.bias&#x27;</span>, ..., <span class="string">&#x27;mlp.9.weight&#x27;</span>, <span class="string">&#x27;mlp.9.bias&#x27;</span>])</span><br></pre></td></tr></table></figure>

<p><code>nn.ModuleDict</code> is similar to <code>nn.ModuleList</code>, but a dictionary. </p>
<h3 id="nn-Parameter"><a href="#nn-Parameter" class="headerlink" title="nn.Parameter"></a>nn.Parameter</h3><p>A plain tensor attributes can not be registered to the model. We need to wrap it with <code>nn.Parameter</code>, to make the model save the tensor’s state correctly. </p>
<p>The following is modified from the <a href="https://pytorch.org/tutorials/beginner/nn_tutorial.html#refactor-using-nn-module">official tutorial</a>. In this example, <code>self.weights</code> is merely a <code>torch.Tensor</code>, which cannot be regarded as a model’s <code>state_dict</code>. The <code>self.bias</code> would works normally, because it’s a <code>nn.Parameter</code>.  </p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Mnist_Logistic</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weights = torch.randn(<span class="number">784</span>, <span class="number">10</span>) / math.sqrt(<span class="number">784</span>) <span class="comment"># WRONG</span></span><br><span class="line">        self.bias = nn.Parameter(torch.zeros(<span class="number">10</span>)) <span class="comment"># CORRECT</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, xb</span>):</span><br><span class="line">        <span class="keyword">return</span> xb @ self.weights + self.bias</span><br></pre></td></tr></table></figure>

<p>Check if submodules is correctly regiestered:</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(Mnist_Logistic().state_dict().keys())</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">odict_keys([<span class="string">&#x27;bias&#x27;</span>]) <span class="comment"># only `bias` regiestered! no `weights` here</span></span><br></pre></td></tr></table></figure>



<h3 id="nn-Sequential"><a href="#nn-Sequential" class="headerlink" title="nn.Sequential"></a>nn.Sequential</h3><p>This is a sequential container. Data will flow by the submodules contained one by one. An example is shown below.  </p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line">model =nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">784</span>, <span class="number">128</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">128</span>, <span class="number">64</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>



<h2 id="model-apply-weight-init"><a href="#model-apply-weight-init" class="headerlink" title="model.apply() &amp; weight init"></a>model.apply() &amp; weight init</h2><p>Applies <code>fn</code> recursively to every submodule (as returned by <code>model.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also <a href="https://pytorch.org/docs/stable/nn.init.html#nn-init-doc">torch.nn.init</a>).</p>
<p>A typical example can be:</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">20</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        <span class="keyword">return</span> F.relu(self.conv2(x))</span><br><span class="line">    </span><br><span class="line">model = Model()   </span><br><span class="line"><span class="comment"># do init params with model.apply():</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        torch.nn.init.xavier_uniform_(m.weight)</span><br><span class="line">        m.bias.data.fill_(<span class="number">0.01</span>)</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">        torch.nn.init.xavier_uniform_(m.weight)</span><br><span class="line">        m.bias.data.fill_(<span class="number">0.01</span>)</span><br><span class="line">model.apply(init_weights)</span><br></pre></td></tr></table></figure>



<h2 id="model-eval-model-train-training"><a href="#model-eval-model-train-training" class="headerlink" title="model.eval() &#x2F; model.train() &#x2F; .training"></a>model.eval() &#x2F; model.train() &#x2F; .training</h2><p>The modules such as <code>BatchNorm</code> and <code>DropOut</code> performs differently on the training and evaluating stage. </p>
<p>We can use <code>model.train()</code> to set the model to the training stage. Use <code>model.eval()</code> to set the model to the training stage. </p>
<p>But, what if our <u>own written modules</u> need to perform differently in two stages? The answer is that, <code>nn.Module</code> has an attribute called <code>training</code>. It’s <code>True</code> when training, <code>False</code> otherwise. </p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># skipped in this example</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            ... <span class="comment"># write the code in training stage here</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            ... <span class="comment"># write the code in evaluating/inferencing stage here</span></span><br></pre></td></tr></table></figure>

<p>As we can see, when we called <code>model.train()</code>, actually, all submodules from <code>model</code> would set the <code>training</code> attribute to <code>True</code>, and <code>False</code> otherwise. </p>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title>PyTorch Practical Hand-Written Modules Basics 3--Tensor-wise operations</title>
    <url>/2023/06/17/Pytorch-Practical-Basics-3/</url>
    <content><![CDATA[<p>In this section we will talk about some PyTorch functions that operates the tensors. </p>
<h2 id="torch-Tensor-expand"><a href="#torch-Tensor-expand" class="headerlink" title="torch.Tensor.expand"></a>torch.Tensor.expand</h2><p>Signature: <code>Tensor.expand(*sizes) -&gt; Tensor</code></p>
<p>The <code>expand</code> function returns a new view of the <code>self</code> tensor, with singleton dimensions expanded to a larger size. The passing parameter indicates the destination size. (“singleton dimensions” means the dimension with shape <code>1</code>)</p>
<p><strong>Basic Usage</strong></p>
<p>Passing <code>-1</code> as the size for a dimension means not changing the size of that dimension.</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]]) <span class="comment"># torch.Size([3, 1])</span></span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(x.expand(<span class="number">3</span>, <span class="number">4</span>))   <span class="comment"># torch.Size([3, 4])</span></span><br><span class="line"><span class="built_in">print</span>(x.expand(-<span class="number">1</span>, <span class="number">4</span>))  <span class="comment"># torch.Size([3, 4])</span></span><br><span class="line"><span class="built_in">print</span>(x.expand(<span class="number">3</span>, -<span class="number">1</span>))  <span class="comment"># torch.Size([3, 1])</span></span><br><span class="line"><span class="built_in">print</span>(x.expand(-<span class="number">1</span>, -<span class="number">1</span>)) <span class="comment"># torch.Size([3, 1])</span></span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">tensor([[<span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>]])</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>]])</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>]])</span><br><span class="line">tensor([[<span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>]])</span><br><span class="line">tensor([[<span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>]])</span><br></pre></td></tr></table></figure>

<p><strong>Wrong Usage</strong></p>
<p>Only the dimension with shape 1 can be expanded:</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]]) <span class="comment"># torch.Size([3, 1])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.expand(<span class="number">2</span>, <span class="number">2</span>))   <span class="comment"># ERROR! can&#x27;t expand axis 0 shape from 3 (not 1)</span></span><br></pre></td></tr></table></figure>

<p><strong>Why use it?</strong></p>
<p>The return is only a view, not a new tensor. <strong>Therefore, if you only want to only read (not write) to an expanded tensor, use expand() will save much GPU memory.</strong> Note that modifying on the expanded tensor would make modification on the original as well. </p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]]) <span class="comment"># torch.Size([3, 1])</span></span><br><span class="line">x.expand(<span class="number">3</span>, <span class="number">4</span>)[<span class="number">0</span>, <span class="number">1</span>] = <span class="number">100</span></span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">tensor([[<span class="number">100</span>],</span><br><span class="line">        [  <span class="number">2</span>],</span><br><span class="line">        [  <span class="number">3</span>]])</span><br></pre></td></tr></table></figure>



<h2 id="torch-Tensor-repeat"><a href="#torch-Tensor-repeat" class="headerlink" title="torch.Tensor.repeat"></a>torch.Tensor.repeat</h2><p>Signature: <code>Tensor.repeat(*sizes) -&gt; Tensor)</code></p>
<p>Repeats this tensor along the specified dimensions. It is somewhat similar to <code>torch.Tensor.expand()</code>, but the passing in parameter indicates the repeat times. Also, this is a deep copy.</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) <span class="comment"># torch.Size([3])</span></span><br><span class="line"><span class="built_in">print</span>(x.repeat(<span class="number">4</span>, <span class="number">2</span>)) <span class="comment"># torch.Size([4, 6])</span></span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]])</span><br></pre></td></tr></table></figure>

<p><strong>More than the given ndimension</strong></p>
<p>If the <code>size</code> has <strong>more</strong> dimension than the self tensor, like the example below, the <code>x</code> only have shape 3x1, while we have more than two input parameters, then additional dimensions will be added at the front. </p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]]) <span class="comment"># torch.Size([3, 1])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.repeat(<span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>).shape)    </span><br><span class="line"><span class="comment"># torch.Size([4, 6, 1])  first 1: same. last 2 dim: [3,1]*[2,1]=[6,1]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.repeat(<span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>).shape)    </span><br><span class="line"><span class="comment"># torch.Size([4, 2, 3, 1])  first 2: same. last 2 dim: [3,1]*[1,1]=[3,1]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.repeat(<span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>).shape)    </span><br><span class="line"><span class="comment"># torch.Size([1, 4, 6, 1])  first 2: same. last 2 dim: [3,1]*[2,1]=[6,1]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.repeat(<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>).shape)    </span><br><span class="line"><span class="comment"># torch.Size([1, 1, 12, 2])  first 2: same. last 2 dim: [3,1]*[4,2]=[12,2]</span></span><br></pre></td></tr></table></figure>



<h2 id="torch-Tensor-transpose"><a href="#torch-Tensor-transpose" class="headerlink" title="torch.Tensor.transpose"></a>torch.Tensor.transpose</h2><p>Signature: <code>torch.transpose(input, dim0, dim1) -&gt; Tensor</code></p>
<p>Signature: <code>torch.Tensor.transpose(dim0, dim1) -&gt; Tensor</code></p>
<p>Returns a tensor that is a transposed version of <code>input</code>. The given dimensions <code>dim0</code> and <code>dim1</code> are swapped. </p>
<p>Therefore, like the examples below, <code>x.transpose(0, 1)</code> and <code>x.transpose(1, 0)</code> are same. </p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x) <span class="comment"># shape: torch.Size([2, 3])</span></span><br><span class="line"><span class="built_in">print</span>(x.transpose(<span class="number">0</span>, <span class="number">1</span>)) <span class="comment"># shape: torch.Size([3, 2])</span></span><br><span class="line"><span class="built_in">print</span>(x.transpose(<span class="number">1</span>, <span class="number">0</span>)) <span class="comment"># shape: torch.Size([3, 2])</span></span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">y = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(y) <span class="comment"># shape: torch.Size([2, 3, 4])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(y.transpose(<span class="number">0</span>, <span class="number">1</span>)) <span class="comment"># shape: torch.Size([3, 2, 4])</span></span><br><span class="line"><span class="built_in">print</span>(y.transpose(<span class="number">1</span>, <span class="number">0</span>)) <span class="comment"># shape: torch.Size([3, 2, 4])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(y.transpose(<span class="number">0</span>, <span class="number">2</span>)) <span class="comment"># shape: torch.Size([4, 3, 2])</span></span><br><span class="line"><span class="built_in">print</span>(y.transpose(<span class="number">2</span>, <span class="number">0</span>)) <span class="comment"># shape: torch.Size([4, 3, 2])</span></span><br></pre></td></tr></table></figure>



<h2 id="torch-Tensor-permute"><a href="#torch-Tensor-permute" class="headerlink" title="torch.Tensor.permute"></a>torch.Tensor.permute</h2><p>Signature: <code>torch.Tensor.permute(dims) -&gt; Tensor</code></p>
<p>Signature: <code>torch.permute(input, dims) -&gt; Tensor</code></p>
<p>This function reorder the dimensions. See the example below. </p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">y = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>) <span class="comment"># Shape: torch.Size([2, 3, 4])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(y.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>)) <span class="comment"># Shape: torch.Size([2, 3, 4])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(y.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)) <span class="comment"># Shape: torch.Size([2, 4, 3])</span></span><br></pre></td></tr></table></figure>

<p>Let’s have a close look to the third line as an example. </p>
<ul>
<li><p>The first argument <code>0</code> means that the new tensor’s first dimension is the original dimension at <code>0</code>, so the shape is 2.</p>
</li>
<li><p>The second argument <code>2</code> means that the new tensor’s second dimension is the original dimension at <code>2</code>, so the shape is 4. </p>
</li>
<li><p>The third argument <code>1</code> means that the new tensor’s third dimension is the original dimension at <code>1</code>, so the shape is 3.</p>
</li>
</ul>
<p>Finally, the result shape is <code>torch.Size([2, 4, 3])</code>. </p>
<h2 id="torch-Tensor-view-torch-Tensor-reshape"><a href="#torch-Tensor-view-torch-Tensor-reshape" class="headerlink" title="torch.Tensor.view &#x2F; torch.Tensor.reshape"></a>torch.Tensor.view &#x2F; torch.Tensor.reshape</h2><p>Signature: <code>Tensor.view(*shape) -&gt; Tensor</code></p>
<p>Signature: <code>Tensor.reshape(*shape) -&gt; Tensor</code></p>
<p>Reshape the Tensor to <code>shape</code>. </p>
<p>The function <code>shape()</code> always return a new copy of the tensor. </p>
<p>For function <code>view()</code>, if the <code>shape</code> satisfies some conditions (see <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.view.html">here</a>), deep copy can be avoided to save the GPU memory. </p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x) <span class="comment"># Shape: torch.Size([4, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.reshape(<span class="number">3</span>, <span class="number">4</span>)) <span class="comment"># Shape: torch.Size([3, 4])</span></span><br><span class="line"><span class="built_in">print</span>(x.reshape(-<span class="number">1</span>, <span class="number">4</span>)) <span class="comment"># Shape: torch.Size([3, 4])</span></span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x) <span class="comment"># Shape: torch.Size([4, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.view(<span class="number">3</span>, <span class="number">4</span>)) <span class="comment"># Shape: torch.Size([3, 4])</span></span><br><span class="line"><span class="built_in">print</span>(x.view(-<span class="number">1</span>, <span class="number">4</span>)) <span class="comment"># Shape: torch.Size([3, 4])</span></span><br></pre></td></tr></table></figure>

<h2 id="torch-cat"><a href="#torch-cat" class="headerlink" title="torch.cat"></a>torch.cat</h2><p>Signature: <code>torch.cat(tensors, dim=0, out=None) -&gt; Tensor</code></p>
<p>Concatenates the given sequence of <code>tensors</code> in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty. For how to determine the <code>dim</code>, please refer to my previous <a href="https://i-am-future.github.io/2023/05/27/Pytorch-Practical-Basics-2/#Key-What-is-the-%E2%80%9Cdim%E2%80%9D-parameter">article</a>. </p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x) <span class="comment"># Shape: torch.Size([2, 3])</span></span><br><span class="line"></span><br><span class="line">y = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(y) <span class="comment"># Shape: torch.Size([2, 3])</span></span><br><span class="line"></span><br><span class="line">z = torch.cat((x, y), dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(z) <span class="comment"># Shape: torch.Size([4, 3]) [2+2, 3]</span></span><br><span class="line"></span><br><span class="line">z = torch.cat((x, y), dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(z) <span class="comment"># Shape: torch.Size([2, 6]) [2, 3+3]</span></span><br></pre></td></tr></table></figure>



<h2 id="torch-stack"><a href="#torch-stack" class="headerlink" title="torch.stack"></a>torch.stack</h2><p>Signature: <code>torch.stack(tensors, dim=0, out=None) -&gt; Tensor</code></p>
<p>Concatenates a sequence of tensors along a new dimension. See example below. </p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>) <span class="comment"># Shape: torch.Size([2, 3])</span></span><br><span class="line">y = torch.randn(<span class="number">2</span>, <span class="number">3</span>) <span class="comment"># Shape: torch.Size([2, 3])</span></span><br><span class="line"></span><br><span class="line">z = torch.stack((x, y), dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(z) <span class="comment"># Shape: torch.Size([*2, 2, 3]) The first 2 is the new dimension</span></span><br><span class="line"></span><br><span class="line">z = torch.stack((x, y), dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(z) <span class="comment"># Shape: torch.Size([2, *2, 3]) The second 2 is the new dimension</span></span><br><span class="line"></span><br><span class="line">z = torch.stack((x, y), dim=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(z) <span class="comment"># Shape: torch.Size([2, 3, *2]) The last 2 is the new dimension</span></span><br></pre></td></tr></table></figure>



<h2 id="torch-vstack-hstack"><a href="#torch-vstack-hstack" class="headerlink" title="torch.vstack&#x2F;hstack"></a>torch.vstack&#x2F;hstack</h2><p><code>torch.vsplit(...)</code> is spliting the tensors vertically, which is equivalent to <code>torch.split(..., dim=0)</code>. </p>
<p> <code>torch.hsplit(...)</code> is spliting the tensors horizontally, which is equivalent to <code>torch.split(..., dim=1)</code>. </p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>) <span class="comment"># Shape: torch.Size([2, 3])</span></span><br><span class="line">y = torch.randn(<span class="number">2</span>, <span class="number">3</span>) <span class="comment"># Shape: torch.Size([2, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> torch.vstack((x, y)).shape == torch.cat((x, y), dim=<span class="number">0</span>).shape</span><br><span class="line"><span class="keyword">assert</span> torch.hstack((x, y)).shape == torch.cat((x, y), dim=<span class="number">1</span>).shape</span><br></pre></td></tr></table></figure>



<h2 id="torch-split"><a href="#torch-split" class="headerlink" title="torch.split"></a>torch.split</h2><p>Signature: <code>torch.split(tensor, split_size_or_sections, dim=0)</code></p>
<ul>
<li>If <code>split_size_or_sections</code> is an integer, then tensor will be split into equally sized chunks (if possible, ptherwise, last would be smaller).</li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">3</span>) <span class="comment"># Shape: torch.Size([4, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.split(x, <span class="number">2</span>, dim=<span class="number">0</span>)) <span class="comment"># 2-item tuple, each Shape: (2, 3)</span></span><br><span class="line"><span class="built_in">print</span>(torch.split(x, <span class="number">1</span>, dim=<span class="number">1</span>)) <span class="comment"># 3-item tuple, each Shape: (4, 1)</span></span><br></pre></td></tr></table></figure>

<ul>
<li>If <code>split_size_or_sections</code> is a list, then tensor will be split into <code>len(split_size_or_sections)</code> chunks with sizes in <code>dim</code> according to <code>split_size_or_sections</code>.</li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">3</span>) <span class="comment"># Shape: torch.Size([4, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.split(x, (<span class="number">1</span>, <span class="number">3</span>), dim=<span class="number">0</span>)) </span><br><span class="line"><span class="comment"># 2-item tuple, each Shape: (1, 3) and (3, 3)</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.split(x, (<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>), dim=<span class="number">1</span>)) </span><br><span class="line"><span class="comment"># 3-item tuple, each Shape: (4, 1) and (4, 1) and (4, 1)</span></span><br></pre></td></tr></table></figure>



<h2 id="torch-vsplit-hsplit"><a href="#torch-vsplit-hsplit" class="headerlink" title="torch.vsplit&#x2F;hsplit"></a>torch.vsplit&#x2F;hsplit</h2><p>This is actually similar to <code>torch.vstack</code> and <code>torch.hstack</code>. v means vertically, along dim&#x3D;0, and h means horizontally, along dim&#x3D;1.</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># The followings are equivalent:</span></span><br><span class="line"><span class="comment"># pair 1</span></span><br><span class="line"><span class="built_in">print</span>(torch.vsplit(x, <span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(torch.split(x, <span class="number">1</span>, dim=<span class="number">0</span>))</span><br><span class="line"><span class="comment"># pair 2</span></span><br><span class="line"><span class="built_in">print</span>(torch.hsplit(x, <span class="number">4</span>))</span><br><span class="line"><span class="built_in">print</span>(torch.split(x, <span class="number">1</span>, dim=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>



<h2 id="torch-flatten"><a href="#torch-flatten" class="headerlink" title="torch.flatten"></a>torch.flatten</h2><p>Signature: <code>torch.flatten(input, start_dim=0, end_dim=-1) -&gt; Tensor</code></p>
<p>flatten the given dimension from <code>start_dim</code> to <code>end_dim</code>. This is especially useful when converting a 3D (image) tensor to a linear vector.</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(x) <span class="comment"># Shape: torch.Size([2, 4, 4])</span></span><br><span class="line"></span><br><span class="line">flattened = torch.flatten(x, start_dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(flattened) <span class="comment"># Shape: torch.Size([2, 16])</span></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title>CMake Quick Reference</title>
    <url>/2023/05/30/CMake-QRH/</url>
    <content><![CDATA[<p>Learned and made up from <a href="https://www.bilibili.com/video/BV1fa411r7zp">video</a> and <a href="https://github.com/parallel101/course">code</a>.</p>
<p>Prerequisite: basic knowledge in C&#x2F;C++.</p>
<h2 id="Compile-a-Basic-Program"><a href="#Compile-a-Basic-Program" class="headerlink" title="Compile a Basic Program"></a>Compile a Basic Program</h2><p><a href="https://github.com/parallel101/course/tree/master/01/05">See code here</a></p>
<p>A project root directory must contain a file called <code>CMakeLists.txt</code>, describing the build procedure of the project. </p>
<p>A typical simple <code>CMakeLists.txt</code> contains the following (assuming we have two source files in the current directory, <code>main.cpp</code> and <code>hello.cpp</code>):</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.12</span>) <span class="comment"># describe the minimum cmake version</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">project</span>(hellocmake LANGUAGES CXX)    <span class="comment"># describe the project name, and lan</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">add_executable</span>(a.out main.cpp hello.cpp)</span><br></pre></td></tr></table></figure>

<p>The <code>add_executable</code> function’s signature is <code>add_executable(target, [src files...])</code>, meaning to use all <code>src files</code> to compile the <code>target</code>.</p>
<p>To build the program, run in the shell:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">cmake -B build</span><br><span class="line">cmake --build build</span><br><span class="line"><span class="comment"># run with</span></span><br><span class="line">./build/&lt;program_name&gt;</span><br></pre></td></tr></table></figure>

<p>To clean and rebuild from scratch, just</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">rm</span> -rf build</span><br></pre></td></tr></table></figure>



<h2 id="Compile-a-Library-Link-it"><a href="#Compile-a-Library-Link-it" class="headerlink" title="Compile a Library &amp; Link it"></a>Compile a Library &amp; Link it</h2><p><a href="https://github.com/parallel101/course/tree/master/01/06">See code here</a></p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="comment"># compile static OR dynamic library</span></span><br><span class="line"><span class="keyword">add_library</span>(hellolib STATIC hello.cpp)</span><br><span class="line"><span class="keyword">add_library</span>(hellolib SHARED hello.cpp)</span><br><span class="line"></span><br><span class="line"><span class="keyword">add_executable</span>(a.out main.cpp)</span><br><span class="line"></span><br><span class="line"><span class="keyword">target_link_libraries</span>(a.out PUBLIC hellolib)</span><br></pre></td></tr></table></figure>

<ul>
<li>The <code>add_library</code> function’s signature is <code>add_library(target, STATIC/SHARED [src files...])</code>, meaning to use all <code>src files</code> to compile the static&#x2F;dynamic <code>target</code> library.</li>
<li>Then, <code>target_link_libraries(a.out PUBLIC hellolib)</code> links the <code>hellolib</code>‘s source to the <code>a.out</code>.</li>
</ul>
<h2 id="Compile-a-subdirectory"><a href="#Compile-a-subdirectory" class="headerlink" title="Compile a subdirectory"></a>Compile a subdirectory</h2><p><a href="https://github.com/parallel101/course/tree/master/01/10">See code here</a></p>
<p>The sub-directory could contain a set of source codes to compile a library&#x2F;executable.</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="comment"># main CMakeLists.txt</span></span><br><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.12</span>)</span><br><span class="line"><span class="keyword">project</span>(hellocmake LANGUAGES CXX)</span><br><span class="line"></span><br><span class="line"><span class="keyword">add_subdirectory</span>(hellolib)  <span class="comment"># the name of subdirectory</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">add_executable</span>(a.out main.cpp)</span><br><span class="line"><span class="keyword">target_link_libraries</span>(a.out PUBLIC hellolib)</span><br></pre></td></tr></table></figure>

<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="comment"># sub-directory CMakeLists.txt</span></span><br><span class="line"><span class="keyword">add_library</span>(hellolib STATIC hello.cpp)</span><br></pre></td></tr></table></figure>



<p>If the <code>main.cpp</code> uses the headers in the subdirectory <code>hellolib</code>, then <code>main.cpp</code> should write <code>#include &quot;hellolib/hello.h&quot;</code>. To simplify the <code>#include</code> statement, we could add the following to main’s <code>CMakeLists.txt</code>:</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">add_executable</span>(a.out main.cpp)</span><br><span class="line"><span class="keyword">target_include_directories</span>(a.out PUBLIC hellolib)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>This is still some complex. If we want to build two executable, we need write the following, with repeated code:</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">add_executable</span>(a.out main.cpp)</span><br><span class="line"><span class="keyword">target_include_directories</span>(a.out PUBLIC hellolib)</span><br><span class="line"><span class="keyword">add_executable</span>(b.out main.cpp)</span><br><span class="line"><span class="keyword">target_include_directories</span>(b.out PUBLIC hellolib)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>A solution is to move the <code>target_include_directories()</code> to the subdirectory. Then all the further library&#x2F;executable relied on the <code>hellolib</code> will include this subdirectory. </p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="comment"># sub-directory</span></span><br><span class="line"><span class="keyword">target_include_directories</span>(hellolib PUBLIC .)</span><br></pre></td></tr></table></figure>

<p>If we change the <code>PUBLIC</code> to <code>PRIVATE</code>, then the further dependent would not have the effects.</p>
<h2 id="Link-existing-library"><a href="#Link-existing-library" class="headerlink" title="Link existing library"></a>Link existing library</h2><p>For example, use the following code to link the <code>OpenMP</code> library. </p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">find_package</span>(OpenMP REQUIRD)</span><br><span class="line"><span class="keyword">target_link_libraries</span>(main PUBLIC OpenMP::OpenMP_CXX)</span><br></pre></td></tr></table></figure>

<p>Use the following code to link the <code>OpenMP</code> library. </p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">find_package</span>(OpenCV REQUIRED)</span><br><span class="line"><span class="keyword">target_link_libraries</span>(main <span class="variable">$&#123;OpenCV_LIBS&#125;</span>)</span><br></pre></td></tr></table></figure>



<h2 id="Further-options"><a href="#Further-options" class="headerlink" title="Further options"></a>Further options</h2><ul>
<li>Set release type (Default type is DEBUG):</li>
</ul>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span>(CMAKE_BUILD_TYPE Release)</span><br><span class="line"><span class="comment"># Or set it when building</span></span><br><span class="line">cmake --build build --config Release</span><br></pre></td></tr></table></figure>

<ul>
<li>Set C++ standard:</li>
</ul>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SET</span>(CMAKE_CXX_STANDARD <span class="number">17</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>Set global &#x2F; special macros:</li>
</ul>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="comment"># global</span></span><br><span class="line"><span class="keyword">add_definitions</span>(-DDEBUG) <span class="comment"># -D is not necessary</span></span><br><span class="line"><span class="keyword">add_definitions</span>(DEBUG)</span><br><span class="line"><span class="comment"># special target</span></span><br><span class="line"><span class="keyword">target_compile_definitions</span>(a.out PUBLIC -DDEBUG)</span><br><span class="line"><span class="keyword">target_compile_definitions</span>(a.out PUBLIC DEBUG)</span><br><span class="line"></span><br><span class="line"><span class="comment"># They have the same effect as</span></span><br><span class="line">g++ xx.cpp -DDEBUG <span class="comment"># (define a `DEBUG` macro to the file)</span></span><br></pre></td></tr></table></figure>

<ul>
<li>Set global &#x2F; special compiling options:</li>
</ul>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="comment"># global</span></span><br><span class="line"><span class="keyword">add_compile_options</span>(-O2)</span><br><span class="line"><span class="comment"># special target</span></span><br><span class="line"><span class="keyword">target_compile_options</span>(a.out PUBLIC -O0)</span><br><span class="line"></span><br><span class="line"><span class="comment"># They have the same effect as</span></span><br><span class="line">g++ xx.cpp -O0 <span class="comment"># (add a `-O0` option in the compilation)</span></span><br></pre></td></tr></table></figure>

<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Set SIMD and fast-math</span></span><br><span class="line"><span class="keyword">target_compile_options</span>(a.out PUBLIC -ffast-<span class="keyword">math</span> -march=native)</span><br></pre></td></tr></table></figure>

<ul>
<li>Set global &#x2F; special include directories:</li>
</ul>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="comment"># global</span></span><br><span class="line"><span class="keyword">include_directories</span>(hellolib)</span><br><span class="line"><span class="comment"># special target</span></span><br><span class="line"><span class="keyword">target_include_directories</span>(a.out PUBLIC hellolib)</span><br></pre></td></tr></table></figure>




<h2 id="CUDA-with-CMake"><a href="#CUDA-with-CMake" class="headerlink" title="CUDA with CMake"></a>CUDA with CMake</h2><p>A common template can be:</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.10</span>)</span><br><span class="line"><span class="keyword">project</span>(main LANGUAGES CUDA CXX)</span><br><span class="line"></span><br><span class="line"><span class="keyword">SET</span>(CMAKE_CXX_STANDARD <span class="number">17</span>)</span><br><span class="line"><span class="keyword">set</span>(CMAKE_CUDA_STANDARD <span class="number">17</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">add_executable</span>(main main.cu)</span><br><span class="line"><span class="keyword">set_target_properties</span>(main PROPERTIES CUDA_ARCHITECTURES <span class="string">&quot;86&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>QRH</category>
      </categories>
  </entry>
  <entry>
    <title>PyTorch Practical Hand-Written Modules Basics 2--Tensor basics (Tensor functions, &quot;axis&quot; and indexing)</title>
    <url>/2023/05/27/Pytorch-Practical-Basics-2/</url>
    <content><![CDATA[<p>In this section, we will briefly talk about the arithmetic functions in the PyTorch. Then, we will introduce the <code>axis</code> parameter in most of these functions in detail. </p>
<p>Finally, we talk about indexing the tensor, which is very tricky in manipulating the tensors as well. </p>
<h2 id="Tensor-functions"><a href="#Tensor-functions" class="headerlink" title="Tensor functions"></a>Tensor functions</h2><p>PyTorch supports many arithmetic functions for tensor. They are vectorized and acts very similar to <code>numpy</code>. (So if you are not familiar with <code>numpy</code>, learn it first). In the following, I’ll introduce some functions with the official docs.</p>
<ul>
<li><p><a href="https://pytorch.org/docs/stable/torch.html#pointwise-ops">binary arithmetic functions</a>, such as <code>+, -, *, /, @</code> etc. Entry-wise operations, supports <a href="https://pytorch.org/docs/stable/notes/broadcasting.html">broadcasting</a>.</p>
</li>
<li><p><a href="https://pytorch.org/docs/stable/torch.html#pointwise-ops">binary logical functions</a>, such as <code>torch.bitwise_and()</code>, <code>torch.bitwise_or</code>…</p>
</li>
<li><p><a href="https://pytorch.org/docs/stable/torch.html#pointwise-ops">math functions</a>, such as <code>exp, log, sigmoid</code> etc.</p>
</li>
<li><p><a href="https://pytorch.org/docs/stable/torch.html#comparison-ops">comparison functions</a>, such as <code>torch.eq</code>, <code>torch.ge</code>. The <code>==</code> and <code>&gt;=</code> operators are overloaded, so they have the same effect.</p>
</li>
<li><p><a href="https://pytorch.org/docs/stable/torch.html#reduction-ops">reduction functions</a>. They are usually very useful. e.g., <code>mean</code>, <code>median</code>, <code>argmax</code>, <code>sum</code>… They do the corresponding operations on a specific dimension, requiring the “dim” parameter (See below). </p>
</li>
<li><p>…… For more functions, please visit the <a href="https://pytorch.org/docs/stable/torch.html#module-torch">docs</a>.</p>
</li>
</ul>
<h3 id="Key-What-is-the-“dim”-parameter"><a href="#Key-What-is-the-“dim”-parameter" class="headerlink" title="Key: What is the “dim” parameter?"></a>Key: What is the “dim” parameter?</h3><p>For the reduction functions such as <code>argmax</code>, we need to pass a parameter called <code>dim</code>. What does it mean?</p>
<ul>
<li><p>The default value or <code>dim</code> is <code>None</code>, indicates that do the <code>argmax</code> for all the entries. </p>
</li>
<li><p>On the other hand, if we specifies the <code>dim</code> parameter, that means, we apply the function <code>argmax</code> on each vector along a specific “axis”. For all of the example below, we use a <code>4x3x4</code> 3D tensor.</p>
</li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># create a 4x3x4 tensor</span></span><br><span class="line">a = torch.randn(<span class="number">4</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>



<ol>
<li>Then, in the first case, we do:</li>
</ol>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">a1 = torch.argmax(a, dim=<span class="number">0</span>)</span><br><span class="line">a1.shape</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure>

<p>See the gif below. If we set <code>dim=0</code>, that means, we apply the <code>argmax</code> function on each yellow vector (they are in the direction of dim0). The original tensor’s shape is 4x3x4, we reduce on the dim0, so now it’s 3x4, containing all results from <code>argmax</code> on the yellow vectors. </p>
<img src="/2023/05/27/Pytorch-Practical-Basics-2/dim0.gif" alt="dim0" style="zoom: 67%;">



<ol start="2">
<li>Then, in the second case, we do:</li>
</ol>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">a2 = torch.argmax(a, dim=<span class="number">1</span>)</span><br><span class="line">a2.shape</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure>

<p>See the gif below. If we set <code>dim=1</code>, that means, we apply the <code>argmax</code> function on each yellow vector (they are in the direction of dim1). The original tensor’s shape is 4x3x4, we reduce on the dim1, so now we will have a result with 4x4 shape. </p>
<img src="/2023/05/27/Pytorch-Practical-Basics-2/dim1.gif" alt="dim1" style="zoom: 67%;">



<ol start="3">
<li>Then, in the third case, we do:</li>
</ol>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">a3 = torch.argmax(a, dim=<span class="number">2</span>)</span><br><span class="line">a3.shape</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>

<p>See the gif below. If we set <code>dim=2</code>, that means, we apply the <code>argmax</code> function on each yellow vector (they are in the direction of dim2). The original tensor’s shape is 4x3x4, we reduce on the dim2, so now we will have a result with 4x3 shape. </p>
<img src="/2023/05/27/Pytorch-Practical-Basics-2/dim2.gif" alt="dim2" style="zoom: 67%;">

<h3 id="As-member-function"><a href="#As-member-function" class="headerlink" title="As member function"></a>As member function</h3><p>Many functions mentioned above has member function style. For example, the following pairs are equivalent.</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">a = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment"># pair1</span></span><br><span class="line">_ = torch.<span class="built_in">sum</span>(a)</span><br><span class="line">_ = a.<span class="built_in">sum</span>()</span><br><span class="line"><span class="comment"># pair2</span></span><br><span class="line">_ = torch.argmax(a, dim=<span class="number">0</span>)</span><br><span class="line">_ = a.argmax(dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<h3 id="As-in-place-function"><a href="#As-in-place-function" class="headerlink" title="As in-place function"></a>As in-place function</h3><p>The functions mentioned above returns a new result tensor, keeping the original one same. In some cases, we can do in-place operation on the tensor. The in-place functions are terminated with a <code>_</code>. </p>
<p>For example, the following pairs are equivalent.</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">a = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment"># pair 1</span></span><br><span class="line">a = torch.cos(a)</span><br><span class="line">a = a.cos()</span><br><span class="line">a.cos_()</span><br><span class="line"><span class="comment"># pair 2</span></span><br><span class="line">a = torch.clamp(a, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">a = a.clamp(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">a.clamp_(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">a.clamp(<span class="number">1</span>, <span class="number">2</span>) <span class="comment"># Wrong: this line has no effect. The a remains same; the return value was assigned to nothing.</span></span><br></pre></td></tr></table></figure>





<h2 id="Tensor-indexing"><a href="#Tensor-indexing" class="headerlink" title="Tensor indexing"></a>Tensor indexing</h2><p>Indexing is very powerful in torch. They are very similar to the one in <code>numpy</code>.  Learn <code>numpy</code> first if you are not familiar with it.</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">a = torch.randn(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># a is</span></span><br><span class="line">tensor([[ <span class="number">1.1351</span>,  <span class="number">0.7592</span>, -<span class="number">3.5945</span>],</span><br><span class="line">        [ <span class="number">0.0192</span>,  <span class="number">0.1052</span>,  <span class="number">0.9603</span>],</span><br><span class="line">        [-<span class="number">0.5672</span>, -<span class="number">0.5706</span>,  <span class="number">1.5980</span>],</span><br><span class="line">        [ <span class="number">0.1115</span>, -<span class="number">0.0392</span>,  <span class="number">1.4112</span>]])</span><br></pre></td></tr></table></figure>

<p>The indexing supports many types, you can pass:</p>
<ul>
<li><p>An integer. <code>a[1, 2]</code> returns just one value 0-D tensor <code>tensor(0.9603)</code>, one element at (row 1, col 2).</p>
</li>
<li><p>A Slice. <code>a[1::2, 2]</code> returns 1-D tensor <code>tensor([0.9603, 1.4112])</code>, two elements at (row 1, col 2) and (row 3, col 2).</p>
</li>
<li><p>A colon. colon means everything on this dim.<code>a[:, 2]</code> returns 1-D tensor <code>tensor([-3.5945,  0.9603,  1.5980,  1.4112])</code>, a column of 4 elements at col 2.</p>
</li>
<li><p>A None. None is used to create a new dim on the given axis. E.g., <code>a[:, None, :]</code> has the shape of <code>torch.Size([4, 1, 3])</code>. A further example:</p>
</li>
</ul>
<p>​		<code>a[:, 2]</code> returns 1-D vector <code>tensor([-3.5945,  0.9603,  1.5980,  1.4112])</code> of col 2.</p>
<p>​		<code>a[:, 2, None]</code> returns 2-D vector <code>tensor([[-3.5945],  [0.9603],  [1.5980],  [1.4112]])</code> of col 2, which the original shape is kept.</p>
<ul>
<li><p>A <code>...</code> (Ellipsis). Ellipsis can be used as multiple <code>:</code>. E.g., </p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">a = torch.arange(<span class="number">16</span>).reshape(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"><span class="comment"># The following returns the same value</span></span><br><span class="line">a[..., <span class="number">1</span>]</span><br><span class="line">a[:, :, :, <span class="number">1</span>]</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title>PyTorch Practical Hand-Written Modules Basics 1--Tensor basics (attributes, creation)</title>
    <url>/2023/05/27/Pytorch-Practical-Basics-1/</url>
    <content><![CDATA[<p>This series would not be a general PyTorch introduction or detailed tutorials. Instead, this would be a very practical introduction to some common basics needed for Implementing Hand-Written Modules. </p>
<p>This is the First Section of this series, we would like to introduce some tensor basics, including: tensor attributes, tensor creation, and some other things. All the things I mentioned will be practical, but not exhaustive. </p>
<h1 id="1-Tensor-attributes"><a href="#1-Tensor-attributes" class="headerlink" title="1. Tensor attributes"></a>1. Tensor attributes</h1><p>We introduce 5 key attributes for <code>torch.tensor</code> <code>a</code> here:</p>
<h2 id="1-1-a-shape"><a href="#1-1-a-shape" class="headerlink" title="1.1 a.shape"></a>1.1 a.shape</h2><ul>
<li><code>a.shape</code>: Returns the shape of <code>a</code>. The return type is <code>torch.Size</code>. Example:</li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">a = torch.randn(<span class="number">10</span>, <span class="number">20</span>) <span class="comment"># create a 10x20 tensor</span></span><br><span class="line">a.shape</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">torch.Size([<span class="number">10</span>, <span class="number">20</span>])</span><br></pre></td></tr></table></figure>
<p>The <code>torch.Size</code> object supports some tricks:</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># unpack</span></span><br><span class="line">h, w = a.shape</span><br><span class="line">h, w</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">(<span class="number">10</span>, <span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># unpack in function calls</span></span><br><span class="line"><span class="built_in">print</span>(*a.shape)</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line"><span class="number">10</span> <span class="number">20</span></span><br></pre></td></tr></table></figure>
<h2 id="1-2-a-ndim"><a href="#1-2-a-ndim" class="headerlink" title="1.2 a.ndim"></a>1.2 a.ndim</h2><ul>
<li><code>a.ndim</code>: Returns number of dimensions of <code>a</code>. </li>
</ul>
<p>It looks like <code>len(a.shape)</code>. It also has a function version, called <code>a.ndimension()</code></p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">a.ndim</span><br><span class="line">a.ndimension()</span><br><span class="line"><span class="built_in">len</span>(a.shape)</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">2</span></span><br></pre></td></tr></table></figure>
<h2 id="1-3-a-device"><a href="#1-3-a-device" class="headerlink" title="1.3 a.device"></a>1.3 a.device</h2><ul>
<li><code>a.device</code>: Returns where the <code>a</code> locates.</li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">a.device</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">device(<span class="built_in">type</span>=<span class="string">&#x27;cpu&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>Convert to CUDA by using <code>a = a.to(&#39;cuda:0&#39;)</code>. Convert back to CPU by using <code>a = a.to(&#39;cpu&#39;)</code> or <code>a = a.cpu()</code>.</p>
<h2 id="1-4-a-dtype"><a href="#1-4-a-dtype" class="headerlink" title="1.4 a.dtype"></a>1.4 a.dtype</h2><ul>
<li><code>a.dtype</code>: Returns the data type of <code>a</code>. </li>
</ul>
<p>The data type of tensor <code>a</code>. It’s very important in PyTorch! Usually, the data type would be <code>torch.float32</code> or <code>torch.int64</code>. Some data type convert method:</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># to float32</span></span><br><span class="line">f = a.<span class="built_in">float</span>()</span><br><span class="line">f.dtype</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">torch.float32</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># to int64</span></span><br><span class="line">l = a.long()</span><br><span class="line">l.dtype</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">torch.int64</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># to int32</span></span><br><span class="line">i = a.<span class="built_in">int</span>()</span><br><span class="line">i.dtype</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">torch.int32</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Also, we can use .to() as well:</span></span><br><span class="line">f = a.to(torch.float32)</span><br><span class="line">f.dtype</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">torch.float32</span><br></pre></td></tr></table></figure>
<h2 id="1-5-a-numel"><a href="#1-5-a-numel" class="headerlink" title="1.5 a.numel"></a>1.5 a.numel</h2><ul>
<li><code>a.numel()</code>: Returns <strong>num</strong>ber of <strong>el</strong>ements in <code>a</code>. Usually used in counting number of parameters in the model.</li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">a.numel()</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line"><span class="number">200</span>  <span class="comment"># it&#x27;s 10*20!</span></span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">model = torchvision.models.resnet50()</span><br><span class="line"><span class="built_in">sum</span>([p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters()])</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line"><span class="number">25557032</span></span><br></pre></td></tr></table></figure>
<h1 id="2-Tensor-creation"><a href="#2-Tensor-creation" class="headerlink" title="2. Tensor creation"></a>2. Tensor creation</h1><p>PyTorch tensors plays key role in writing deep learning programs. Usually, tensor are from two types: data and auxiliary variables (e.g., masks).</p>
<h2 id="2-1-From-data"><a href="#2-1-From-data" class="headerlink" title="2.1 From data"></a>2.1 From data</h2><p>For the data tensor, they are usually converted from other packages, such as <code>numpy</code>. We have several methods to convert it to <code>torch.tensor</code>.</p>
<ul>
<li><code>torch.tensor(arr)</code> Returns a deep copy of <code>arr</code>, i.e., the storage data is independent with <code>arr</code>. (Very memory and time consuming, not recommended for most cases)</li>
<li><p><code>torch.from_numpy(arr)</code> Returns a shallow copy tensor, i.e., the storage data is shared with <code>arr</code>. </p>
</li>
<li><p><code>torch.as_tensor(arr, dtype=..., device=...)</code> If <code>dtype</code> and <code>device</code> is same as <code>arr</code>, then it behaves like <code>torch.from_numpy()</code> function, shallow copy. Otherwise, it acts like <code>torch.tensor()</code>, deep copy. So using the function is recommended. </p>
</li>
</ul>
<h2 id="2-2-Special-tensors"><a href="#2-2-Special-tensors" class="headerlink" title="2.2 Special tensors"></a>2.2 Special tensors</h2><p>For the special tensors, PyTorch provides some common methods:</p>
<ul>
<li>Linear:</li>
</ul>
<p>We have <code>torch.linspace</code> and <code>torch.arange</code>. They are easy to understand. Please see the docs <a href="https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace">linspace</a> and <a href="https://pytorch.org/docs/stable/generated/torch.arange.html#torch.arange">arange</a>.</p>
<ul>
<li>Random: </li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">torch.randn(<span class="number">1</span>, <span class="number">2</span>) <span class="comment"># normal distribution, shape 1x2</span></span><br><span class="line">torch.rand(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># uniform[0, 1) distribution, shape 1x2</span></span><br><span class="line">torch.randint(<span class="number">0</span>, <span class="number">100</span>, (<span class="number">1</span>, <span class="number">2</span>)) <span class="comment"># uniform[0, 100) distribution, shape 1x2</span></span><br></pre></td></tr></table></figure>
<p>These functions also support passing in <code>torch.Size()</code> or a sequence as the size parameter. </p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">a = torch.randn(<span class="number">10</span>, <span class="number">10</span>) <span class="comment"># a is in shape 10x10</span></span><br><span class="line">torch.randn(a.shape)    <span class="comment"># good!</span></span><br><span class="line">torch.randn([<span class="number">10</span>, <span class="number">10</span>])   <span class="comment"># good!</span></span><br></pre></td></tr></table></figure>
<ul>
<li>Special tensors:</li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">torch.zeros(<span class="number">10</span>, <span class="number">10</span>) <span class="comment"># all zero tensor, shape 10x10</span></span><br><span class="line">torch.ones(<span class="number">10</span>, <span class="number">10</span>)  <span class="comment"># all one tensor, shape 10x10</span></span><br><span class="line"><span class="comment"># By default, the dtype is float32.</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>xxx_like()</code></li>
</ul>
<p>PyTorch has a series of function looks like <code>xxx_like()</code>, such as <code>ones_like()</code>, <code>zeros_like()</code>, <code>randn_like()</code>. These functions generates the tensor with the name, and the <code>dtype</code> and <code>device</code> and <code>layout</code> is same as the passing-in tensor. </p>
<p><code>torch.rand_like(input)</code> is equivalent to <code>torch.rand(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)</code>.</p>
<p>An example:</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">arr = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], dtype=torch.float64)</span><br><span class="line"><span class="built_in">print</span>(arr.shape) <span class="comment"># torch.Size([3])</span></span><br><span class="line"><span class="built_in">print</span>(arr.dtype) <span class="comment"># torch.float64</span></span><br><span class="line"></span><br><span class="line">z = torch.zeros_like(arr)</span><br><span class="line"><span class="built_in">print</span>(z.shape)   <span class="comment"># torch.Size([3])</span></span><br><span class="line"><span class="built_in">print</span>(z.dtype)   <span class="comment"># torch.float64</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title>My Setup to New Linux Server Account</title>
    <url>/2023/05/07/My-Setup-to-New-Linux-Server-Account/</url>
    <content><![CDATA[<p>In this article, I list some procedures I do when setting up new Linux server account.</p>
<h1 id="0-Change-the-shell"><a href="#0-Change-the-shell" class="headerlink" title="0. Change the shell"></a>0. Change the shell</h1><p>This step is <strong>optional</strong>. Sometimes the default shell is <code>sh</code>. We can change it to other better shells like <code>bash</code>, <code>zsh</code>.</p>
<h2 id="0-1-Show-the-current-shells"><a href="#0-1-Show-the-current-shells" class="headerlink" title="0.1 Show the current shells"></a>0.1 Show the current shells</h2><p>You can display the current shell name by either the following commands:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="variable">$0</span></span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">-bash</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$SHELL</span></span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">/bin/bash</span><br></pre></td></tr></table></figure>

<h2 id="0-2-Get-all-available-shells"><a href="#0-2-Get-all-available-shells" class="headerlink" title="0.2 Get all available shells"></a>0.2 Get all available shells</h2><p>To check what shells are installed, type the following commands:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cat</span> /etc/shells</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># /etc/shells: valid login shells</span></span><br><span class="line">/bin/sh</span><br><span class="line">/bin/bash</span><br><span class="line">/bin/rbash</span><br><span class="line">/bin/dash</span><br><span class="line">/usr/bin/tmux</span><br><span class="line">/usr/bin/screen</span><br><span class="line">/bin/zsh</span><br><span class="line">/usr/bin/zsh</span><br></pre></td></tr></table></figure>

<h2 id="0-3-Change-the-shell"><a href="#0-3-Change-the-shell" class="headerlink" title="0.3 Change the shell"></a>0.3 Change the shell</h2><p>Use the <code>chsh</code> (<strong>ch</strong>ange <strong>sh</strong>ell) command to change the shell, with <code>-s</code> flag:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">chsh -s /bin/bash</span><br><span class="line">Password:</span><br></pre></td></tr></table></figure>

<p>Then type the password of the login account (The password is hidden, just type it). Finally, you can quit the shell, and restart it once. You’ll see the new shell. </p>
<h1 id="1-Install-conda"><a href="#1-Install-conda" class="headerlink" title="1. Install conda"></a>1. Install conda</h1><p>Please refer to the official docs:</p>
<p><a href="https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html">https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html</a></p>
<h1 id="2-Setup-ssh-key-password-free-login"><a href="#2-Setup-ssh-key-password-free-login" class="headerlink" title="2. Setup ssh key (password-free login)"></a>2. Setup ssh key (password-free login)</h1><p>Modified from <a href="https://github.com/bokesyo/CSC4005_2022Fall_Demo/blob/main/docs/Instruction%20on%20Passwordlessly%20Connecting%20to%20Cluster%20by%20Setting%20Up%20SSH%20Key%20Authentification.md">here</a>.</p>
<p>(Replace <code>&#123;KeyName&#125;</code> to any name you like)</p>
<h2 id="2-1-Generate-SSH-Key-Pair-on-Your-Local-Machine"><a href="#2-1-Generate-SSH-Key-Pair-on-Your-Local-Machine" class="headerlink" title="2.1. Generate SSH Key Pair on Your Local Machine"></a>2.1. Generate SSH Key Pair on Your Local Machine</h2><p>(macOS)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## Execute the following commands on your local machine</span></span><br><span class="line"><span class="built_in">cd</span> ~/.ssh</span><br><span class="line">ssh-keygen -t rsa -b 1024 -f <span class="string">&quot;&#123;KeyName&#125;&quot;</span> -C <span class="string">&quot;&#123;Put Any Comment You Like&#125;&quot;</span></span><br><span class="line">ssh-add -K ./&#123;KeyName&#125;</span><br></pre></td></tr></table></figure>
<p>To check if you make it right, type the following command and you should see a string as the output.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cat</span> ~/.ssh/&#123;KeyName&#125;.pub</span><br></pre></td></tr></table></figure>

<p>(Windows)</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line"><span class="comment">## Execute the following commands on your local machine</span></span><br><span class="line"><span class="built_in">cd</span> C:\Users\&#123;UserName&#125;\.ssh</span><br><span class="line">ssh<span class="literal">-keygen</span> <span class="literal">-t</span> rsa <span class="literal">-b</span> <span class="number">1024</span> <span class="operator">-f</span> <span class="string">&quot;&#123;KeyName&#125;&quot;</span> <span class="literal">-C</span> <span class="string">&quot;&#123;Put Any Comment You Like&#125;&quot;</span></span><br></pre></td></tr></table></figure>
<p>To check if you make it right, double click the key file and you should see a string.</p>
<p><strong>Notes:</strong></p>
<ul>
<li>Execute ssh-keygen on your own computer instead of the cluster.</li>
<li>It is fine to use the SSH key file generated before (eg. id_rsa.pub), if you have.</li>
<li>ssh-keygen will ask you to set a paraphrase, which improves the security of using SSH key authentication. Type “Enter” for no paraphrase.</li>
</ul>
<p><strong>Parameters for ssh-keygen command:</strong></p>
<ul>
<li><strong>-t</strong>: type for ssh key generation. Here we use rsa</li>
<li><strong>-b</strong>: bits</li>
<li><strong>-f</strong>: name of your ssh key file. You are recommended to set this parameter in case it<br>is conflict with the ssh key file you generated before.</li>
<li><strong>-C</strong>: comments to distinguish the ssh key file from others</li>
</ul>
<h2 id="2-2-Transfer-Your-SSH-Public-Key-File-to-the-Server"><a href="#2-2-Transfer-Your-SSH-Public-Key-File-to-the-Server" class="headerlink" title="2.2. Transfer Your SSH Public Key File to the Server"></a>2.2. Transfer Your SSH Public Key File to the Server</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## Execute the following commands on your local machine</span></span><br><span class="line">scp ~/.ssh/&#123;KeyName&#125;.pub &#123;Your Account Name&#125;@&#123;Server ip&#125;:~</span><br></pre></td></tr></table></figure>
<p><strong>Notes:</strong></p>
<ul>
<li>This command will ask you for password for data transfer. Please make sure that you type in the correct password. If you are Windows user and you want to copy &amp; paste the password for convenience, try “Ctrl + Shift + V” if you fail to paste the password with “Ctrl + V”.</li>
</ul>
<h2 id="2-3-Configure-authorized-keys-on-the-Server"><a href="#2-3-Configure-authorized-keys-on-the-Server" class="headerlink" title="2.3. Configure authorized_keys on the Server"></a>2.3. Configure authorized_keys on the Server</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## Login the cluster with password</span></span><br><span class="line">ssh &#123;Your Account Name&#125;@&#123;Server ip&#125;</span><br><span class="line"><span class="comment">## All the following commands should be executed on the cluster</span></span><br><span class="line"><span class="built_in">mkdir</span> -p ~/.ssh</span><br><span class="line"><span class="built_in">cat</span> ./&#123;KeyName&#125;.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class="line"><span class="built_in">rm</span> -f ~/&#123;KeyName&#125;.pub</span><br><span class="line"><span class="built_in">chmod</span> 700 ~/.ssh</span><br><span class="line"><span class="built_in">chmod</span> 600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure>
<p>To check if you make it right, execute the following command and you should see a string as the output that is the same as in Step-1.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cat</span> ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure>

<h2 id="2-4-Prepare-for-SSH-Connection-with-Key-Authentication"><a href="#2-4-Prepare-for-SSH-Connection-with-Key-Authentication" class="headerlink" title="2.4. Prepare for SSH Connection with Key Authentication"></a>2.4. Prepare for SSH Connection with Key Authentication</h2><p>Add the following content to <code>~/.ssh/config</code> file on your local machine:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Host &#123;Any Name You Like&#125;</span><br><span class="line">    HostName &#123;ip&#125;</span><br><span class="line">    IdentityFile ~/.ssh/&#123;KeyName&#125;</span><br><span class="line">    User &#123;Your Account Name&#125;</span><br></pre></td></tr></table></figure>
<p><strong>Notes:</strong></p>
<ul>
<li>It is recommended to configure with the Remote SSH extension on VS Code. Please refer to <a href="https://code.visualstudio.com/docs/remote/ssh">Remote Development using SSH</a> for more information.</li>
</ul>
<h2 id="2-5-Login-the-Server-Password-free-with-SSH-key-Authentication"><a href="#2-5-Login-the-Server-Password-free-with-SSH-key-Authentication" class="headerlink" title="2.5. Login the Server Password-free with SSH key Authentication"></a>2.5. Login the Server Password-free with SSH key Authentication</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh &#123;Your Account Name&#125;@&#123;Server ip&#125;</span><br></pre></td></tr></table></figure>
<p>or</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh &#123;Your Student ID&#125;@&#123;Any Name You Like&#125; <span class="comment"># above in Sec. 4</span></span><br></pre></td></tr></table></figure>

<p>or </p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">ssh &#123;Any Name You Like&#125; <span class="comment"># above in Sec. 4</span></span><br></pre></td></tr></table></figure>



<h1 id="3-Set-some-alias"><a href="#3-Set-some-alias" class="headerlink" title="3. Set some alias"></a>3. Set some alias</h1><p>Alias convenient us typing some common commands.</p>
<p>Add the following to the end of <code>~/.bashrc</code>:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">alias</span> cf=<span class="string">&quot;ls -l | grep &quot;</span>^-<span class="string">&quot; | wc -l&quot;</span></span><br><span class="line"><span class="built_in">alias</span> ns=<span class="string">&quot;nvidia-smi&quot;</span></span><br><span class="line"><span class="built_in">alias</span> py=<span class="string">&quot;python&quot;</span></span><br><span class="line"><span class="built_in">alias</span> act=<span class="string">&quot;conda activate&quot;</span></span><br></pre></td></tr></table></figure>

<p>The first alias, <code>cf</code> means count files, can count number of visible files (excluding directories) under the current working directory.</p>
<p>The second alias, <code>ns</code> is short for <code>nvidia-smi</code> to check for the GPU information.</p>
<p>The third alias <code>py</code> is short for <code>python</code>.</p>
<p>The fourth alias <code>act</code> can be used like <code>act py39</code> (activate the conda <code>py39</code> environment)</p>
<h1 id="4-Configure-the-CUDA-compiler-version"><a href="#4-Configure-the-CUDA-compiler-version" class="headerlink" title="4. Configure the CUDA compiler version"></a>4. Configure the CUDA compiler version</h1><p>Add the following to the end of <code>~/.bashrc</code>:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=/usr/local/cuda-11.1/bin:<span class="variable">$PATH</span></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=/usr/local/cuda-11.1/lib64:<span class="variable">$LD_LIBRARY_PATH</span></span><br></pre></td></tr></table></figure>

<p>Change the CUDA directory (<code>/usr/local/cuda-11.1</code> above) to the one you actually want to use.</p>
]]></content>
      <categories>
        <category>Techniques</category>
      </categories>
  </entry>
  <entry>
    <title>Conda Common Commands Quick Reference</title>
    <url>/2023/05/07/Conda-common-commands/</url>
    <content><![CDATA[<h2 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h2><p>Please refer to the official <a href="https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html">docs</a>. </p>
<p>Mirrors: to download faster in China, it’s recommended to add some mirror configs:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ </span><br><span class="line">conda config --<span class="built_in">set</span> show_channel_urls <span class="built_in">yes</span></span><br></pre></td></tr></table></figure>



<h2 id="Common-commands"><a href="#Common-commands" class="headerlink" title="Common commands"></a>Common commands</h2><h3 id="Create-env"><a href="#Create-env" class="headerlink" title="Create env"></a>Create env</h3><p>Create an environment with <code>&lt;env_name&gt;</code> and python with <code>3.9</code>. </p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">conda create -n &lt;env_name&gt; python=3.9</span><br></pre></td></tr></table></figure>

<h3 id="Activate-env"><a href="#Activate-env" class="headerlink" title="Activate env"></a>Activate env</h3><p>Activate &#x2F; Deactivate the environment <code>&lt;env_name&gt;</code>.</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># activate</span></span><br><span class="line">conda activate &lt;env_name&gt;</span><br><span class="line"><span class="comment"># deactivate</span></span><br><span class="line">conda deactivate &lt;env_name&gt;</span><br></pre></td></tr></table></figure>

<h3 id="Remove-env"><a href="#Remove-env" class="headerlink" title="Remove env"></a>Remove env</h3><p>Remove the environment <code>&lt;env_name&gt;</code>.</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">conda remove -n &lt;env_name&gt; --all</span><br></pre></td></tr></table></figure>

<h3 id="Install-packages"><a href="#Install-packages" class="headerlink" title="Install packages"></a>Install packages</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># with conda</span></span><br><span class="line">conda install &lt;packages&gt;</span><br><span class="line"><span class="comment"># with pip</span></span><br><span class="line">pip install &lt;packages&gt;</span><br><span class="line"><span class="comment"># with pip and mirrors</span></span><br><span class="line">pip install &lt;packages&gt; -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>

<h3 id="Update-packages"><a href="#Update-packages" class="headerlink" title="Update packages"></a>Update packages</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">conda update &lt;packages&gt;</span><br></pre></td></tr></table></figure>

<h3 id="List-packages-in-env"><a href="#List-packages-in-env" class="headerlink" title="List packages in env"></a>List packages in env</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">conda list</span><br></pre></td></tr></table></figure>

<h3 id="List-all-envs"><a href="#List-all-envs" class="headerlink" title="List all envs"></a>List all envs</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">conda info --envs</span><br></pre></td></tr></table></figure>

<h3 id="Clean-caches"><a href="#Clean-caches" class="headerlink" title="Clean caches"></a>Clean caches</h3><p>The downloaded caches occupies the storage data. We may clean them by the following commands:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">conda clean --all</span><br></pre></td></tr></table></figure>

<h3 id="Export-env-configs"><a href="#Export-env-configs" class="headerlink" title="Export env configs"></a>Export env configs</h3><p>Export the current environment configs to a yaml file, then we can follow the yaml file to create another environment.</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># change to the desired env</span></span><br><span class="line">conda activate &lt;env_name&gt;</span><br><span class="line"><span class="comment"># export </span></span><br><span class="line">conda <span class="built_in">env</span> <span class="built_in">export</span> &gt; environment.yml</span><br><span class="line"><span class="comment"># import and create</span></span><br><span class="line">conda <span class="built_in">env</span> create -f environment.yml</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>QRH</category>
      </categories>
  </entry>
  <entry>
    <title>CUDA No process found but GPU memory occupied</title>
    <url>/2023/05/06/CUDA-No-process-but-GPU-memory-occupied/</url>
    <content><![CDATA[<h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h2><p>when typing <code>nvidia-smi</code>, we find there is GPU memory occupied (See Red Boxes), but we cannot see any relevant process on that GPU (See Orange Boxes).</p>
<img src="/2023/05/06/CUDA-No-process-but-GPU-memory-occupied/problem.png" alt="problem" style="zoom:50%;">

<h2 id="Possible-Answer"><a href="#Possible-Answer" class="headerlink" title="Possible Answer"></a>Possible Answer</h2><p>This can be caused by <code>torch.distributed</code> and other multi-processing CUDA programs. When the main process terminated, the background process still alive, not killed.</p>
<ol>
<li>To figure which processes used the GPU, we can use the following command:</li>
</ol>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">fuser -v /dev/nvidia&lt;<span class="built_in">id</span>&gt;</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">                     Users     PID         Command</span><br><span class="line">/dev/nvidia5:        XXXXXX    14701 F...m python</span><br><span class="line">                     XXXXXX    14703 F...m python</span><br><span class="line">                     XXXXXX    14705 F...m python</span><br><span class="line">                     XXXXXX    14706 F...m python</span><br><span class="line">                     XXXXXX    37041 F...m python</span><br><span class="line">                     XXXXXX    37053 F...m python</span><br></pre></td></tr></table></figure>

<p>This will list all of the processes that use GPU. Note that if this is executed from a <strong>normal user</strong>, then only the user’s processes displayed. If this is executed from <strong>root</strong>, then all user’s relevant processes will be displayed.</p>
<ol start="2">
<li>Then use the following command to kill the process shown above.</li>
</ol>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">kill</span> -9 [PID]</span><br></pre></td></tr></table></figure>

<p>That will kill the process on the GPU. After killing the processes, you will find the GPU memory is freed. If still occupied, this may be caused by other users. You need to ask other users&#x2F;administrators to kill it manually.</p>
]]></content>
      <categories>
        <category>Problem Solving</category>
      </categories>
  </entry>
  <entry>
    <title>Common LaTeX Blocks Templates</title>
    <url>/2023/05/02/common-latex-templates/</url>
    <content><![CDATA[<h2 id="1-Set-geometries-boundary"><a href="#1-Set-geometries-boundary" class="headerlink" title="1. Set geometries (boundary)"></a>1. Set geometries (boundary)</h2><figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\usepackage</span>&#123;geometry&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">\geometry</span>&#123;a4paper, top = 1.25in, bottom = 1.25in, left = 1.25in, right = 1.25in, headheight = 1.25in&#125;</span><br></pre></td></tr></table></figure>
<h2 id="2-Figures"><a href="#2-Figures" class="headerlink" title="2. Figures"></a>2. Figures</h2><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\usepackage</span>&#123;float&#125;</span><br><span class="line"><span class="keyword">\usepackage</span>&#123;subcaption&#125;</span><br><span class="line"><span class="keyword">\usepackage</span>&#123;graphicx&#125; </span><br></pre></td></tr></table></figure>
<p><img src="/2023/05/02/common-latex-templates/figures.png" alt="figures" style="zoom:60%;"></p>
<h3 id="2-1-Single-Figure"><a href="#2-1-Single-Figure" class="headerlink" title="2.1 Single Figure:"></a>2.1 Single Figure:</h3><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\begin</span>&#123;figure&#125;[H]</span><br><span class="line">    <span class="keyword">\centering</span></span><br><span class="line">    <span class="keyword">\includegraphics</span>[width=0.35<span class="keyword">\textwidth</span>]&#123;blankfig.png&#125;</span><br><span class="line">    <span class="keyword">\caption</span>&#123;XXXXXX&#125;</span><br><span class="line">    <span class="comment">% \label&#123;XXXX&#125;</span></span><br><span class="line"><span class="keyword">\end</span>&#123;figure&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-2-Two-Independent-Figures-in-One-Row"><a href="#2-2-Two-Independent-Figures-in-One-Row" class="headerlink" title="2.2 Two Independent Figures in One Row"></a>2.2 Two Independent Figures in One Row</h3><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\begin</span>&#123;figure&#125;[H]<span class="comment">%[htp]</span></span><br><span class="line"><span class="keyword">\centering</span></span><br><span class="line">    <span class="keyword">\begin</span>&#123;minipage&#125;[t]&#123;0.47<span class="keyword">\textwidth</span>&#125;</span><br><span class="line">        <span class="keyword">\centering</span></span><br><span class="line">        <span class="keyword">\includegraphics</span>[width=1<span class="keyword">\textwidth</span>]&#123;blankfig.png&#125;</span><br><span class="line">        <span class="keyword">\caption</span>&#123;XXXX&#125;</span><br><span class="line">    <span class="comment">% \label&#123;XXXX&#125;</span></span><br><span class="line">    <span class="keyword">\end</span>&#123;minipage&#125;</span><br><span class="line">    <span class="keyword">\begin</span>&#123;minipage&#125;[t]&#123;0.47<span class="keyword">\textwidth</span>&#125;</span><br><span class="line">        <span class="keyword">\centering</span></span><br><span class="line">        <span class="keyword">\includegraphics</span>[width=1<span class="keyword">\textwidth</span>]&#123;blankfig.png&#125;</span><br><span class="line">        <span class="keyword">\caption</span>&#123;XXXX&#125;</span><br><span class="line">        <span class="comment">% \label&#123;XXXX&#125;</span></span><br><span class="line">    <span class="keyword">\end</span>&#123;minipage&#125;</span><br><span class="line"><span class="keyword">\end</span>&#123;figure&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-3-Two-Subfigures-in-One-Row"><a href="#2-3-Two-Subfigures-in-One-Row" class="headerlink" title="2.3 Two Subfigures in One Row:"></a>2.3 Two Subfigures in One Row:</h3><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\begin</span>&#123;figure&#125;[H]</span><br><span class="line">     <span class="keyword">\centering</span></span><br><span class="line">     <span class="keyword">\begin</span>&#123;subfigure&#125;[b]&#123;0.47<span class="keyword">\textwidth</span>&#125;</span><br><span class="line">         <span class="keyword">\centering</span></span><br><span class="line">         <span class="keyword">\includegraphics</span>[width=<span class="keyword">\textwidth</span>]&#123;blankfig.png&#125;</span><br><span class="line">         <span class="keyword">\caption</span>&#123;xxxx&#125;</span><br><span class="line">         <span class="comment">% \label&#123;xxxx&#125;</span></span><br><span class="line">     <span class="keyword">\end</span>&#123;subfigure&#125;</span><br><span class="line">     <span class="comment">% \hfill</span></span><br><span class="line">     <span class="keyword">\begin</span>&#123;subfigure&#125;[b]&#123;0.47<span class="keyword">\textwidth</span>&#125;</span><br><span class="line">         <span class="keyword">\centering</span></span><br><span class="line">         <span class="keyword">\includegraphics</span>[width=<span class="keyword">\textwidth</span>]&#123;blankfig.png&#125;</span><br><span class="line">         <span class="keyword">\caption</span>&#123;xxxx&#125;</span><br><span class="line">         <span class="comment">% \label&#123;xxxx&#125;</span></span><br><span class="line">     <span class="keyword">\end</span>&#123;subfigure&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">\caption</span>&#123;XXXX&#125;</span><br><span class="line">    <span class="comment">% \label&#123;xxxx&#125;</span></span><br><span class="line"><span class="keyword">\end</span>&#123;figure&#125;</span><br></pre></td></tr></table></figure>
<h2 id="3-Pseudo-codes"><a href="#3-Pseudo-codes" class="headerlink" title="3. Pseudo-codes"></a>3. Pseudo-codes</h2><figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\usepackage</span>[ruled, vlined, linesnumbered]&#123;algorithm2e&#125;</span><br></pre></td></tr></table></figure>
<p><img src="/2023/05/02/common-latex-templates/pseudo-codes.png" alt="pseudo-codes" style="zoom:60%;"></p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\begin</span>&#123;algorithm&#125;[H]</span><br><span class="line">	<span class="keyword">\caption</span>&#123;Residual&#x27;s Distribution Simulation&#125;</span><br><span class="line">	<span class="keyword">\BlankLine</span></span><br><span class="line">    <span class="keyword">\SetAlgoLined</span></span><br><span class="line">	<span class="keyword">\KwIn</span>&#123;Number of sample needed (<span class="built_in">$</span>num<span class="built_in">_</span>&#123;sample&#125;<span class="built_in">$</span>), <span class="built_in">$</span>w<span class="built_in">_</span>1<span class="built_in">$</span>,<span class="built_in">$</span>w<span class="built_in">_</span>2<span class="built_in">$</span>,<span class="built_in">$</span>w<span class="built_in">_</span>3<span class="built_in">$</span>,<span class="built_in">$</span><span class="keyword">\mu</span><span class="built_in">_</span>1<span class="built_in">$</span>,<span class="built_in">$</span><span class="keyword">\mu</span><span class="built_in">_</span>2<span class="built_in">$</span>,<span class="built_in">$</span><span class="keyword">\mu</span><span class="built_in">_</span>3<span class="built_in">$</span>,<span class="built_in">$</span><span class="keyword">\sigma</span><span class="built_in">_</span>1<span class="built_in">$</span>,<span class="built_in">$</span><span class="keyword">\sigma</span><span class="built_in">_</span>2<span class="built_in">$</span>,<span class="built_in">$</span><span class="keyword">\sigma</span><span class="built_in">_</span>3<span class="built_in">$</span>.&#125;</span><br><span class="line">	<span class="keyword">\KwOut</span>&#123;A sequence of random variables <span class="built_in">$</span><span class="keyword">\&#123;</span>X<span class="built_in">_</span>i<span class="keyword">\&#125;</span><span class="built_in">_</span>&#123;i = 1&#125;<span class="built_in">^</span>&#123;num<span class="built_in">_</span>&#123;sample&#125;&#125;<span class="built_in">$</span> following target distribution.&#125; </span><br><span class="line"></span><br><span class="line">	<span class="keyword">\BlankLine</span></span><br><span class="line">    i = 0<span class="keyword">\\</span></span><br><span class="line">    <span class="keyword">\While</span>&#123;<span class="built_in">$</span> i &lt; num<span class="built_in">_</span>&#123;sample&#125; <span class="built_in">$</span>&#125;&#123;</span><br><span class="line">        <span class="built_in">$</span>v<span class="built_in">_</span>i<span class="built_in">$</span> <span class="built_in">$</span><span class="keyword">\sim</span> <span class="keyword">\mathcal</span>&#123;U&#125;[0,1]<span class="built_in">$</span><span class="keyword">\\</span></span><br><span class="line">        <span class="keyword">\uIf</span>&#123;<span class="built_in">$</span>0&lt;v<span class="built_in">_</span>i <span class="keyword">\leq</span> w<span class="built_in">_</span>1<span class="built_in">$</span>&#125;&#123;</span><br><span class="line">            <span class="built_in">$</span>X<span class="built_in">_</span>i <span class="keyword">\sim</span> <span class="keyword">\mathcal</span>&#123;N&#125;(<span class="keyword">\mu</span><span class="built_in">_</span>1,<span class="keyword">\sigma</span><span class="built_in">_</span>1<span class="built_in">^</span>2)<span class="built_in">$</span> <span class="keyword">\;</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">\uElseIf</span>&#123;<span class="built_in">$</span>w<span class="built_in">_</span>1&lt;v<span class="built_in">_</span>i<span class="keyword">\leq</span> w<span class="built_in">_</span>2<span class="built_in">$</span>&#125;&#123;</span><br><span class="line">            <span class="built_in">$</span>X<span class="built_in">_</span>i <span class="keyword">\sim</span> <span class="keyword">\mathcal</span>&#123;N&#125;(<span class="keyword">\mu</span><span class="built_in">_</span>2,<span class="keyword">\sigma</span><span class="built_in">_</span>2<span class="built_in">^</span>2)<span class="built_in">$</span> <span class="keyword">\;</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">\Else</span>&#123;</span><br><span class="line">            <span class="built_in">$</span>X<span class="built_in">_</span>i <span class="keyword">\sim</span> <span class="keyword">\mathcal</span>&#123;N&#125;(<span class="keyword">\mu</span><span class="built_in">_</span>3,<span class="keyword">\sigma</span><span class="built_in">_</span>3<span class="built_in">^</span>2)<span class="built_in">$</span> <span class="keyword">\;</span></span><br><span class="line">        &#125;</span><br><span class="line">        i = i + 1</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">\Return</span>&#123;<span class="built_in">$</span><span class="keyword">\&#123;</span>X<span class="built_in">_</span>i<span class="keyword">\&#125;</span><span class="built_in">_</span>&#123;i = 1&#125;<span class="built_in">^</span>&#123;num<span class="built_in">_</span>&#123;sample&#125;&#125;<span class="built_in">$</span>    &#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">\BlankLine</span></span><br><span class="line"><span class="keyword">\end</span>&#123;algorithm&#125;</span><br></pre></td></tr></table></figure>
<h2 id="4-Tables"><a href="#4-Tables" class="headerlink" title="4. Tables"></a>4. Tables</h2><figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\usepackage</span>&#123;tabularx&#125;</span><br><span class="line"><span class="keyword">\usepackage</span>&#123;booktabs&#125;</span><br><span class="line"><span class="keyword">\usepackage</span>&#123;threeparttable&#125;</span><br></pre></td></tr></table></figure>
<p><img src="/2023/05/02/common-latex-templates/tables.png" alt="tables" style="zoom:60%;"></p>
<h3 id="4-1-Narrow-table"><a href="#4-1-Narrow-table" class="headerlink" title="4.1 Narrow table"></a>4.1 Narrow table</h3><figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\begin</span>&#123;table&#125;[H]</span><br><span class="line">    <span class="keyword">\centering</span><span class="keyword">\small</span></span><br><span class="line">    <span class="keyword">\setlength</span>&#123;<span class="keyword">\tabcolsep</span>&#125;&#123;3mm&#125;&#123;</span><br><span class="line">        <span class="keyword">\caption</span>&#123;XXXX&#125;</span><br><span class="line">        <span class="keyword">\begin</span>&#123;tabular&#125;&#123;cccc&#125;</span><br><span class="line">            <span class="keyword">\specialrule</span>&#123;0.05em&#125;&#123;3pt&#125;&#123;3pt&#125;</span><br><span class="line">            <span class="keyword">\toprule</span></span><br><span class="line">            X <span class="built_in">&amp;</span> X <span class="built_in">&amp;</span> X <span class="built_in">&amp;</span> X  <span class="keyword">\\</span></span><br><span class="line">            <span class="keyword">\midrule</span></span><br><span class="line">            XXX <span class="built_in">&amp;</span> 0.928 <span class="built_in">&amp;</span> 0.2935 <span class="built_in">&amp;</span> 1.000 <span class="keyword">\\</span></span><br><span class="line">            XXX <span class="built_in">&amp;</span> 0.747 <span class="built_in">&amp;</span> 0.0526 <span class="built_in">&amp;</span> 1.301 <span class="keyword">\\</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">\specialrule</span>&#123;0.05em&#125;&#123;3pt&#125;&#123;3pt&#125; </span><br><span class="line">        <span class="keyword">\bottomrule</span></span><br><span class="line">        <span class="keyword">\label</span>&#123;tab:compare2&#125;</span><br><span class="line">    	<span class="keyword">\end</span>&#123;tabular&#125;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">\end</span>&#123;table&#125;</span><br></pre></td></tr></table></figure>
<h3 id="4-2-Text-width-table"><a href="#4-2-Text-width-table" class="headerlink" title="4.2 Text-width table"></a>4.2 Text-width table</h3><p>The width may be adjusted manually.</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\begin</span>&#123;table&#125;[H]</span><br><span class="line"><span class="keyword">\centering</span></span><br><span class="line">    <span class="keyword">\caption</span>&#123;Experiment results for different &quot;joins&quot;&#125;</span><br><span class="line">    <span class="keyword">\label</span>&#123;X&#125;</span><br><span class="line">    <span class="keyword">\begin</span>&#123;tabularx&#125;&#123;<span class="keyword">\textwidth</span>&#125;&#123;X X X&#125;</span><br><span class="line">    <span class="keyword">\toprule</span></span><br><span class="line">    Header 1 <span class="built_in">&amp;</span> Header 2 <span class="built_in">&amp;</span> Header 3 <span class="keyword">\\</span></span><br><span class="line">    <span class="keyword">\midrule</span></span><br><span class="line">    Data 1   <span class="built_in">&amp;</span> Data 2   <span class="built_in">&amp;</span> Data 3   <span class="keyword">\\</span></span><br><span class="line">    Data 4   <span class="built_in">&amp;</span> Data 5   <span class="built_in">&amp;</span> Data 6   <span class="keyword">\\</span></span><br><span class="line">    <span class="keyword">\bottomrule</span></span><br><span class="line">    <span class="keyword">\end</span>&#123;tabularx&#125;</span><br><span class="line">    </span><br><span class="line"><span class="keyword">\end</span>&#123;table&#125;</span><br></pre></td></tr></table></figure>
<h3 id="4-3-With-Footnote"><a href="#4-3-With-Footnote" class="headerlink" title="4.3 With Footnote"></a>4.3 With Footnote</h3><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\begin</span>&#123;table&#125;[htb]</span><br><span class="line"><span class="keyword">\centering</span></span><br><span class="line">    <span class="keyword">\caption</span>&#123;Final experiment results.&#125;</span><br><span class="line">    <span class="keyword">\label</span>&#123;Table:finalresults&#125;</span><br><span class="line">    <span class="keyword">\begin</span>&#123;tabularx&#125;&#123;<span class="keyword">\textwidth</span>&#125;&#123;X X X&#125;</span><br><span class="line">        <span class="keyword">\toprule</span></span><br><span class="line">        Header 1<span class="built_in">$</span><span class="built_in">^</span>a<span class="built_in">$</span> <span class="built_in">&amp;</span> Header 2 <span class="built_in">&amp;</span> Header 3 <span class="keyword">\\</span></span><br><span class="line">        <span class="keyword">\midrule</span></span><br><span class="line">        Data 1   <span class="built_in">&amp;</span> Data 2<span class="built_in">$</span><span class="built_in">^</span>b<span class="built_in">$</span>   <span class="built_in">&amp;</span> Data 3   <span class="keyword">\\</span></span><br><span class="line">        Data 4   <span class="built_in">&amp;</span> Data 5   <span class="built_in">&amp;</span> Data 6<span class="built_in">$</span><span class="built_in">^</span>c<span class="built_in">$</span>   <span class="keyword">\\</span></span><br><span class="line">        <span class="keyword">\bottomrule</span></span><br><span class="line">    <span class="keyword">\end</span>&#123;tabularx&#125;</span><br><span class="line">    <span class="keyword">\begin</span>&#123;tablenotes&#125;</span><br><span class="line">    <span class="keyword">\item</span>[1]  <span class="keyword">\scriptsize</span> a. Footnote a.</span><br><span class="line">    <span class="keyword">\item</span>[2]  <span class="keyword">\scriptsize</span> b. Footnote b. </span><br><span class="line">    <span class="keyword">\item</span>[3]  <span class="keyword">\scriptsize</span> c. Footnote c. </span><br><span class="line">    <span class="keyword">\end</span>&#123;tablenotes&#125;</span><br><span class="line"><span class="keyword">\vspace</span>&#123;-1.25em&#125;</span><br><span class="line"><span class="keyword">\end</span>&#123;table&#125;</span><br></pre></td></tr></table></figure>
<h2 id="5-Listing-Code-blocks"><a href="#5-Listing-Code-blocks" class="headerlink" title="5. Listing (Code blocks)"></a>5. Listing (Code blocks)</h2><figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\usepackage</span>&#123;listings&#125;</span><br></pre></td></tr></table></figure>
<h3 id="5-1-SQL-Style-Setting"><a href="#5-1-SQL-Style-Setting" class="headerlink" title="5.1 SQL Style Setting"></a>5.1 SQL Style Setting</h3><p><img src="/2023/05/02/common-latex-templates/listings.png" alt="listings" style="zoom:60%;"></p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="comment">% outside of document</span></span><br><span class="line"><span class="keyword">\lstset</span>&#123;</span><br><span class="line">    language=SQL,    </span><br><span class="line">    basicstyle = <span class="keyword">\tiny</span><span class="keyword">\ttfamily</span>,</span><br><span class="line">    breaklines=true,</span><br><span class="line">    numberstyle=<span class="keyword">\tiny</span>,keywordstyle=<span class="keyword">\color</span>&#123;blue!70&#125;,</span><br><span class="line">    commentstyle=<span class="keyword">\color</span>&#123;red!50!green!50!blue!50&#125;,frame=shadowbox,</span><br><span class="line">    columns=flexible,</span><br><span class="line">    rulesepcolor=<span class="keyword">\color</span>&#123;red!20!green!20!blue!20&#125;,basicstyle=<span class="keyword">\ttfamily</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">% inside of document</span></span><br><span class="line"><span class="keyword">\begin</span>&#123;lstlisting&#125;</span><br><span class="line">    SELECT u.province, c.chip<span class="built_in">_</span>name AS ChipName, SUM(p.budget) AS revenue</span><br><span class="line">    FROM user AS u NATURAL JOIN package AS p, chip AS c</span><br><span class="line">    WHERE p.package<span class="built_in">_</span>id=c.package<span class="built_in">_</span>id AND province IN (<span class="comment">%s)</span></span><br><span class="line">    GROUP BY c.chip<span class="built_in">_</span>name</span><br><span class="line">    ORDER BY SUM(p.budget) DESC; </span><br><span class="line"><span class="keyword">\end</span>&#123;lstlisting&#125;</span><br></pre></td></tr></table></figure>
<h3 id="5-2-Python-Style-Setting"><a href="#5-2-Python-Style-Setting" class="headerlink" title="5.2 Python Style Setting"></a>5.2 Python Style Setting</h3><p><img src="/2023/05/02/common-latex-templates/listing2.png" alt="listing2" style="zoom:60%;"></p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="comment">% outside of document</span></span><br><span class="line"><span class="keyword">\lstset</span>&#123;</span><br><span class="line">  language=Python,</span><br><span class="line">  basicstyle=<span class="keyword">\small</span><span class="keyword">\ttfamily</span>,</span><br><span class="line">  commentstyle=<span class="keyword">\color</span>&#123;gray&#125;,</span><br><span class="line">  keywordstyle=<span class="keyword">\color</span>&#123;blue&#125;<span class="keyword">\bfseries</span>,</span><br><span class="line">  stringstyle=<span class="keyword">\color</span>&#123;red&#125;,</span><br><span class="line">  showstringspaces=false,</span><br><span class="line">  numbers=left,</span><br><span class="line">  numberstyle=<span class="keyword">\tiny</span><span class="keyword">\color</span>&#123;gray&#125;,</span><br><span class="line">  stepnumber=1,</span><br><span class="line">  numbersep=10pt,</span><br><span class="line">  tabsize=4,</span><br><span class="line">  showspaces=false,</span><br><span class="line">  showtabs=false,</span><br><span class="line">  breaklines=true,</span><br><span class="line">  breakatwhitespace=true,</span><br><span class="line">  aboveskip=<span class="keyword">\bigskipamount</span>,</span><br><span class="line">  belowskip=<span class="keyword">\bigskipamount</span>,</span><br><span class="line">  frame=single</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">% inside of document</span></span><br><span class="line"><span class="keyword">\begin</span>&#123;lstlisting&#125;</span><br><span class="line">    print(&quot;hello world!&quot;)</span><br><span class="line">    for qid in range(hs.shape[0]):</span><br><span class="line">    if qid &lt; 1:</span><br><span class="line">        lvl = 0</span><br><span class="line">    elif qid &gt;= 1 and qid &lt; 3:</span><br><span class="line">        lvl = 1</span><br><span class="line">    elif qid &gt;= 3 and qid &lt; 6:</span><br><span class="line">        lvl = 2</span><br><span class="line">    elif qid &gt;= 6 and qid &lt; 11:</span><br><span class="line">        lvl = 3</span><br><span class="line"><span class="keyword">\end</span>&#123;lstlisting&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Techniques</category>
      </categories>
  </entry>
  <entry>
    <title>Pytorch C packages CUDA Installation Mismatch Problems</title>
    <url>/2023/04/27/pytorch-C-packages-installation-failed-problems/</url>
    <content><![CDATA[<p>When installing some non-pip installed packages, especially in the deep learning field, we may use <code>python setup.py build install</code> to build the packages locally. Then, some typical problems may happen in this stage. An CUDA mismatch error may be:</p>
<p><img src="/2023/04/27/pytorch-C-packages-installation-failed-problems/problem.png" alt="problem"></p>
<p>This error can be caused for many reasons. I just report my situation and how do I solve it. </p>
<h2 id="Why-this-happen"><a href="#Why-this-happen" class="headerlink" title="Why this happen?"></a>Why this happen?</h2><p>Some packages need to be compiled by the local CUDA compilers and to be installed locally. Then, those packages cooperate with the pytorch in the conda environment. Therefore, they need to be a compiled with the same version (at least same major version, like <code>cuda 11.x</code>) CUDA compilers. </p>
<ul>
<li>First, we inspect the conda environment’s pytorch’s CUDA version by:</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.version.cuda</span><br><span class="line"><span class="string">&#x27;11.3&#x27;</span></span><br></pre></td></tr></table></figure>

<p>This means that our pytorch is compiled by cuda <code>11.3</code>. (Same as the error message above!)</p>
<ul>
<li>Then, we inspect the system’s CUDA compiler version by:</li>
</ul>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">nvcc -V</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">nvcc: NVIDIA (R) Cuda compiler driver</span><br><span class="line">Copyright (c) 2005-2019 NVIDIA Corporation</span><br><span class="line">Built on Wed_Oct_23_19:24:38_PDT_2019</span><br><span class="line">Cuda compilation tools, release 10.2, V10.2.89</span><br></pre></td></tr></table></figure>

<p>This means that our system’s current CUDA version is <code>10.2</code>. (Same as the error message above!)</p>
<p>Therefore, the compiler version going to compile the package is NOT consistent with the compiler compiled pytorch. The Error is reported.</p>
<h2 id="How-to-solve-it"><a href="#How-to-solve-it" class="headerlink" title="How to solve it?"></a>How to solve it?</h2><p>So to solve this problem, the easiest way is to install a new CUDA with corresponding version. (In my test, I don’t need to install an exact <code>11.3</code> version, only an <code>11.1</code> version is OK)</p>
<ol>
<li>Install the CUDA with specific version. Many installation tutorials can be found online (skipped)</li>
<li>Export the new path in <code>~/.bashrc</code>: Add following command at the end of <code>~/.bashrc</code>:</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=/usr/local/cuda-&lt;YOUR VERSION&gt;/bin:<span class="variable">$PATH</span></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=/usr/local/cuda-&lt;YOUR VERSION&gt;/lib64:<span class="variable">$&#123;LD_LIBRARY_PATH&#125;</span></span><br></pre></td></tr></table></figure>

<p>(Remember to change &lt;YOUR VERSION&gt; above to your CUDA version!!)</p>
<ol start="3">
<li>Open a new terminal, type in:</li>
</ol>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">nvcc -V</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">nvcc: NVIDIA (R) Cuda compiler driver</span><br><span class="line">Copyright (c) 2005-2020 NVIDIA Corporation</span><br><span class="line">Built on Mon_Oct_12_20:09:46_PDT_2020</span><br><span class="line">Cuda compilation tools, release 11.1, V11.1.105</span><br><span class="line">Build cuda_11.1.TC455_06.29190527_0</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>Then it should work! Go to the installation directory, and then switch to the target conda environment, and install!</li>
</ol>
<h2 id="Common-techniques-for-debugging"><a href="#Common-techniques-for-debugging" class="headerlink" title="Common techniques for debugging"></a>Common techniques for debugging</h2><ul>
<li>Inspecting the pytorch’s CUDA version:</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.version.cuda</span><br><span class="line"><span class="string">&#x27;11.3&#x27;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>Inspecting the system’s CUDA compiler version:</li>
</ul>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">nvcc -V</span><br></pre></td></tr></table></figure>

<p>or</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> torch.utils.cpp_extension <span class="keyword">import</span> CUDA_HOME</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>CUDA_HOME</span><br><span class="line"><span class="string">&#x27;/usr/local/cuda-11.1&#x27;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>Change the <code>$PATH</code> variable, so the new CUDA can be found:</li>
</ul>
<p>Add following command at the end of <code>~/.bashrc</code>:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=/usr/local/cuda-11.1/bin:<span class="variable">$PATH</span></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=/usr/local/cuda-11.1/lib64:<span class="variable">$LD_LIBRARY_PATH</span></span><br></pre></td></tr></table></figure>

<p>REMEMBER to change the CUDA version to your version.</p>
<ul>
<li>Delete the cached installing data</li>
</ul>
<p>In some situation, when we modified the compiler, we shall build the package from scratch. </p>
<p><strong>Remove</strong> any of the build, cached, dist, temp directory! E.g., the <code>build</code> and <code>DCNv3.egg-info</code> and <code>dist</code> directory below.</p>
<img src="/2023/04/27/pytorch-C-packages-installation-failed-problems/package.png" alt="package" style="zoom:60%;"> 

<p>(But be careful that don’t remove the source code!!!)</p>
]]></content>
      <categories>
        <category>Problem Solving</category>
      </categories>
  </entry>
  <entry>
    <title>signal system with python</title>
    <url>/2023/04/04/signal-system-with-python/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>PyTorch collate_fn and Python zip function</title>
    <url>/2023/03/30/collate-fn-and-zip/</url>
    <content><![CDATA[<p>Part of the article is generated by [ChatGPT]</p>
<h2 id="collate-fn"><a href="#collate-fn" class="headerlink" title="collate_fn"></a>collate_fn</h2><p>This post records about the <code>collate_fn</code> from <code>torch.util.data.DataLoader</code> and the python built-in function <code>zip</code>. </p>
<p>Each batch, the dataloader collects <code>batch_size</code> number of items. They are picked from the dataset one by one. So currently, the batch data is <code>[(data1, target1), (data2, target2), ..., (dataN, targetN)]</code>. </p>
<p>The default <code>collate_fn</code> would change it into <code>[torch.tensor([data1, data2, ..., dataN]), torch.tensor([target1, target2, ..., targetN])]</code>. </p>
<p>However, in some NLP tasks, the data is not in the same length. So we need to apply <code>torch.nn.utils.rnn.pad_sequence</code> to make each data same length (usually the maximum length in this batch). A typical implementation is:</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">collate_fn_train</span>(<span class="params">batch</span>):</span><br><span class="line">    x, y = <span class="built_in">zip</span>(*batch)</span><br><span class="line">    x_pad = pad_sequence(x, batch_first=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># y = torch.Tensor(y) # optional</span></span><br><span class="line">    <span class="keyword">return</span> x_pad, y</span><br></pre></td></tr></table></figure>



<h2 id="zip-and"><a href="#zip-and" class="headerlink" title="zip and *"></a>zip and *</h2><ul>
<li>What does the <code>zip</code> do in the above function?</li>
</ul>
<p>The <code>zip()</code> function in Python is a built-in function that takes one or more iterables (such as lists, tuples, or strings) and “zips” them together, returning an iterator of tuples where the i-th tuple contains the i-th element from each of the input iterables.</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">b = [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>]</span><br><span class="line">c = [<span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">True</span>]</span><br><span class="line"></span><br><span class="line">zipped = <span class="built_in">zip</span>(a, b, c)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(zipped))</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">[(<span class="number">1</span>, <span class="string">&#x27;a&#x27;</span>, <span class="literal">True</span>), (<span class="number">2</span>, <span class="string">&#x27;b&#x27;</span>, <span class="literal">False</span>), (<span class="number">3</span>, <span class="string">&#x27;c&#x27;</span>, <span class="literal">True</span>)]</span><br></pre></td></tr></table></figure>

<ul>
<li>What is the <code>*</code>?</li>
</ul>
<p>In Python, the asterisk (*) symbol can be used to unpack iterables like lists or tuples. When used in this way, the asterisk is sometimes called the “splat” operator or the “unpacking” operator. The unpacking operator is used to extract the individual elements from an iterable and assign them to separate variables.</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">my_list = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"><span class="built_in">print</span>(*my_list)</span><br><span class="line"><span class="comment"># equals</span></span><br><span class="line"><span class="built_in">print</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line"><span class="number">1</span> <span class="number">2</span> <span class="number">3</span></span><br></pre></td></tr></table></figure>

<ul>
<li>So, what is the <code>x, y = zip(*batch)</code>?</li>
</ul>
<p>First, <code>batch</code> is <code>[(data1, target1), (data2, target2), ..., (dataN, targetN)]</code>.</p>
<p><code>*batch</code> would unpack the outmost list, to be <code>zip((data1, target1), (data2, target2), ..., (dataN, targetN))</code>. The result would be two tuples, <code>[data1, data2, ..., dataN]</code> and <code>[target1, target2, ..., targetN]</code>. The former one is assigned to <code>x</code> and the other is assigned to <code>y</code>.</p>
<p>In this way, we get separate data structure, that contains all <code>data</code> and <code>target</code> respectively.  </p>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title>Convert Python Dictionary to Source Code of Latex and Markdown</title>
    <url>/2023/03/30/dict-to-latex-md/</url>
    <content><![CDATA[<p>The code is generated by [ChatGPT].</p>
<p>Here are two common functions, that can convert the python built-in dictionary into the format of latex&#x2F;markdown unnumbered list, so that we can copy it into the latex&#x2F;markdown. Example can be:</p>
<p>The input python dictionary is:</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">my_dict = &#123;</span><br><span class="line">    <span class="string">&quot;fruits&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;apples&quot;</span>: <span class="string">&quot;red&quot;</span>,</span><br><span class="line">        <span class="string">&quot;bananas&quot;</span>: <span class="string">&quot;yellow&quot;</span>,</span><br><span class="line">        <span class="string">&quot;grapes&quot;</span>: <span class="string">&quot;purple&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&quot;vegetables&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;carrots&quot;</span>: <span class="string">&quot;orange&quot;</span>,</span><br><span class="line">        <span class="string">&quot;spinach&quot;</span>: <span class="string">&quot;green&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&quot;meat&quot;</span>: <span class="string">&quot;beef&quot;</span>,</span><br><span class="line">    <span class="string">&quot;dairy&quot;</span>: <span class="string">&quot;milk&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>The result for markdown is:</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line"><span class="bullet">-</span> fruits</span><br><span class="line"><span class="bullet">  -</span> apples</span><br><span class="line"><span class="bullet">  -</span> bananas</span><br><span class="line"><span class="bullet">  -</span> grapes</span><br><span class="line"><span class="bullet">-</span> vegetables</span><br><span class="line"><span class="bullet">  -</span> carrots</span><br><span class="line"><span class="bullet">  -</span> spinach</span><br><span class="line"><span class="bullet">-</span> meat</span><br><span class="line"><span class="bullet">  -</span> beef</span><br><span class="line"><span class="bullet">-</span> dairy</span><br><span class="line"><span class="bullet">  -</span> milk</span><br></pre></td></tr></table></figure>

<p>and result for LaTeX is:</p>
<figure class="highlight latex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\item</span> fruits</span><br><span class="line"><span class="keyword">\begin</span>&#123;itemize&#125;</span><br><span class="line">  <span class="keyword">\item</span> apples: red</span><br><span class="line">  <span class="keyword">\item</span> bananas: yellow</span><br><span class="line">  <span class="keyword">\item</span> grapes: purple</span><br><span class="line"><span class="keyword">\end</span>&#123;itemize&#125;</span><br><span class="line"><span class="keyword">\item</span> vegetables</span><br><span class="line"><span class="keyword">\begin</span>&#123;itemize&#125;</span><br><span class="line">  <span class="keyword">\item</span> carrots: orange</span><br><span class="line">  <span class="keyword">\item</span> spinach: green</span><br><span class="line"><span class="keyword">\end</span>&#123;itemize&#125;</span><br><span class="line"><span class="keyword">\item</span> meat: beef</span><br><span class="line"><span class="keyword">\item</span> dairy: milk</span><br></pre></td></tr></table></figure>



<p>Function explanation: it is as simple as it shows. We use the recursion strategy here. When we are going to print a sub-dictionary, we recursively call the function, with more (two) spaces indented.</p>
<h2 id="Dict-to-latex-unnumbered-list"><a href="#Dict-to-latex-unnumbered-list" class="headerlink" title="Dict to latex unnumbered list"></a>Dict to latex unnumbered list</h2><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dict_to_latex</span>(<span class="params">d, level=<span class="number">0</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Converts a dictionary to a string in LaTeX format for an unnumbered list.</span></span><br><span class="line"><span class="string">    Nested dictionaries are considered as second level items.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">    - d (dict): The dictionary to be converted.</span></span><br><span class="line"><span class="string">    - level (int): The current nesting level (defaults to 0).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - A string in LaTeX format for an unnumbered list.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    latex_str = <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> d.items():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(k, <span class="built_in">str</span>):</span><br><span class="line">            k = k.replace(<span class="string">&#x27;_&#x27;</span>, <span class="string">&#x27;\_&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(v, <span class="built_in">str</span>):</span><br><span class="line">            v = v.replace(<span class="string">&#x27;_&#x27;</span>, <span class="string">&#x27;\_&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(v, <span class="built_in">dict</span>):</span><br><span class="line">            latex_str += <span class="string">f&quot;<span class="subst">&#123;<span class="string">&#x27;  &#x27;</span>*level&#125;</span>\\item <span class="subst">&#123;k&#125;</span>\n\\begin&#123;&#123;itemize&#125;&#125;\n<span class="subst">&#123;dict_to_latex(v, level+<span class="number">1</span>)&#125;</span>\\end&#123;&#123;itemize&#125;&#125;\n&quot;</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            latex_str += <span class="string">f&quot;<span class="subst">&#123;<span class="string">&#x27;  &#x27;</span>*level&#125;</span>\\item <span class="subst">&#123;k&#125;</span>: <span class="subst">&#123;v&#125;</span>\n&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> latex_str</span><br></pre></td></tr></table></figure>

<p>Note that we should avoid the <code>_</code> in our string. So we need first convert it into escape character, <code>\_</code>.</p>
<p>The result of the string should be wrapped by a <code>\begin&#123;itemize&#125; \end&#123;itemize&#125;</code>.</p>
<h2 id="Dict-to-markdown-unnumbered-list"><a href="#Dict-to-markdown-unnumbered-list" class="headerlink" title="Dict to markdown unnumbered list"></a>Dict to markdown unnumbered list</h2><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dict_to_markdown</span>(<span class="params">d, level=<span class="number">0</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Converts a dictionary to a string in Markdown format for an unnumbered list.</span></span><br><span class="line"><span class="string">    Nested dictionaries are considered as second level items.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">    - d (dict): The dictionary to be converted.</span></span><br><span class="line"><span class="string">    - level (int): The current nesting level (defaults to 0).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - A string in Markdown format for an unnumbered list.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    md_str = <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> d.items():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(v, <span class="built_in">dict</span>):</span><br><span class="line">            md_str += <span class="string">f&quot;<span class="subst">&#123;<span class="string">&#x27;  &#x27;</span>*level&#125;</span>- <span class="subst">&#123;k&#125;</span>\n<span class="subst">&#123;dict_to_markdown(v, level+<span class="number">1</span>)&#125;</span>&quot;</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            md_str += <span class="string">f&quot;<span class="subst">&#123;<span class="string">&#x27;  &#x27;</span>*level&#125;</span>- <span class="subst">&#123;k&#125;</span>: <span class="subst">&#123;v&#125;</span>\n&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> md_str</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Techniques</category>
      </categories>
  </entry>
  <entry>
    <title>C/C++ 复杂类型声明规则（2）-识读const关键字</title>
    <url>/2023/03/24/cpp-declare-2/</url>
    <content><![CDATA[<p>“C&#x2F;C++ 复杂类型声明规则” 将分为两节。第一节介绍螺旋法则声明规范，第二节介绍const的修饰关系。</p>
<p>本节主要解决以下问题：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">int</span> *a;  <span class="comment">// a可变嘛? *a可变嘛?</span></span><br><span class="line"><span class="type">int</span> * <span class="type">const</span> a; <span class="comment">// a可变嘛? *a可变嘛?</span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> * <span class="type">const</span> a; <span class="comment">// a可变嘛? *a可变嘛?</span></span><br></pre></td></tr></table></figure>

<p>这两节内容，我都尝试以一种不用死记硬背，而是理解的方式去掌握。</p>
<p>本文参考自【初中生也能看懂的C&#x2F;C++类型声明规则教学，很简单的！】 <a href="https://www.bilibili.com/video/BV1mB4y1L7HB/">https://www.bilibili.com/video/BV1mB4y1L7HB/</a> 。里面通过一些例子介绍了螺旋法则和对const关键词的判读。我将在此基础上加上我对其的解释，使其变得更加通俗易懂。</p>
<p>基本解决思路：<strong>const 只修饰右边最近的东西。</strong></p>
<p>例如<code>const r</code>, 说明<code>r</code>这个变量是const的，不能动。<code>const *r</code>，意为<code>const (*r)</code>，说明<code>*r</code>，r指向的东西，是const的。r自身可以变化。</p>
<p>再例如，<code>const int r</code>, 说明这个int是const的。这个整数是常量。<code>r</code>是一个整型常量。</p>
<p>举一些例子：</p>
<h3 id="例子1"><a href="#例子1" class="headerlink" title="例子1"></a>例子1</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="type">const</span> a1 = <span class="number">5</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> a2 = <span class="number">6</span>;</span><br></pre></td></tr></table></figure>

<p>在这里，两个其实是等价的。对于第一个，<code>a1</code>被const修饰了，所以<code>a1</code>不能变。<code>a1</code>是一个整型变量。所以连起来，<code>a1</code>是一个常量整型变量。对于第二个，可以理解成<code>a2</code>是一个const int，<code>a2</code>是一个常量整型变量。</p>
<p><strong>所以，不同的声明形式，意义可能相同。</strong></p>
<h3 id="例子2"><a href="#例子2" class="headerlink" title="例子2"></a>例子2</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="type">const</span> *p1 = &amp;a1;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> *p2 = &amp;a1;</span><br></pre></td></tr></table></figure>

<p>在这里，两个其实也是等价的。对于第一个，<code>*p1</code>被const修饰了，所以<code>*p1</code>不能变。即<code>p1</code>指向的是一个整型常量。对于第二个，可以理解成<code>p2</code>指向一个const int，<code>p2</code>指向的是一个常量整型变量。对于这两个来说，<code>p1</code> <code>p2</code>自己可以变，但是他们指向的不能变。</p>
<p><strong>一个声明里的多个const，意义可能相同。例如：</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">int</span> <span class="type">const</span> *p3</span><br></pre></td></tr></table></figure>

<p>这个例子里虽然有两个const，但其实还是只修饰了一个<code>*p3</code>。<code>p3</code>自身没有被const修饰，可以变。</p>
<h3 id="例子3"><a href="#例子3" class="headerlink" title="例子3"></a>例子3</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> *<span class="type">const</span> r2;</span><br></pre></td></tr></table></figure>

<p>在这里，<code>r2</code>直接被const修饰，所以<code>r2</code>不能变。<code>r2</code>指向一个int，那个int没有额外修饰，所以那个int是可以变的。总之，<code>r2</code>是不能变的，但是<code>r2</code>指向的int是可以变的。</p>
<h3 id="例子4"><a href="#例子4" class="headerlink" title="例子4"></a>例子4</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">int</span> *<span class="type">const</span> r3;</span><br></pre></td></tr></table></figure>

<p>在这里，<code>r3</code>直接被const修饰，所以<code>r3</code>不能变。<code>r3</code>指向一个const int，那个int没有额外修饰，所以那个int是可以变的。总之，<code>r3</code>是不能变的，<code>r3</code>指向的int也不可以变的。</p>
<h3 id="例子5"><a href="#例子5" class="headerlink" title="例子5"></a>例子5</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">int</span> **r5;</span><br></pre></td></tr></table></figure>

<p><code>r5</code>指向一个指向int的指针。这个被指向的指针指向的int是被const所修饰的。所以<code>r5</code>可以变，<code>r5</code>指向的那个（指向int的指针）也是可以变的，但是<code>r5</code>指向的指针指向的int没得变。</p>
<h3 id="例子6"><a href="#例子6" class="headerlink" title="例子6"></a>例子6</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="type">const</span> * <span class="type">const</span> * <span class="type">const</span> r6;</span><br></pre></td></tr></table></figure>

<p>上面那个例子的升级版。<code>r6</code>指向一个指向int的指针。<code>r6</code>被const修饰，<code>r6</code>不能变。<code>r6</code>指向的（指向int的指针）被const修饰，即<code>*r6</code>不能变。<code>r6</code>指向的指针指向的int，<code>**r6</code>也被const修饰，也不能变。</p>
<h3 id><a href="#" class="headerlink" title></a></h3>]]></content>
      <categories>
        <category>Techniques</category>
      </categories>
  </entry>
  <entry>
    <title>C/C++ 复杂类型声明规则（1）-螺旋法则</title>
    <url>/2023/03/23/cpp-declare-1/</url>
    <content><![CDATA[<p>“C&#x2F;C++ 复杂类型声明规则” 将分为两节。第一节介绍螺旋法则声明规范，第二节介绍const的修饰关系。这两节内容，我都尝试以一种不用死记硬背，而是理解的方式去掌握。</p>
<p>本文参考自【初中生也能看懂的C&#x2F;C++类型声明规则教学，很简单的！】 <a href="https://www.bilibili.com/video/BV1mB4y1L7HB/">https://www.bilibili.com/video/BV1mB4y1L7HB/</a> 。里面通过一些例子介绍了螺旋法则。我将在此基础上加上我对其的解释，使其变得更加通俗易懂。</p>
<h3 id="螺旋法则："><a href="#螺旋法则：" class="headerlink" title="螺旋法则："></a>螺旋法则：</h3><ul>
<li><p>第一步，找到变量名，如果没有变置名，找到最里面的结构</p>
</li>
<li><p>第二步，向右看，读出你看到的东西但是不要跳过括号</p>
</li>
<li><p>第三步，再向左看，读出你看到的东西，但是也不要跳过括号</p>
</li>
<li><p>第四步，如果有括号的话，跳出一层括号</p>
</li>
<li><p>第五步，重复上述过程，直到你读出最终的类型</p>
</li>
</ul>
<h3 id="举例1："><a href="#举例1：" class="headerlink" title="举例1："></a>举例1：</h3><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> *v[<span class="number">5</span>];</span><br></pre></td></tr></table></figure>

<ul>
<li>第一步：</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> *v[<span class="number">5</span>];</span><br><span class="line">     ^</span><br></pre></td></tr></table></figure>

<p>如上，我们找到了<code>v</code>是变量名。读作：<code>v</code>是……</p>
<ul>
<li>第二步，向右看，读出你看到的东西但是不要跳过括号。</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> *v[<span class="number">5</span>];</span><br><span class="line">      ^^^</span><br></pre></td></tr></table></figure>

<p><code>[5]</code>意为是一个5个元素的数组。读作：v是一个五个元素的数组。</p>
<ul>
<li>第三步，再向左看，读出你看到的东西，但是也不要跳过括号。</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> *v[<span class="number">5</span>];</span><br><span class="line">^^^^^</span><br></pre></td></tr></table></figure>

<p><code>int *</code>意为每个东西是一个指向int的指针。读作：<code>v</code>是一个五个元素的数组，数组每个东西指向一个int型指针。</p>
<ul>
<li>第四步，如果有括号的话，跳出一层括号。</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> *v[<span class="number">5</span>];</span><br></pre></td></tr></table></figure>

<p>对于上面这个例子，无这一步。我们已经读完了：<code>v</code>是一个五个元素的数组，数组每个东西指向一个int型指针。</p>
<p>我们可以验证一下我们理解正不正确。我们可以用下列的代码给v数组赋值：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> a = <span class="number">2023</span>;</span><br><span class="line">v[<span class="number">0</span>] = &amp;a;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;%p %p&quot;</span>, &amp;a, v[<span class="number">0</span>]);</span><br></pre></td></tr></table></figure>

<p>g++编译通过且运行正确，可见我们的理解没有问题。</p>
<h3 id="举例2"><a href="#举例2" class="headerlink" title="举例2"></a>举例2</h3><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> (*func)()</span><br></pre></td></tr></table></figure>

<ul>
<li>第一步：</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> (*func)()</span><br><span class="line">      ^^^^</span><br></pre></td></tr></table></figure>

<p>如上，我们找到了<code>func</code>是变量名。读作：<code>func</code>是……</p>
<ul>
<li>第二步：向右看，读出你看到的东西但是不要跳过括号。</li>
</ul>
<p>向右边遇到括号了，不跳出括号，skip。</p>
<ul>
<li>第三步，再向左看，读出你看到的东西，但是也不要跳过括号。</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> (*func)()</span><br><span class="line">     ^ </span><br></pre></td></tr></table></figure>

<p>单独一个*意为“指向”。读作：<code>func</code>指向……</p>
<ul>
<li>第四步，如果有括号的话，跳出一层括号。向右看。</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> (*func)()</span><br><span class="line">           ^^</span><br></pre></td></tr></table></figure>

<p>这样一组括号是函数的意思。括号内为空，说明这个函数没有参数。整个<code>(*func)</code>的左边是<code>int</code>，它是这个函数的返回值，是<code>int</code>。读作：<code>func</code>指向一个接收空参且返回值为<code>int</code>的函数。</p>
<p>我们可以验证一下我们理解正不正确。我们可以用下列的代码用匿名函数给<code>func</code>赋值：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">int</span> (*func1)();</span><br><span class="line">func1 = []()-&gt;<span class="type">int</span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">2023</span>;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;%d&quot;</span>, <span class="built_in">func1</span>());</span><br></pre></td></tr></table></figure>

<p>g++编译通过且运行正确，可见我们的理解没有问题。</p>
<h3 id="总结-一个网站推荐"><a href="#总结-一个网站推荐" class="headerlink" title="总结&amp;一个网站推荐"></a>总结&amp;一个网站推荐</h3><p>螺旋法则五步骤；</p>
<p>当读的时候，看到<code>[]</code>意为一个数组， <code>()</code>意为是一个函数，那么在主体的左侧肯定配套还会有一个返回值（当然，这个返回值也可能是一个复杂复合类型）。 <code>*</code>如果单独出现，意为指向，如果和类型出现，如<code>int*</code>，那就可以理解为指向int。</p>
<p>这里顺便推荐一个可以辅助阅读的网站：<a href="https://cdecl.org/">https://cdecl.org/</a> 。这个网站可以将表达式翻译成人话（英语），帮助大家的理解。但是总体来说只要会了螺旋法则，啥声明都能理解确切含义。</p>
]]></content>
      <categories>
        <category>Techniques</category>
      </categories>
  </entry>
  <entry>
    <title>Numpy快速入门总览</title>
    <url>/2023/03/22/Numpy-intro/</url>
    <content><![CDATA[<p>本文是一个Python强大的计算库，NumPy（以下简称np）的入门介绍。在本文中，你会了解到：</p>
<ol>
<li>什么是numpy？</li>
<li>为什么要用numpy？</li>
<li>从python内置list到numpy，有什么不同？</li>
<li>怎么安装与学习numpy？</li>
</ol>
<p>因为网上numpy基础教程非常多，而且基本都覆盖了核心内容。因此，我不会再重复介绍numpy的一些基本操作，我只会留一些链接在第四部分。相反，我会从一些high-level的角度，介绍numpy，给读者构建一个大体印象。</p>
<h2 id="1-什么是numpy？"><a href="#1-什么是numpy？" class="headerlink" title="1. 什么是numpy？"></a>1. 什么是numpy？</h2><p>根据numpy官网（ <a href="https://numpy.org/doc/stable/user/whatisnumpy.html">https://numpy.org/doc/stable/user/whatisnumpy.html</a> ）的介绍，numpy是一个python的科学计算库（如其名，numpy&#x3D;numeric python）。他提供了多维数组等实用对象，和大量对数组的快速操作与算法。</p>
<blockquote>
<p>NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays.</p>
</blockquote>
<h2 id="2-为什么要用numpy？"><a href="#2-为什么要用numpy？" class="headerlink" title="2. 为什么要用numpy？"></a>2. 为什么要用numpy？</h2><h3 id="2-1-方便"><a href="#2-1-方便" class="headerlink" title="2.1 方便"></a>2.1 方便</h3><p>python的内置list仅提供了一些简单操作，如append增加元素，sort排序元素等。如果要做一些复杂运算，就会略显吃力。numpy集成了大量的数学、数组操作函数，<strong>不需要自己造轮子</strong>了。例如直接使用<code>np.corr()</code>就可以计算相关系数，直接使用<code>np.linalg.inv()</code>就可以计算矩阵的逆矩阵，直接调用<code>np.fft</code>模块可以进行一些傅立叶变换相关的操作。</p>
<h3 id="2-2-快速高效"><a href="#2-2-快速高效" class="headerlink" title="2.2 快速高效"></a>2.2 快速高效</h3><p>高效是numpy的核心亮点之一。我想可以分为程序运行高效和程序员编程高效。</p>
<p>运行高效：因为python是一种解释型语言，其<strong>运行速度比c&#x2F;c++语言会慢几十倍</strong>左右。numpy的底层代码（见 <a href="https://github.com/numpy/numpy">https://github.com/numpy/numpy</a> ）都是使用c语言写的，并被高度优化过。因此其相同任务运行速度远快于python的list，和c++水平大抵相同。举个例子，两个2,000,000长的向量相加，纯用python list和for循环需要0.173s，而使用numpy则只需要0.008s。</p>
<p>编程高效：numpy里的许多运算符也被重载过，进行一般四则运算也很方便。如两个<code>np.ndarray</code> a, b, 可以直接通过加号<code>a+b</code>实现向量的逐位相加，而若是两个list a,b, <code>a+b</code>只是将两个列表拼接起来。另外，numpy也实现了<code>-</code>, <code>*</code>, <code>&amp;</code>进行逐位相减、逐位向乘、逐位与的操作。下面是例子展示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># a, b are list</span></span><br><span class="line">a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">b = [<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">a+b</span><br><span class="line"><span class="comment">#OUTPUT: [1, 2, 3, 3, 2, 1]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># a, b are ndarray</span></span><br><span class="line">a = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = np.array([<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line">a+b</span><br><span class="line"><span class="comment">#OUTPUT: np.array([4, 4, 4])</span></span><br></pre></td></tr></table></figure>



<h3 id="2-3-Python数据科学之基础"><a href="#2-3-Python数据科学之基础" class="headerlink" title="2.3 Python数据科学之基础"></a>2.3 Python数据科学之基础</h3><p>许多python的科学计算、数据科学库都是以numpy为数据容器的基础根基的。例如sklearn（机器学习库）中的算法使用numpy来实现，用户传入的均为<code>numpy.ndarray</code>数据；matplotlib（画图库）中接收<code>numpy.ndarray</code>数组绘图，等等。因此numpy是要用python玩数据科学等领域的基础。学会numpy，可以做以下事情：</p>
<ol>
<li><p>复现论文中的算法</p>
</li>
<li><p>研究一些数据科学库的源码，实现细节</p>
</li>
<li><p>掌握一个以python为接口的高性能计算的核心工具，做任何想做的事情</p>
</li>
<li><p>……</p>
</li>
</ol>
<h2 id="3-从python-list到numpy"><a href="#3-从python-list到numpy" class="headerlink" title="3. 从python list到numpy"></a>3. 从python list到numpy</h2><p>从python的内置list列表到numpy的ndarray有一些“同”也有一些“不同”。这里简单进行一个介绍，以使读者可以对numpy数组有一个更清晰的大体影响。</p>
<p><code>numpy.ndarray</code>对象更像是其他编程语言（如C++&#x2F;Java）里的数组（这样的数组结构才可以做到快速操作）。一旦构建，就要声明数组大小形状和数据类型。例如在C++中声明一个二维10*10的整型数组可以是：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> array[<span class="number">10</span>][<span class="number">10</span>];</span><br></pre></td></tr></table></figure>

<p>而与之对应，在numpy里声明这样一个数组是：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">array = np.ndarray(shape=(<span class="number">10</span>, <span class="number">10</span>), dtype=<span class="built_in">int</span>)</span><br></pre></td></tr></table></figure>

<p>而python中的list实现本质是一个动态数组，其长度会根据运行时来决定。</p>
<p>接下来，我便由此列几点numpy数组和list的不同：</p>
<ul>
<li>改变大小。</li>
</ul>
<p>因为list是动态数组，其长度会根据运行时来决定，因此list有append、pop等方法改变列表长短。而<code>numpy.ndarray</code>数组则“<strong>一旦构建，就有固定的大小</strong>”。在运行时不能改变大小（有几个元素），只能改变形状（如每行每列有几个元素）。<code>np.append()</code>方法虽说叫append，但是其本质也是要构建一个新的数组，而不是在原来的上做添加。</p>
<ul>
<li>统一数据类型。</li>
</ul>
<p>一个python list中可以存放不同类型，例如<code>[1, 1.2f, &#39;str&#39;]</code>。</p>
<p>一个numpy数组具有统一的数据类型，这使得其indexing<strong>索引、存储效率远高于</strong>list。一个数组内只能是一个类型，例如<code>np.int32</code>, <code>int</code>, <code>np.float32</code>, <code>float</code>等。如果需要存储不同类型，则需要将类型设为<code>object</code>。但是这样许多numpy函数便不支持使用了。</p>
<p>但是作为python语言的模块，numpy的设计还是非常清晰易懂的。大体上就是创建数组对象，然后对其进行运算或函数操作。语法上也非常相近。例如，numpy也支持list的切片索引（如<code>a[::2]</code>，且其功能玩法更多）。</p>
<h2 id="4-怎么安装与学习numpy？"><a href="#4-怎么安装与学习numpy？" class="headerlink" title="4. 怎么安装与学习numpy？"></a>4. 怎么安装与学习numpy？</h2><h3 id="4-1-安装"><a href="#4-1-安装" class="headerlink" title="4.1 安装"></a>4.1 安装</h3><p>网上的教程非常多，涵盖Windows，macOS和Linux系统。只需在网上找到相关教程即可。使用anaconda、miniconda、pip安装都可以。</p>
<h3 id="4-2-学习"><a href="#4-2-学习" class="headerlink" title="4.2 学习"></a>4.2 学习</h3><p>以个人经验来看，numpy的学习胜在多用。只要用得多，自然而然就熟练、学会了。numpy已经包含了大部分需要用的函数，<strong>如有对应需求，一定要先去网上搜索</strong>看看有无相关函数（现在还有chatGPT之类的可以问了）。</p>
<p>对于新手朋友我在这里推荐几个学习资源链接：</p>
<ul>
<li><p>NumPy官网 官方文档 <a href="https://numpy.org/doc/stable/index.html">https://numpy.org/doc/stable/index.html</a></p>
</li>
<li><p>【一个10分钟的numpy入门教程】<strong>（十分推荐）</strong> <a href="https://www.bilibili.com/video/BV1Wy4y1h7ii/?share_source=copy_web&vd_source=0c067fd928325f3684e2a932b9539e44">https://www.bilibili.com/video/BV1Wy4y1h7ii/?share_source=copy_web&amp;vd_source=0c067fd928325f3684e2a932b9539e44</a></p>
</li>
<li><p>NumPy 教程 菜鸟教程<strong>（文字+例子，较全面）</strong>: <a href="https://www.runoob.com/numpy/numpy-tutorial.html">https://www.runoob.com/numpy/numpy-tutorial.html</a></p>
</li>
<li><p>全网最全Numpy图解教程 知乎<strong>（有很多图，生动形象）</strong><a href="https://zhuanlan.zhihu.com/p/528144539">https://zhuanlan.zhihu.com/p/528144539</a></p>
</li>
<li><p>百度 Google搜索 （以“numpy xxxxxx”提问，基本上都会有想要的回答的）</p>
</li>
</ul>
]]></content>
      <categories>
        <category>QuickIntro</category>
      </categories>
  </entry>
  <entry>
    <title>我在其他平台发的文章链接</title>
    <url>/2023/03/21/Redirect-other-webpages/</url>
    <content><![CDATA[<p>这里会摆放我在其他平台上写的文章的链接（因为我懒得搬过来了），欢迎点击阅读。</p>
<ul>
<li><p>原生word仿制latex伪代码块，数学建模、作业报告均适用！【带图手把手教学】 <a href="https://zhuanlan.zhihu.com/p/472381746">https://zhuanlan.zhihu.com/p/472381746</a></p>
</li>
<li><p>个人炼丹炉配置经验分享（上）：利用校园网LAN启动、智能插座远程启动并远程桌面连接 <a href="https://zhuanlan.zhihu.com/p/460263000">https://zhuanlan.zhihu.com/p/460263000</a></p>
</li>
<li><p>个人炼丹炉配置经验分享（下）：利用JupyterLab实现何时何地都可远程炼丹 <a href="https://zhuanlan.zhihu.com/p/461491632">https://zhuanlan.zhihu.com/p/461491632</a></p>
</li>
<li><p>【新手向】Python x VS Code 简单debug与调试 <a href="https://zhuanlan.zhihu.com/p/357874811">https://zhuanlan.zhihu.com/p/357874811</a></p>
</li>
<li><p>【新手向】Python x VS Code debug按钮 与 进阶debug介绍 <a href="https://zhuanlan.zhihu.com/p/359283509">https://zhuanlan.zhihu.com/p/359283509</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>Others</category>
      </categories>
  </entry>
  <entry>
    <title>Matplotlib Quick Reference</title>
    <url>/2023/03/21/mpl-cheatsheets/</url>
    <content><![CDATA[<h3 id="Cheatsheet"><a href="#Cheatsheet" class="headerlink" title="Cheatsheet"></a>Cheatsheet</h3><p>来自mpl官网的 <a href="https://matplotlib.org/cheatsheets/cheatsheets.pdf">https://matplotlib.org/cheatsheets/cheatsheets.pdf</a> 的官方cheatsheet，但是pdf不方便即时查看，遂导出成图片，可以存到相册。图片可以单击后放大查看。</p>
<p>page1:</p>
<p><img src="/2023/03/21/mpl-cheatsheets/mpl-cheatsheets-1.png" alt="Cheatsheet"></p>
<p>page2:</p>
<p><img src="/2023/03/21/mpl-cheatsheets/mpl-cheatsheets-2.png" alt="Cheatsheet"></p>
<p>这里面覆盖了大部分使用mpl时会用到的函数，详细使用方法可见官网（<a href="https://matplotlib.org/%EF%BC%89%E3%80%82">https://matplotlib.org/）。</a></p>
<p>本文后续将会挑一些进行额外解释。</p>
<h3 id="colormap-class"><a href="#colormap-class" class="headerlink" title="colormap class"></a>colormap class</h3><p>colormap是把图变好看的灵魂。属于<code>matplotlib.colors.Colormap()</code>类. 这个类实现了<code>__call__()</code>方法，因此可以传入一个介于0-1之间的数，其返回一个四元组，表示一个RGBA颜色。这个元组可以被mpl的各类画图函数接收。</p>
<p>基础用法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">cmap = plt.get_cmap(<span class="string">&#x27;magma&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(cmap(<span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(cmap(<span class="number">0.5</span>))</span><br><span class="line"><span class="built_in">print</span>(cmap(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># OUTPUT:</span></span><br><span class="line">(<span class="number">0.001462</span>, <span class="number">0.000466</span>, <span class="number">0.013866</span>, <span class="number">1.0</span>)</span><br><span class="line">(<span class="number">0.716387</span>, <span class="number">0.214982</span>, <span class="number">0.47529</span>, <span class="number">1.0</span>)</span><br><span class="line">(<span class="number">0.002258</span>, <span class="number">0.001295</span>, <span class="number">0.018331</span>, <span class="number">1.0</span>)</span><br></pre></td></tr></table></figure>

<p>实践用法：画一条渐变的线。官网有例子 <a href="https://matplotlib.org/stable/gallery/lines_bars_and_markers/multicolored_line.html#sphx-glr-gallery-lines-bars-and-markers-multicolored-line-py">https://matplotlib.org/stable/gallery/lines_bars_and_markers/multicolored_line.html#sphx-glr-gallery-lines-bars-and-markers-multicolored-line-py</a>, 我这是另一种实现方法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">cmap = plt.get_cmap(<span class="string">&#x27;magma&#x27;</span>)</span><br><span class="line"></span><br><span class="line">t = np.linspace(<span class="number">0</span>, <span class="number">10</span>, <span class="number">100</span>)</span><br><span class="line">xt = np.sin(t)</span><br><span class="line">normalized_t = t / t.<span class="built_in">max</span>() <span class="comment"># make it into [0, 1]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(t)-<span class="number">1</span>): <span class="comment"># draw piece by piece</span></span><br><span class="line">    plt.plot(t[i:i+<span class="number">2</span>], xt[i:i+<span class="number">2</span>], color=cmap(normalized_t[i]))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>效果：</p>
<p><img src="/2023/03/21/mpl-cheatsheets/cmap.png" alt="Cheatsheet"></p>
]]></content>
      <categories>
        <category>QRH</category>
      </categories>
  </entry>
  <entry>
    <title>Hello World (Hexo Quick Reference)</title>
    <url>/2023/03/21/Hello-World-Hexo-QRH/</url>
    <content><![CDATA[<p>This blog is built with <a href="https://hexo.io/">Hexo</a>! This is the originally first post, automatically generated by Hexo. I modified it to a Quick Reference Handbook (QRH) for using this.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Clean-cache"><a href="#Clean-cache" class="headerlink" title="Clean cache"></a>Clean cache</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo clean</span><br><span class="line"><span class="comment"># shortcut:</span></span><br><span class="line">$ hexo c</span><br></pre></td></tr></table></figure>

<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br><span class="line"><span class="comment"># shortcut:</span></span><br><span class="line">$ hexo g</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Run-server-local-testing"><a href="#Run-server-local-testing" class="headerlink" title="Run server (local testing)"></a>Run server (local testing)</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br><span class="line"><span class="comment"># shortcut:</span></span><br><span class="line">$ hexo s</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br><span class="line"><span class="comment"># shortcut:</span></span><br><span class="line">$ hexo d</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
      <categories>
        <category>QRH</category>
      </categories>
  </entry>
</search>
