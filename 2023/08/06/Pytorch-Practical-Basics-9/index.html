<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/blogs/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/blogs/images/time-machine.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/blogs/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/blogs/images/logo.svg" color="#222">

<link rel="stylesheet" href="/blogs/css/main.css">


<link rel="stylesheet" href="/blogs/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"i-am-future.github.io","root":"/blogs/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="In the section 6 to 9, we’ll investigate how to use torch.autograd.Function to implement the hand-written operators. The tentative outline is:   In the section 6, we talk about the basics of torch.aut">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch Practical Hand-Written Modules Basics 9--Compiling and Testing the Module">
<meta property="og:url" content="https://i-am-future.github.io/2023/08/06/Pytorch-Practical-Basics-9/index.html">
<meta property="og:site_name" content="Future&#39;s blog">
<meta property="og:description" content="In the section 6 to 9, we’ll investigate how to use torch.autograd.Function to implement the hand-written operators. The tentative outline is:   In the section 6, we talk about the basics of torch.aut">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://i-am-future.github.io/blogs/2023/08/06/Pytorch-Practical-Basics-9/cudaops-struct-improved.drawio.png">
<meta property="article:published_time" content="2023-08-07T06:07:03.000Z">
<meta property="article:modified_time" content="2023-08-15T11:02:40.454Z">
<meta property="article:author" content="Future">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i-am-future.github.io/blogs/2023/08/06/Pytorch-Practical-Basics-9/cudaops-struct-improved.drawio.png">

<link rel="canonical" href="https://i-am-future.github.io/2023/08/06/Pytorch-Practical-Basics-9/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>PyTorch Practical Hand-Written Modules Basics 9--Compiling and Testing the Module | Future's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/blogs/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Future's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/blogs/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/blogs/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/blogs/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://i-am-future.github.io/2023/08/06/Pytorch-Practical-Basics-9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blogs/images/avatar.gif">
      <meta itemprop="name" content="Future">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Future's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          PyTorch Practical Hand-Written Modules Basics 9--Compiling and Testing the Module
        </h1>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-08-07 14:07:03" itemprop="dateCreated datePublished" datetime="2023-08-07T14:07:03+08:00">2023-08-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-08-15 19:02:40" itemprop="dateModified" datetime="2023-08-15T19:02:40+08:00">2023-08-15</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/blogs/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>In the section 6 to 9, we’ll investigate how to use <code>torch.autograd.Function</code> to implement the hand-written operators. The tentative outline is:</p>
<ul>
<li><input checked disabled type="checkbox"> In the section 6, we talk about the basics of <code>torch.autograd.Function</code>. <strong>The operators defined by <code>torch.autograd.Function</code> can be automatically back-propagated.</strong></li>
<li><input checked disabled type="checkbox"> In the last section (7), we’ll talk about <strong>mathematic derivation</strong> for the “linear layer” operator. </li>
<li><input checked disabled type="checkbox"> In the section (8), we talk about <strong>writing C++ CUDA extension</strong> for the “linear layer” operator. </li>
<li><input checked disabled type="checkbox"> In this section (9), we talk details about <strong>building the extension</strong> to a python module, as well as <strong>testing</strong> the module. Then we’ll <strong>conclude</strong> the things we’ve done so far.</li>
</ul>
<p><strong>Note:</strong></p>
<ul>
<li>This blog is written with following reference: <ul>
<li>PyTorch official tutorial about CUDA extension: <a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/advanced/cpp_extension.html">website</a>.</li>
<li>YouTube video about writing CUDA extension: <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=l_Rpk6CRJYI">video</a>, <a target="_blank" rel="noopener" href="https://github.com/kwea123/pytorch-cppcuda-tutorial">code</a>.</li>
</ul>
</li>
<li>For how to write CUDA code, you can follow <a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">official documentation</a>, <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/34587739">blogs</a> (In Chinese). You can search by yourself for English tutorials and video tutorials.</li>
<li>This blog only talk some important points in the matrix multiplication example. Code are picked by pieces for illustration. Whole code is at: <a target="_blank" rel="noopener" href="https://github.com/I-am-Future/PyTorch-Linear-Operator-CUDA">code</a>.</li>
</ul>
<h1 id="Python-side-Wrapper"><a href="#Python-side-Wrapper" class="headerlink" title="Python-side Wrapper"></a>Python-side Wrapper</h1><p>Purely using C++ extension functions is not enough in our case. As mentioned in the Section 6, we need to build our operators with <code>torch.autograd.Function</code>. It is not convenient to let the user define the operator wrapper every time, so it’s better if we can write the wrapper in a python module. Then, users can easily import our python module, and using the wrapper class and functions in it.</p>
<img src="/blogs/2023/08/06/Pytorch-Practical-Basics-9/cudaops-struct-improved.drawio.png" alt="cudaops-struct-improved.drawio" style="zoom:50%;">

<p>The python module is at <code>mylinearops/</code>. Follow the section 6, we define some <code>autograd.Function</code> operators and <code>nn.Module</code> modules in the <code>mylinearops/mylinearops.py</code>. Then, we export the operators and modules by the code in the <code>mylinearops/__init__.py</code>:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> .mylinearops <span class="keyword">import</span> matmul</span><br><span class="line"><span class="keyword">from</span> .mylinearops <span class="keyword">import</span> linearop</span><br><span class="line"><span class="keyword">from</span> .mylinearops <span class="keyword">import</span> LinearLayer</span><br></pre></td></tr></table></figure>

<p>As a result, when user imports the <code>mylinearops</code>, only the <code>matmul</code> (Y &#x3D; XW) function, <code>linearop</code> (Y &#x3D; XW+b) function and <code>LinearLayer</code> module are public to the users. </p>
<h1 id="Writing-setup-py-and-Building"><a href="#Writing-setup-py-and-Building" class="headerlink" title="Writing setup.py and Building"></a>Writing setup.py and Building</h1><h2 id="setup-py-script"><a href="#setup-py-script" class="headerlink" title="setup.py script"></a>setup.py script</h2><p>The <code>setup.py</code> script is general same for all packages. Next time, you can just copy-paste the code above and modify some key components. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> os.path <span class="keyword">as</span> osp</span><br><span class="line"><span class="keyword">from</span> setuptools <span class="keyword">import</span> setup</span><br><span class="line"><span class="keyword">from</span> torch.utils.cpp_extension <span class="keyword">import</span> CUDAExtension, BuildExtension</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ROOT_DIR = osp.dirname(osp.abspath(__file__))</span><br><span class="line">include_dirs = [osp.join(ROOT_DIR, <span class="string">&quot;include&quot;</span>)]</span><br><span class="line"></span><br><span class="line">SRC_DIR = osp.join(ROOT_DIR, <span class="string">&quot;src&quot;</span>)</span><br><span class="line">sources = glob.glob(osp.join(SRC_DIR, <span class="string">&#x27;*.cpp&#x27;</span>))+glob.glob(osp.join(SRC_DIR, <span class="string">&#x27;*.cu&#x27;</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">setup(</span><br><span class="line">    name=<span class="string">&#x27;mylinearops&#x27;</span>,</span><br><span class="line">    version=<span class="string">&#x27;1.0&#x27;</span>,</span><br><span class="line">    author=...,</span><br><span class="line">    author_email=...,</span><br><span class="line">    description=<span class="string">&#x27;Hand-written Linear ops for PyTorch&#x27;</span>,</span><br><span class="line">    long_description=<span class="string">&#x27;Simple demo for writing Linear ops in CUDA extensions with PyTorch&#x27;</span>,</span><br><span class="line">    ext_modules=[</span><br><span class="line">        CUDAExtension(</span><br><span class="line">            name=<span class="string">&#x27;mylinearops_cuda&#x27;</span>,</span><br><span class="line">            sources=sources,</span><br><span class="line">            include_dirs=include_dirs,</span><br><span class="line">            extra_compile_args=&#123;<span class="string">&#x27;cxx&#x27;</span>: [<span class="string">&#x27;-O2&#x27;</span>],</span><br><span class="line">                                <span class="string">&#x27;nvcc&#x27;</span>: [<span class="string">&#x27;-O2&#x27;</span>]&#125;</span><br><span class="line">        )</span><br><span class="line">    ],</span><br><span class="line">    py_modules=[<span class="string">&#x27;mylinearops.mylinearops&#x27;</span>],</span><br><span class="line">    cmdclass=&#123;</span><br><span class="line">        <span class="string">&#x27;build_ext&#x27;</span>: BuildExtension</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>At the beginning, we first get the path information. We get the <code>include_dirs</code> (Where we store our <code>.h</code> headers), <code>sources</code> (Where we store our C++&#x2F;CUDA source code) directory. </p>
<p>Then, we call the <code>setup</code> function. The parameter explanation are as following:</p>
<ul>
<li><code>name</code>: The package name, how do users call this program</li>
<li><code>version</code>: The version number, decided by the creator</li>
<li><code>author</code>: The creator’s name</li>
<li><code>author_email</code>: The creator’s email</li>
<li><code>description</code>: The package’s description, short version</li>
<li><code>long_description</code>: The package’s description, long version</li>
<li><code>ext_modules</code>: <strong>Key</strong> in our building process. When we are building the PyTorch CUDA extension, we should use <code>CUDAExtension</code>, so that the build helper can know how to compile correctly<ul>
<li><code>name</code>: the CUDA extension name. We import this name in our wrapper to access the cuda functions</li>
<li><code>sources</code>: the source files</li>
<li><code>include_dirs</code>: the header files</li>
<li><code>extra_compile_args</code>: The extra compiling flags. <code>&#123;&#39;cxx&#39;: [&#39;-O2&#39;], nvcc&#39;: [&#39;-O2&#39;]&#125;</code> is commonly used, which means using <code>-O2</code> optimization level when compiling</li>
</ul>
</li>
<li><code>py_modules</code>: The Python modules needed for the package, which is our wrapper, <code>mylinearops</code>. In most cases, the wrapper module has the same name as the overall package name. (<code>&#39;mylinearops.mylinearops&#39;</code> stands for <code>&#39;mylinearops/mylinearops.py&#39;</code>)</li>
<li><code>cmdclass</code>: When building the PyTorch CUDA extension, we always pass in this: <code>&#123;&#39;build_ext&#39;: BuildExtension&#125;</code></li>
</ul>
<h2 id="Building"><a href="#Building" class="headerlink" title="Building"></a>Building</h2><p>Then, we can build the package. We first activate the conda environment where we want to install in:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate &lt;target_env&gt;</span><br></pre></td></tr></table></figure>

<p>Then run:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> &lt;proj_root&gt;</span><br><span class="line">python setup.py install</span><br></pre></td></tr></table></figure>

<p><strong>Note:</strong> Don’t run <code>pip install .</code>, otherwise your python module will not be successfully installed, at least in my case.</p>
<p>It may take some time to compile it. If the building process ends up with some error message, go and fix them. If it finally displays something as “successfully installed mylinearops”, then you are ready to go.</p>
<p>To check if the installation is successful, we can try to import it:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ python</span><br><span class="line">Python 3.9.15 (main, Nov 24 2022, 14:31:59) </span><br><span class="line">[GCC 11.2.0] :: Anaconda, Inc. on linux</span><br><span class="line">Type <span class="string">&quot;help&quot;</span>, <span class="string">&quot;copyright&quot;</span>, <span class="string">&quot;credits&quot;</span> or <span class="string">&quot;license&quot;</span> <span class="keyword">for</span> more information.</span><br><span class="line">&gt;&gt;&gt; import mylinearops</span><br><span class="line">&gt;&gt;&gt; <span class="built_in">dir</span>(mylinearops)</span><br><span class="line">[<span class="string">&#x27;LinearLayer&#x27;</span>, <span class="string">&#x27;__builtins__&#x27;</span>, <span class="string">&#x27;__cached__&#x27;</span>, <span class="string">&#x27;__doc__&#x27;</span>, <span class="string">&#x27;__file__&#x27;</span>, <span class="string">&#x27;__loader__&#x27;</span>, <span class="string">&#x27;__name__&#x27;</span>, <span class="string">&#x27;__package__&#x27;</span>, <span class="string">&#x27;__path__&#x27;</span>, <span class="string">&#x27;__spec__&#x27;</span>, <span class="string">&#x27;linearop&#x27;</span>, <span class="string">&#x27;matmul&#x27;</span>, <span class="string">&#x27;mylinearops&#x27;</span>]</span><br><span class="line">&gt;&gt;&gt; </span><br></pre></td></tr></table></figure>

<p>Further testing will be mentioned in the next subsection.</p>
<h1 id="Module-Testing"><a href="#Module-Testing" class="headerlink" title="Module Testing"></a>Module Testing</h1><p>We will test the forward and backward of <code>matmul</code> and <code>LinearLayer</code> calculations respectively. To verify the answer, we’ll compare our answer with the PyTorch’s implementation or with <code>torch.autograd.gradcheck</code>. To increase the accuracy, we recommend to use <code>double</code> (<code>torch.float64</code>) type instead of <code>float</code> (<code>torch.float32</code>).</p>
<p>For tensors: create with argument <code>dtype=torch.float64</code>.</p>
<p>For modules: a good way is to use <code>model.double()</code> to convert all the parameters and buffers to <code>double</code>. </p>
<h2 id="forward"><a href="#forward" class="headerlink" title="forward"></a>forward</h2><p>A typical method is to use <code>torch.allclose</code> to verify if two tensors are close to each other. We can create the reference answer by PyTorch’s implementation.</p>
<ul>
<li><code>matmul</code>:</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> mylinearops</span><br><span class="line"></span><br><span class="line">A = torch.randn(<span class="number">20</span>, <span class="number">30</span>, dtype=torch.float64).cuda().requires_grad_()</span><br><span class="line">B = torch.randn(<span class="number">30</span>, <span class="number">40</span>, dtype=torch.float64).cuda().requires_grad_()</span><br><span class="line"></span><br><span class="line">res_my = mylinearops.matmul(A, B)</span><br><span class="line">res_torch = torch.matmul(A, B)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.allclose(res_my, res_torch))</span><br></pre></td></tr></table></figure>

<ul>
<li><code>LinearLayer</code>:</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> mylinearops</span><br><span class="line"></span><br><span class="line">A = torch.randn(<span class="number">40</span>, <span class="number">30</span>, dtype=torch.float64).cuda().requires_grad_() * <span class="number">100</span></span><br><span class="line">linear = mylinearops.LinearLayer(<span class="number">30</span>, <span class="number">50</span>).cuda().double()</span><br><span class="line"></span><br><span class="line">res_my = linear(A)</span><br><span class="line">res_torch = torch.matmul(A, linear.weight) + linear.bias</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.allclose(res_my, res_torch))</span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">max</span>(torch.<span class="built_in">abs</span>(res_my - res_torch)))</span><br></pre></td></tr></table></figure>

<p>It is worthwhile that sometimes, because of the floating number error, the answer from PyTorch is not consistent with the answer from our implementations. We have three methods:</p>
<ol>
<li>Pass <code>atol=1e-5, rtol=1e-5</code> into the <code>torch.allclose</code> to increase the tolerance level.</li>
<li><strong>[Not very recommended]</strong> We can observe the absolute error by <code>torch.max(torch.abs(res_my - res_torch))</code> for reference. If the result is merely 0.01 ~ 0.1, That would be OK in most cases.</li>
</ol>
<h2 id="backward"><a href="#backward" class="headerlink" title="backward"></a>backward</h2><p>For backward calculation, we can use <code>torch.autograd.gradcheck</code> to verify the result. If some tensors are only <code>float</code>, an warning will occur:</p>
<blockquote>
<p>……&#x2F;torch&#x2F;autograd&#x2F;gradcheck.py:647: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. </p>
</blockquote>
<p>So it is recommended to use the <code>double</code> type. Otherwise the check will likely fail.</p>
<ul>
<li><code>matmul</code>:</li>
</ul>
<p>As mentioned above, for pure calculation functions, we can assign all tensor as <code>double</code> (<code>torch.float64</code>) type. We are ready to go: </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> mylinearops</span><br><span class="line"></span><br><span class="line">A = torch.randn(<span class="number">20</span>, <span class="number">30</span>, dtype=torch.float64).cuda().requires_grad_()</span><br><span class="line">B = torch.randn(<span class="number">30</span>, <span class="number">40</span>, dtype=torch.float64).cuda().requires_grad_()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.autograd.gradcheck(mylinearops.matmul, (A, B)))    <span class="comment"># pass</span></span><br></pre></td></tr></table></figure>

<ul>
<li><code>LinearLayer</code>:</li>
</ul>
<p>As mentioned above, we can use <code>model.double()</code>. We are ready to go: </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> mylinearops</span><br><span class="line"></span><br><span class="line"><span class="comment">## CHECK for Linear Layer with bias ##</span></span><br><span class="line">A = torch.randn(<span class="number">40</span>, <span class="number">30</span>, dtype=torch.float64).cuda().requires_grad_()</span><br><span class="line">linear = mylinearops.LinearLayer(<span class="number">30</span>, <span class="number">40</span>).cuda().double()</span><br><span class="line"><span class="built_in">print</span>(torch.autograd.gradcheck(linear, (A,)))    <span class="comment"># pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## CHECK for Linear Layer without bias ##</span></span><br><span class="line">A = torch.randn(<span class="number">40</span>, <span class="number">30</span>, dtype=torch.float64).cuda().requires_grad_()</span><br><span class="line">linear_nobias = mylinearops.LinearLayer(<span class="number">30</span>, <span class="number">40</span>, bias=<span class="literal">False</span>).cuda().double()</span><br><span class="line"><span class="built_in">print</span>(torch.autograd.gradcheck(linear_nobias, (A,)))    <span class="comment"># pass</span></span><br></pre></td></tr></table></figure>



<h1 id="Full-Example"><a href="#Full-Example" class="headerlink" title="Full Example"></a>Full Example</h1><p>Now, we use our linear module to build a three layer classic linear model <code>[784, 256, 10]</code>to classify the MNIST digits. See the <code>examples/main.py</code> file. </p>
<p>Just as the <code>nn.Linear</code>, we create the model by:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MLP, self).__init__()</span><br><span class="line">        self.linear1 = mylinearops.LinearLayer(<span class="number">784</span>, <span class="number">256</span>, bias=<span class="literal">True</span>)<span class="comment">#.cuda()</span></span><br><span class="line">        self.linear2 = mylinearops.LinearLayer(<span class="number">256</span>, <span class="number">256</span>, bias=<span class="literal">True</span>)<span class="comment">#.cuda()</span></span><br><span class="line">        self.linear3 = mylinearops.LinearLayer(<span class="number">256</span>, <span class="number">10</span>, bias=<span class="literal">True</span>)<span class="comment">#.cuda()</span></span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        <span class="comment"># self.softmax = nn.Softmax(dim=1)</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">784</span>)</span><br><span class="line">        x = self.relu(self.linear1(x))</span><br><span class="line">        x = self.relu(self.linear2(x))</span><br><span class="line">        <span class="comment"># x = self.softmax(self.linear3(x))</span></span><br><span class="line">        x = self.linear3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<p>After writing some basic things, we can run our model: <code>python examples/tests.py</code>.</p>
<p>We also build the model by PyTorch’s <code>nn.Linear</code>. The result logging is:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mylinearops</span></span><br><span class="line">...</span><br><span class="line">Epoch: [10/10], Step: [100/468], Loss: 0.0417, Acc: 0.9844</span><br><span class="line">Epoch: [10/10], Step: [200/468], Loss: 0.0971, Acc: 0.9609</span><br><span class="line">Epoch: [10/10], Step: [300/468], Loss: 0.0759, Acc: 0.9766</span><br><span class="line">Epoch: [10/10], Step: [400/468], Loss: 0.0777, Acc: 0.9766</span><br><span class="line">Time: 23.4661s</span><br><span class="line"></span><br><span class="line"><span class="comment"># torch</span></span><br><span class="line">...</span><br><span class="line">Epoch: [10/10], Step: [100/468], Loss: 0.1048, Acc: 0.9688</span><br><span class="line">Epoch: [10/10], Step: [200/468], Loss: 0.0412, Acc: 0.9844</span><br><span class="line">Epoch: [10/10], Step: [300/468], Loss: 0.0566, Acc: 0.9688</span><br><span class="line">Epoch: [10/10], Step: [400/468], Loss: 0.0217, Acc: 0.9922</span><br><span class="line">Time: 26.5896s</span><br></pre></td></tr></table></figure>

<p>It is surprising that our implementation is even faster than the torch’s one. (But relax, after trying for some repetitions, we find ours is just as fast as the torch’s one). This is because the data scale is relatively small, the computation proportion is small. When the data scale is larger, ours may be slower than torch’s. </p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/blogs/2023/08/01/Pytorch-Practical-Basics-8/" rel="prev" title="PyTorch Practical Hand-Written Modules Basics 8--Linear Layer Ops on C++/CUDA">
      <i class="fa fa-chevron-left"></i> PyTorch Practical Hand-Written Modules Basics 8--Linear Layer Ops on C++/CUDA
    </a></div>
      <div class="post-nav-item">
    <a href="/blogs/2023/08/15/Pytorch-Practical-Basics-10/" rel="next" title="PyTorch Practical Hand-Written Modules Basics 10--Summary and Conclusion">
      PyTorch Practical Hand-Written Modules Basics 10--Summary and Conclusion <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Python-side-Wrapper"><span class="nav-number">1.</span> <span class="nav-text">Python-side Wrapper</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Writing-setup-py-and-Building"><span class="nav-number">2.</span> <span class="nav-text">Writing setup.py and Building</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#setup-py-script"><span class="nav-number">2.1.</span> <span class="nav-text">setup.py script</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Building"><span class="nav-number">2.2.</span> <span class="nav-text">Building</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Module-Testing"><span class="nav-number">3.</span> <span class="nav-text">Module Testing</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#forward"><span class="nav-number">3.1.</span> <span class="nav-text">forward</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#backward"><span class="nav-number">3.2.</span> <span class="nav-text">backward</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Full-Example"><span class="nav-number">4.</span> <span class="nav-text">Full Example</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Future</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/blogs/archives/">
        
          <span class="site-state-item-count">37</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/blogs/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Future</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/blogs/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/blogs/lib/velocity/velocity.min.js"></script>
  <script src="/blogs/lib/velocity/velocity.ui.min.js"></script>

<script src="/blogs/js/utils.js"></script>

<script src="/blogs/js/motion.js"></script>


<script src="/blogs/js/schemes/muse.js"></script>


<script src="/blogs/js/next-boot.js"></script>




  




  
<script src="/blogs/js/local-search.js"></script>













  

  

</body>
</html>
